---
title: "Daily Issue (JST): 2026-02-13"
---

# Daily Issue (JST): 2026-02-13

## What's New Today

**AI Advances in Reasoning, Agents, and Materials Discovery; Physics Insights on Electrostatics and Altermagnetism**

Today's research spans efficient AI reasoning, robust agent systems, and cross-domain applications linking language models to scientific discovery. Highlights include a counterintuitive finding that data repetition outperforms scaling for chain-of-thought fine-tuning, and a new framework enabling natural language-driven materials exploration. In physics, a rigorous analysis clarifies the macroscopic polarization of ionic crystals, while new work establishes antiferroaxial altermagnetism as a prevalent multiferroic mechanism.

**Highlights**
- Data repetition beats scaling for supervised fine-tuning on chain-of-thought data, improving benchmark scores by 12-26 points.
- Materials Knowledge Navigation Agent (MKNA) translates natural language scientific intent into executable actions for database retrieval and property prediction.
- A two-scale convergence analysis resolves the unit-cell dependence of macroscopic polarization in ionic crystals.
- Antiferroaxial altermagnetism is identified as a generic multiferroic mechanism where distortions induce and switch spin splitting.
- Token-level early stopping for diffusion language models achieves state-of-the-art efficiency by freezing stable tokens early.
- FormalJudge proposes a neuro-symbolic framework using LLMs as specification compilers for verifiable agent oversight.

**Themes**
- AI: Efficient Training & Reasoning
- AI: Agents & Robotics
- AI: Models & Architectures
- Materials: Discovery & Simulation
- Physics: Theory & Properties
- Cross-Domain: AI for Science

**Keywords**
- chain-of-thought
- supervised fine-tuning
- knowledge editing
- materials discovery
- agent oversight
- diffusion models
- polarization
- altermagnetism
- reinforcement learning
- vision-language models
- neural PDE surrogates
- optimal transport

**Total new papers:** 237

## Featured Papers

### [Data Repetition Beats Data Scaling in Long-CoT Supervised Fine-Tuning](https://arxiv.org/abs/2602.11149v1)
**Authors:** Dawid J. Kopiczko, Sagar Vaze, Tijmen Blankevoort, Yuki M. Asano
**Published:** 2026-02-11
**Summary:** Repeating a small chain-of-thought dataset for many epochs during fine-tuning yields better reasoning performance than using a much larger dataset for a single epoch.

**What's new**
- Shows that supervised fine-tuning benefits from data repetition over data scaling under a fixed compute budget.
- Demonstrates 12-26 percentage point gains on reasoning benchmarks by training for 128 epochs on 400 samples versus 1 epoch on 51200 samples.
- Identifies training token accuracy as a reliable stopping signal, with gains plateauing at full memorization.
- Poses the repetition advantage, where memorization coincides with generalization, as an open problem in LLM training dynamics.

### [Just on Time: Token-Level Early Stopping for Diffusion Language Models](https://arxiv.org/abs/2602.11133v1)
**Authors:** Zahar Kohut, Severyn Shykula, Dmytro Khamula, Mykola Vysotskyi, Taras Rumezhak, Volodymyr Karpiv
**Published:** 2026-02-11
**Summary:** A training-free method that stops diffusion steps early for each token when it stabilizes, speeding up text generation without quality loss.

**What's new**
- Introduces token-level early stopping for diffusion language models, freezing tokens individually as they converge.
- Uses lightweight, local signals from model predictions to decide when to stop, requiring no task-specific fine-tuning.
- Dynamically reduces total diffusion steps needed, achieving state-of-the-art efficiency across diverse benchmarks.
- Maintains generation quality on tasks like mathematical reasoning, QA, and scientific understanding.

### [Two-Scale Analysis of the Electrostatics of Dielectric Crystals: Emergence of Polarization Density and Boundary Charges](https://arxiv.org/abs/2602.10927v1)
**Authors:** Shoham Sen, Yang Wang, Timothy Breitzman, Kaushik Dayal
**Published:** 2026-02-11
**Summary:** A rigorous two-scale analysis shows how bulk polarization and surface charges emerge in dielectric crystals, with unit cell choices compensated to yield unique electric fields.

**What's new**
- Applies 2-scale convergence to rigorously derive macroscopic polarization from atomic-scale electrostatics in crystals.
- Shows different unit cell choices yield different bulk polarization and surface charges that compensate.
- Demonstrates the total electric field and energy are independent of the arbitrary unit cell selection.

### [Antiferroaxial altermagnetism](https://arxiv.org/abs/2602.10641v1)
**Authors:** Yichen Liu, Cheng-Cheng Liu
**Published:** 2026-02-11
**Summary:** Antiferroaxial distortions induce and enable reversible switching of altermagnetism via a trilinear coupling, offering a universal control mechanism for spintronics.

**What's new**
- Introduces antiferroaxial altermagnetism as a prevalent multiferroic mechanism linking structural distortions to altermagnetic order.
- Identifies a symmetry-allowed trilinear coupling that locks altermagnetism to antiferroaxial order, enabling deterministic switching.
- Provides a spin group dictionary mapping Néel vectors to d-, g-, and i-wave altermagnetism for material prediction.
- Validates the theory with tight-binding models and first-principles calculations, and screens databases to identify candidate materials.

### [Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows](https://arxiv.org/abs/2602.11142v1)
**Authors:** Shaswat Garg, Matin Moezzi, Brandon Da Silva
**Published:** 2026-02-11
**Summary:** NF-HIQL improves hierarchical goal-conditioned RL by using expressive normalizing flow policies for better data efficiency and multimodal behavior modeling.

**What's new**
- Replaces Gaussian policies with normalizing flow policies at both hierarchy levels for richer multimodal behavior.
- Provides theoretical guarantees like KL-divergence bounds and PAC-style sample efficiency results.
- Demonstrates superior performance in long-horizon tasks under limited data compared to prior methods.

### [YOR: Your Own Mobile Manipulator for Generalizable Robotics](https://arxiv.org/abs/2602.11150v1)
**Authors:** Manan H Anjaria, Mehmet Enes Erciyes, Vedant Ghatnekar, Neha Navarkar, Haritheja Etukuru, Xiaole Jiang, Kanad Patel, Dhawal Kabra, Nicholas Wojno, Radhika Ajay Prayage, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui
**Published:** 2026-02-11
**Summary:** YOR is an open-source, low-cost (<$10k) mobile manipulator with an omnidirectional base, telescopic lift, and two arms for whole-body mobility and manipulation research.

**What's new**
- Introduces a modular, affordable mobile manipulator design under $10,000 using off-the-shelf components
- Features an omnidirectional base, telescopic vertical lift, and two gripper-equipped arms for whole-body control
- Demonstrates capabilities in bimanual manipulation, coordinated whole-body tasks, and autonomous navigation
- Provides an open-source platform to lower cost barriers for mobile manipulation research

### [DataChef: Cooking Up Optimal Data Recipes for LLM Adaptation via Reinforcement Learning](https://arxiv.org/abs/2602.11089v1)
**Authors:** Yicheng Chen, Zerun Ma, Xinchen Xie, Yining Li, Kai Chen
**Published:** 2026-02-11
**Summary:** DataChef automates the creation of data recipes for LLM adaptation using reinforcement learning, generating recipes that match expert performance on downstream tasks.

**What's new**
- Formulates end-to-end data recipe generation for LLM adaptation, automating the design of data processing pipelines.
- Introduces DataChef-32B, which uses online reinforcement learning with a proxy reward predicting downstream performance.
- Achieves comparable performance to human expert recipes across six held-out tasks, demonstrating practical utility.
- Adapts Qwen3-1.7B-Base to math, scoring 66.7 on AIME'25 and surpassing the original model.

### [Safety Recovery in Reasoning Models Is Only a Few Early Steering Steps Away](https://arxiv.org/abs/2602.11096v1)
**Authors:** Soumya Suvra Ghosal, Souradip Chakraborty, Vaibhav Singh, Furong Huang, Dinesh Manocha, Amrit Singh Bedi
**Published:** 2026-02-11
**Summary:** SafeThink is a lightweight inference-time defense that recovers safety in reasoning models by conditionally injecting a short corrective prefix when safety thresholds are violated.

**What's new**
- Treats safety recovery as a satisficing constraint rather than a maximization objective during inference.
- Monitors reasoning traces with a safety reward model and injects an optimized corrective prefix only when needed.
- Reduces jailbreak attack success rates by 30-60% across multiple benchmarks while preserving reasoning performance.
- Shows safety recovery often requires intervening in only the first 1-3 reasoning steps to redirect generation.

### [Conversational Behavior Modeling Foundation Model With Multi-Level Perception](https://arxiv.org/abs/2602.11065v1)
**Authors:** Dingkun Zhou, Shuchang Pan, Jiachen Lian, Siddharth Banerjee, Sarika Pasumarthy, Dhruv Hebbar, Siddhant Patel, Zeyi Austin Li, Kan Jen Cheng, Sanay Bordia, Krish Patel, Akshaj Gupta, Tingle Li, Gopala Anumanchipalli
**Published:** 2026-02-11
**Summary:** A foundation model for full-duplex dialogue systems that uses a Graph-of-Thoughts to predict speech acts by modeling multi-level conversational perception and intent-to-action pathways.

**What's new**
- Introduces a multi-level perception framework modeling conversational behavior via a hierarchical Graph-of-Thoughts (GoT).
- Formalizes intent-to-action pathway with a novel hierarchical labeling scheme for communicative intents and speech acts.
- Creates a high-quality annotated corpus of event-rich dialogues to train the system.
- Enables real-time prediction of next speech acts with interpretable justifications and dynamic reasoning refinement.

### [Token-Efficient Change Detection in LLM APIs](https://arxiv.org/abs/2602.11083v1)
**Authors:** Timothée Chauvin, Clément Lalanne, Erwan Le Merrer, Jean-Michel Loubes, François Taïani, Gilles Tredan
**Published:** 2026-02-11
**Summary:** A black-box method using 'border inputs' to detect changes in LLM APIs efficiently, reducing costs by 30x compared to existing approaches.

**What's new**
- Introduces 'border inputs' where the model has multiple top output tokens for efficient change detection.
- Proposes B3IT, a strict black-box scheme requiring only output tokens, no model weights or log probabilities.
- Achieves performance comparable to best grey-box methods while reducing costs by 30 times.
- Demonstrates border inputs are easily found for non-reasoning endpoints in extensive experiments.

### [Growth and Transport Properties of InAsSb Nanoflags](https://arxiv.org/abs/2602.11060v1)
**Authors:** Sebastian Serra, Gaurav Shukla, Giada Bucci, Robert Sorodoc, Valentina Zannier, Fabio Beltram, Lucia Sorba, Stefan Heun
**Published:** 2026-02-11
**Summary:** First growth of free-standing InAsSb nanoflags with high mobility, large g-factor, and surface Fermi level pinning for quantum applications.

**What's new**
- First demonstration of high-quality free-standing InAsSb nanoflag growth with tunable composition.
- InAs0.77Sb0.23 nanoflags show a Landé g-factor larger than pure InAs or InSb.
- Mobility comparable to best InAs and InSb nanoflags, suitable for quantum devices.
- Surface Fermi level pinned in conduction band, similar to InAs, promising for superconductor integration.

### [A Gibbs posterior sampler for inverse problem based on prior diffusion model](https://arxiv.org/abs/2602.11059v1)
**Authors:** Jean-François Giovannelli
**Published:** 2026-02-11
**Summary:** A Gibbs sampling method for Bayesian inverse problems using a diffusion model as the prior, ensuring convergence and effective posterior sampling.

**What's new**
- Introduces a Gibbs sampler for posterior sampling in ill-posed inverse problems with diffusion model priors.
- Proposes a novel, unexplored approach that is both effective and remarkably simple to implement.
- Provides theoretical convergence guarantees under clearly identified conditions.
- Validates the method's performance through numerical simulations.

### [GraphSeek: Next-Generation Graph Analytics with LLMs](https://arxiv.org/abs/2602.11052v1)
**Authors:** Maciej Besta, Łukasz Jarmocik, Orest Hrycyna, Shachar Klaiman, Konrad Mączka, Robert Gerstenberger, Jürgen Müller, Piotr Nyczyk, Hubert Niewiadomski, Torsten Hoefler
**Published:** 2026-02-11
**Summary:** GraphSeek is an LLM-powered framework that uses a semantic catalog to plan and execute complex analytics on large, evolving property graphs, improving efficiency and accuracy.

**What's new**
- Introduces a semantic catalog abstraction for planning graph analytics instead of direct NL-to-query generation
- Separates LLM reasoning (Semantic Plane) from deterministic execution (Execution Plane) for scalability
- Achieves high success rates (e.g., 86%) on complex tasks with token-efficient small-context LLMs
- Enables database-grade query execution on large, heterogeneous, and dynamic property graphs

### [Fine-Tuning GPT-5 for GPU Kernel Generation](https://arxiv.org/abs/2602.11000v1)
**Authors:** Ali Tehrani, Yahya Emara, Essam Wissam, Wojciech Paluch, Waleed Atallah, Łukasz Dudziak, Mohamed S. Abdelfattah
**Published:** 2026-02-11
**Summary:** Reinforcement learning fine-tuning of GPT-5 for GPU kernel generation significantly improves correctness and performance over supervised methods and prior models.

**What's new**
- Introduces Makora, an RL environment and toolset for fine-tuning frontier models on GPU kernel generation.
- Fine-tuned GPT-5 achieves 77.0% kernel correctness, a +33.3 percentage point improvement over baseline GPT-5.
- The model outperforms PyTorch TorchInductor on 72.9% of problems with a 2.12x geometric mean speedup.
- Demonstrates RL as a data-efficient alternative to supervised fine-tuning for specialized, data-scarce domains.

### [LoRA-Squeeze: Simple and Effective Post-Tuning and In-Tuning Compression of LoRA Modules](https://arxiv.org/abs/2602.10993v1)
**Authors:** Ivan Vulić, Adam Grycner, Quentin de Laroussilhe, Jonas Pfeiffer
**Published:** 2026-02-11
**Summary:** LoRA-Squeeze compresses LoRA modules after or during training by starting with a high rank and using RSVD to reduce it, improving performance over direct low-rank training.

**What's new**
- Proposes post-hoc compression: train LoRA at high rank, then compress to lower rank via RSVD, often outperforming direct low-rank training
- Introduces in-tuning rank annealing: dynamically reduce LoRA rank during training for optimal size-performance trade-off
- Avoids pre-selection of optimal rank and complex hyper-parameter tuning by learning expressively first then compressing
- Validated across 13 text and 10 vision-language tasks, showing consistent gains in LoRA efficiency and performance

### [FeatureBench: Benchmarking Agentic Coding for Complex Feature Development](https://arxiv.org/abs/2602.10975v1)
**Authors:** Qixing Zhou, Jiacheng Zhang, Haiyang Wang, Rui Hao, Jiahe Wang, Minghao Han, Yuxue Yang, Shuzhe Wu, Feiyang Pan, Lue Fan, Dandan Tu, Zhaoxiang Zhang
**Published:** 2026-02-11
**Summary:** FeatureBench is a benchmark for evaluating LLM agents on complex, multi-commit feature development tasks, using automated test-driven task extraction from real repositories.

**What's new**
- Evaluates end-to-end feature development across multiple commits/PRs, not just single-file bug fixes.
- Automatically derives tasks from repos via test dependency graphs, enabling scalable updates.
- Uses execution-based evaluation with 200 tasks from 24 repos, showing a large gap vs. simpler benchmarks.
- Ensures feature isolation and verifiability, reducing data leakage risks for future agent training.

### [What do people want to fact-check?](https://arxiv.org/abs/2602.10935v1)
**Authors:** Bijean Ghafouri, Dorsaf Sallami, Luca Luceri, Taylor Lynn Curtis, Jean-Francois Godbout, Emilio Ferrara, Reihaneh Rabbany
**Published:** 2026-02-11
**Summary:** Analyzes what people actually ask AI to fact-check, revealing a mismatch between user demand and what fact-checking tools can handle.

**What's new**
- First large-scale study of user-submitted fact-checking requests, analyzing ~2,500 statements from 457 participants.
- Finds users default to simple descriptive claims about the present, with a narrow range of question types.
- Reveals ~25% of requests are for non-empirical statements (e.g., moral judgments), creating a tool-user mismatch.
- Shows standard AI benchmarks (like FEVER) encode a synthetic claim environment misaligned with real user needs.

### [Magneto-optical properties of the neutral silicon-vacancy center in diamond under extreme isotropic strain fields](https://arxiv.org/abs/2602.10690v1)
**Authors:** Meysam Mohseni, Gergő Thiering, Adam Gali
**Published:** 2026-02-11
**Summary:** First-principles study shows extreme isotropic strain can tune the optical and spin properties of the neutral silicon-vacancy center in diamond, enhancing its quantum emission stability.

**What's new**
- Quantifies the center's response to extreme isotropic strain from -80 to 180 GPa using density-functional theory.
- Shows compression blue-shifts the zero-phonon line, stiffens phonons, and increases spin-orbit splitting.
- Reveals tensile strain enhances vibronic effects and can induce symmetry breaking beyond a critical point.
- Establishes calibration relations linking optical and spin observables directly to isotropic strain.

### [Diffusion-Pretrained Dense and Contextual Embeddings](https://arxiv.org/abs/2602.11151v1)
**Authors:** Sedigheh Eslami, Maksim Gaiduk, Markus Krimmel, Louis Milliken, Bo Wang, Denis Bykov
**Published:** 2026-02-11
**Summary:** A family of multilingual embedding models using diffusion-pretrained language models and multi-stage contrastive learning for web-scale retrieval.

**What's new**
- Uses diffusion-pretrained language model backbone for comprehensive bidirectional context in passages.
- Introduces two model types: standard retrieval and contextualized embeddings with global document context.
- Achieves competitive performance on MTEB, MIRACL, BERGEN, and ToolRet benchmarks.
- Sets new records on the ConTEB benchmark with the contextualized embedding model.
- Demonstrates strong performance in real-world, large-scale search scenarios over tens of millions of documents.

### [Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling](https://arxiv.org/abs/2602.11146v1)
**Authors:** Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao, Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo
**Published:** 2026-02-11
**Summary:** A new reward model for aligning diffusion models that works directly on noisy latent states, avoiding expensive pixel-space VLM rewards and improving optimization efficiency.

**What's new**
- Proposes a reward model that operates directly on noisy diffusion latents, eliminating domain mismatch from pixel-space rewards.
- Introduces a noise-calibrated Thurstone likelihood with uncertainty dependent on the diffusion noise level.
- Enables inference-time noise ensembling for test-time scaling and more robust reward signals.
- Achieves performance competitive with state-of-the-art VLMs at a significantly lower computational cost.
- Improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.

### [SCRAPL: Scattering Transform with Random Paths for Machine Learning](https://arxiv.org/abs/2602.11145v1)
**Authors:** Christopher Mitcheltree, Vincent Lostanlen, Emmanouil Benetos, Mathieu Lagrange
**Published:** 2026-02-11
**Summary:** SCRAPL is a stochastic optimization method that speeds up the scattering transform for neural network training by randomly sampling paths, applied to audio synthesis tasks.

**What's new**
- Proposes SCRAPL to efficiently compute scattering transforms via random path sampling for use as a loss in SGD.
- Implements SCRAPL for joint time-frequency scattering to model intermittent auditory textures.
- Applies SCRAPL to unsupervised sound matching for granular synthesis and drum machines.
- Introduces an importance sampling heuristic to adapt SCRAPL to dataset content, improving convergence.

### [GENIUS: Generative Fluid Intelligence Evaluation Suite](https://arxiv.org/abs/2602.11144v1)
**Authors:** Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang
**Published:** 2026-02-11
**Summary:** GENIUS is a benchmark for evaluating Generative Fluid Intelligence in multimodal models, focusing on pattern induction, constraint execution, and contextual adaptation beyond memorized knowledge.

**What's new**
- Introduces a benchmark for Generative Fluid Intelligence, moving beyond tests of memorized knowledge.
- Formalizes GFI as three primitives: inducing implicit patterns, executing ad-hoc constraints, and adapting to contextual knowledge.
- Systematic evaluation of 12 models reveals significant deficits in these fluid reasoning tasks.
- Diagnostic analysis shows failures stem from limited context comprehension, not insufficient generative capability.
- Proposes a training-free attention intervention strategy to help bridge the identified performance gap.

### [LCIP: Loss-Controlled Inverse Projection of High-Dimensional Image Data](https://arxiv.org/abs/2602.11141v1)
**Authors:** Yu Wang, Frederik L. Dennig, Michael Behrisch, Alexandru Telea
**Published:** 2026-02-11
**Summary:** A new inverse projection method that allows user-controlled sweeping through high-dimensional data space, overcoming limitations of fixed surface-like reconstructions.

**What's new**
- Enables sweeping through data space under user control via two intuitive parameters, rather than generating fixed surface-like structures.
- Works generically with any projection technique and dataset, and is simple to implement.
- Demonstrated for image manipulation tasks such as style transfer, supporting data augmentation and analysis.

### [TabICLv2: A better, faster, scalable, and open tabular foundation model](https://arxiv.org/abs/2602.11139v1)
**Authors:** Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan
**Published:** 2026-02-11
**Summary:** TabICLv2 is a new state-of-the-art tabular foundation model that outperforms previous best models through novel synthetic data generation, architectural improvements, and optimized training.

**What's new**
- Novel synthetic data generation engine designed for high pretraining diversity
- Architectural innovations including a new scalable softmax in attention for better generalization
- Optimized pretraining protocols, notably replacing AdamW with the Muon optimizer
- Surpasses RealTabPFN-2.5 performance without tuning while being faster and more memory efficient
- Generalizes effectively to million-scale datasets under 50GB GPU memory constraints

<details><summary>More papers (213)</summary>

**Materials & Physics (cond-mat / comp-ph / chem-ph)**
- [From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent](https://arxiv.org/abs/2602.11123v1)
- [Data-Efficient Multidimensional Free Energy Estimation via Physics-Informed Score Learning](https://arxiv.org/abs/2602.11098v1)
- [Statistical Learning Analysis of Physics-Informed Neural Networks](https://arxiv.org/abs/2602.11097v1)
- [Long-range electrostatics in atomistic machine learning: a physical perspective](https://arxiv.org/abs/2602.11071v1)
- [Machine learning exploration of binding energy distributions of H2O at astrochemically relevant dust grain surfaces](https://arxiv.org/abs/2602.11050v1)
- [Pseudorotation and N-body Forces in an Optical Matter System](https://arxiv.org/abs/2602.11043v1)
- [Initial Guesses for Multicomponent Mean-Field Methods: Assessment and New Developments](https://arxiv.org/abs/2602.11013v1)
- [Noise-balanced multilevel on-the-fly sparse grid surrogates for coupling Monte Carlo models into continuum models with application to heterogeneous catalysis](https://arxiv.org/abs/2602.11006v1)
- [Eliminating Delocalization Error through Localized Orbital Scaling Correction with Orbital Relaxation from Linear Response](https://arxiv.org/abs/2602.11003v1)
- [Coarse-Grained Molecular Dynamics Simulations of Primary Antioxidant-Containing Polymer Melts: Effects of Antioxidant Concentration and Molecular Architecture](https://arxiv.org/abs/2602.10954v1)
- [Scalable Solar-Blind Imaging Enabled by Single-Crystalline Beta-Ga2O3 Membranes on Silicon Backplanes](https://arxiv.org/abs/2602.10941v1)
- [Low magnetic moment and unconventional magneto-transport in half-Heusler alloy CoVGe](https://arxiv.org/abs/2602.10939v1)
- [Annotated digital image correlation displacement fields from fatigue crack growth experiments](https://arxiv.org/abs/2602.10930v1)
- [Connection between $GW$ and Extended Coupled Cluster](https://arxiv.org/abs/2602.10887v1)
- [Staggered Dzyaloshinskii-Moriya and canting angle in centrosymmetric altermagnetic and ferromagnetic phases: influence on the anomalous Hall effect and Weyl points](https://arxiv.org/abs/2602.10879v1)
- [On generating Special Quasirandom Structures: Optimization for the DFT computational efficiency](https://arxiv.org/abs/2602.10872v1)
- [Interlayer-mediated catalyst engineering for ultra-high aspect ratio silicon nanostructures](https://arxiv.org/abs/2602.10860v1)
- [Distorted polyhedral architecture enabled high thermoelectric performance of columnar double halide perovskites Cs2AgPdCl5 and Cs2AgPtCl5](https://arxiv.org/abs/2602.10789v1)
- [A statistical theory of structure in many-particle systems with local interactions](https://arxiv.org/abs/2602.10779v1)
- [Layer-dependent antiferromagnetic Chern and axion insulating states in UOTe](https://arxiv.org/abs/2602.10705v1)
- [Nonlinear dynamics in magnonic Fabry-Pérot resonators: Low-power neuron-like activation and transmission suppression](https://arxiv.org/abs/2602.10650v1)
- [Coarse-Grained Boltzmann Generators](https://arxiv.org/abs/2602.10637v1)
- [Projection-Based Memory Kernel Coupling Theory for Quantum Dynamics: A Stable Framework for Non-Markovian Simulations](https://arxiv.org/abs/2602.10629v1)
- [On the Role of Consistency Between Physics and Data in Physics-Informed Neural Networks](https://arxiv.org/abs/2602.10611v1)
- [Ferroelectric Quantum Point Contact in Twisted Transition Metal Dichalcogenides](https://arxiv.org/abs/2602.10554v1)
- [Probing Plasmonic Oscillations in 2D Moiré Nanocrystal Superlattices by Low-Loss EELS](https://arxiv.org/abs/2602.10534v1)

**AI/ML (cs.AI/cs.LG/stat.ML/cs.CL)**
- [Weight Decay Improves Language Model Plasticity](https://arxiv.org/abs/2602.11137v1)
- [FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight](https://arxiv.org/abs/2602.11136v1)
- [From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers](https://arxiv.org/abs/2602.11130v1)
- [Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2602.11128v1)
- [The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization](https://arxiv.org/abs/2602.11126v1)
- [A Doubly Robust Machine Learning Approach for Disentangling Treatment Effect Heterogeneity with Functional Outcomes](https://arxiv.org/abs/2602.11118v1)
- [Learning to Compose for Cross-domain Agentic Workflow Generation](https://arxiv.org/abs/2602.11114v1)
- [Renet: Principled and Efficient Relaxation for the Elastic Net via Dynamic Objective Selection](https://arxiv.org/abs/2602.11107v1)
- [TEGRA: Text Encoding With Graph and Retrieval Augmentation for Misinformation Detection](https://arxiv.org/abs/2602.11106v1)
- [GameDevBench: Evaluating Agentic Capabilities Through Game Development](https://arxiv.org/abs/2602.11103v1)
- [MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning](https://arxiv.org/abs/2602.11092v1)
- [Can Large Language Models Make Everyone Happy?](https://arxiv.org/abs/2602.11091v1)
- [Direct Learning of Calibration-Aware Uncertainty for Neural PDE Surrogates](https://arxiv.org/abs/2602.11090v1)
- [General Flexible $f$-divergence for Challenging Offline RL Datasets with Low Stochasticity and Diverse Behavior Policies](https://arxiv.org/abs/2602.11087v1)
- [First International StepUP Competition for Biometric Footstep Recognition: Methods, Results and Remaining Challenges](https://arxiv.org/abs/2602.11086v1)
- [GRASP: group-Shapley feature selection for patients](https://arxiv.org/abs/2602.11084v1)
- [SteuerLLM: Local specialized large language model for German tax law analysis](https://arxiv.org/abs/2602.11081v1)
- [In-the-Wild Model Organisms: Mitigating Undesirable Emergent Behaviors in Production LLM Post-Training via Data Attribution](https://arxiv.org/abs/2602.11079v1)
- [Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing](https://arxiv.org/abs/2602.11076v1)
- [Chatting with Images for Introspective Visual Thinking](https://arxiv.org/abs/2602.11073v1)
- [Simultaneous Speech-to-Speech Translation Without Aligned Data](https://arxiv.org/abs/2602.11072v1)
- [Motion Capture is Not the Target Domain: Scaling Synthetic Data for Learning Motion Representations](https://arxiv.org/abs/2602.11064v1)
- [MoToRec: Sparse-Regularized Multimodal Tokenization for Cold-Start Recommendation](https://arxiv.org/abs/2602.11062v1)
- [Divide, Harmonize, Then Conquer It: Shooting Multi-Commodity Flow Problems with Multimodal Language Models](https://arxiv.org/abs/2602.11057v1)
- [Embedding Inversion via Conditional Masked Diffusion Language Models](https://arxiv.org/abs/2602.11047v1)
- [Language Model Inversion through End-to-End Differentiation](https://arxiv.org/abs/2602.11044v1)
- [Characterizing Trainability of Instantaneous Quantum Polynomial Circuit Born Machines](https://arxiv.org/abs/2602.11042v1)
- [Learning Page Order in Shuffled WOO Releases](https://arxiv.org/abs/2602.11040v1)
- [Linguistic Indicators of Early Cognitive Decline in the DementiaBank Pitt Corpus: A Statistical and Machine Learning Study](https://arxiv.org/abs/2602.11028v1)
- [Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting](https://arxiv.org/abs/2602.11024v1)
- [ContactGaussian-WM: Learning Physics-Grounded World Model from Videos](https://arxiv.org/abs/2602.11021v1)
- [When Fusion Helps and When It Breaks: View-Aligned Robustness in Same-Source Financial Imaging](https://arxiv.org/abs/2602.11020v1)
- [OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories](https://arxiv.org/abs/2602.11018v1)
- [From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design](https://arxiv.org/abs/2602.11016v1)
- [CVPL: A Geometric Framework for Post-Hoc Linkage Risk Assessment in Protected Tabular Data](https://arxiv.org/abs/2602.11015v1)
- [ROCKET: Rapid Optimization via Calibration-guided Knapsack Enhanced Truncation for Efficient Model Compression](https://arxiv.org/abs/2602.11008v1)
- [Enhancing Predictability of Multi-Tenant DNN Inference for Autonomous Vehicles' Perception](https://arxiv.org/abs/2602.11004v1)
- [CLI-Gym: Scalable CLI Task Generation via Agentic Environment Inversion](https://arxiv.org/abs/2602.10999v1)
- [The emergence of numerical representations in communicating artificial agents](https://arxiv.org/abs/2602.10996v1)
- [Variational Optimality of Föllmer Processes in Generative Diffusions](https://arxiv.org/abs/2602.10989v1)
- [TVCACHE: A Stateful Tool-Value Cache for Post-Training LLM Agents](https://arxiv.org/abs/2602.10986v1)
- [Sample Efficient Generative Molecular Optimization with Joint Self-Improvement](https://arxiv.org/abs/2602.10984v1)
- [RiemannGL: Riemannian Geometry Changes Graph Deep Learning](https://arxiv.org/abs/2602.10982v1)
- [A Jointly Efficient and Optimal Algorithm for Heteroskedastic Generalized Linear Bandits with Adversarial Corruptions](https://arxiv.org/abs/2602.10971v1)
- [Weighting-Based Identification and Estimation in Graphical Models of Missing Data](https://arxiv.org/abs/2602.10969v1)
- [Healthy Harvests: A Comparative Look at Guava Disease Classification Using InceptionV3](https://arxiv.org/abs/2602.10967v1)
- [MoEEdit: Efficient and Routing-Stable Knowledge Editing for Mixture-of-Experts LLMs](https://arxiv.org/abs/2602.10965v1)
- [Can LLMs Cook Jamaican Couscous? A Study of Cultural Novelty in Recipe Generation](https://arxiv.org/abs/2602.10964v1)
- [Rotary Positional Embeddings as Phase Modulation: Theoretical Bounds on the RoPE Base for Long-Context Transformers](https://arxiv.org/abs/2602.10959v1)
- [Stochastic Parroting in Temporal Attention -- Regulating the Diagonal Sink](https://arxiv.org/abs/2602.10956v1)
- [Search or Accelerate: Confidence-Switched Position Beam Search for Diffusion Language Models](https://arxiv.org/abs/2602.10953v1)
- [Optimal Initialization in Depth: Lyapunov Initialization and Limit Theorems for Deep Leaky ReLU Networks](https://arxiv.org/abs/2602.10949v1)
- [Computational Phenomenology of Temporal Experience in Autism: Quantifying the Emotional and Narrative Characteristics of Lived Unpredictability](https://arxiv.org/abs/2602.10947v1)
- [CMAD: Cooperative Multi-Agent Diffusion via Stochastic Optimal Control](https://arxiv.org/abs/2602.10933v1)
- [Spatial-Morphological Modeling for Multi-Attribute Imputation of Urban Blocks](https://arxiv.org/abs/2602.10923v1)
- [Near-Constant Strong Violation and Last-Iterate Convergence for Online CMDPs via Decaying Safety Margins](https://arxiv.org/abs/2602.10917v1)
- [Traceable, Enforceable, and Compensable Participation: A Participation Ledger for People-Centered AI Governance](https://arxiv.org/abs/2602.10916v1)
- [Blind Gods and Broken Screens: Architecting a Secure, Intent-Centric Mobile Agent Operating System](https://arxiv.org/abs/2602.10915v1)
- [Tuning the burn-in phase in training recurrent neural networks improves their performance](https://arxiv.org/abs/2602.10911v1)
- [SoftMatcha 2: A Fast and Soft Pattern Matcher for Trillion-Scale Corpora](https://arxiv.org/abs/2602.10908v1)
- [Natural Hypergradient Descent: Algorithm Design, Convergence Analysis, and Parallel Implementation](https://arxiv.org/abs/2602.10905v1)
- [Resource-Efficient Model-Free Reinforcement Learning for Board Games](https://arxiv.org/abs/2602.10894v1)
- [Interactive LLM-assisted Curriculum Learning for Multi-Task Evolutionary Policy Search](https://arxiv.org/abs/2602.10891v1)
- [Anomaly Detection with Machine Learning Algorithms in Large-Scale Power Grids](https://arxiv.org/abs/2602.10888v1)
- [The CLEF-2026 FinMMEval Lab: Multilingual and Multimodal Evaluation of Financial AI Systems](https://arxiv.org/abs/2602.10886v1)
- [Reinforcing Chain-of-Thought Reasoning with Self-Evolving Rubrics](https://arxiv.org/abs/2602.10885v1)
- [Diagnosing Structural Failures in LLM-Based Evidence Extraction for Meta-Analysis](https://arxiv.org/abs/2602.10881v1)
- [C-MOP: Integrating Momentum and Boundary-Aware Clustering for Enhanced Prompt Evolution](https://arxiv.org/abs/2602.10874v1)
- [FedPS: Federated data Preprocessing via aggregated Statistics](https://arxiv.org/abs/2602.10870v1)
- [The Sample Complexity of Uniform Approximation for Multi-Dimensional CDFs and Fixed-Price Mechanisms](https://arxiv.org/abs/2602.10868v1)
- [Deep Learning of Compositional Targets with Hierarchical Spectral Methods](https://arxiv.org/abs/2602.10867v1)
- [ICA: Information-Aware Credit Assignment for Visually Grounded Long-Horizon Information-Seeking Agents](https://arxiv.org/abs/2602.10863v1)
- [Automated Model Design using Gated Neuron Selection in Telecom](https://arxiv.org/abs/2602.10854v1)
- [Time Series Foundation Models for Energy Load Forecasting on Consumer Hardware: A Multi-Dimensional Zero-Shot Benchmark](https://arxiv.org/abs/2602.10848v1)
- [Enhancing Multivariate Time Series Forecasting with Global Temporal Retrieval](https://arxiv.org/abs/2602.10847v1)
- [SynergyKGC: Reconciling Topological Heterogeneity in Knowledge Graph Completion via Topology-Aware Synergy](https://arxiv.org/abs/2602.10845v1)
- [SimuScene: Training and Benchmarking Code Generation to Simulate Physical Scenarios](https://arxiv.org/abs/2602.10840v1)
- [Training-Induced Bias Toward LLM-Generated Content in Dense Retrieval](https://arxiv.org/abs/2602.10833v1)
- [I can tell whether you are a Native Hawlêri Speaker! How ANN, CNN, and RNN perform in NLI-Native Language Identification](https://arxiv.org/abs/2602.10832v1)
- [Self-Supervised Learning for Speaker Recognition: A study and review](https://arxiv.org/abs/2602.10829v1)
- [Flow caching for autoregressive video generation](https://arxiv.org/abs/2602.10825v1)
- [Adaptive Sampling for Private Worst-Case Group Optimization](https://arxiv.org/abs/2602.10820v1)
- [RePO: Bridging On-Policy Learning and Off-Policy Knowledge through Rephrasing Policy Optimization](https://arxiv.org/abs/2602.10819v1)
- [Beyond Confidence: The Rhythms of Reasoning in Generative Models](https://arxiv.org/abs/2602.10816v1)
- [Why Does RL Generalize Better Than SFT? A Data-Centric Perspective on VLM Post-Training](https://arxiv.org/abs/2602.10815v1)
- [See, Plan, Snap: Evaluating Multimodal GUI Agents in Scratch](https://arxiv.org/abs/2602.10814v1)
- [PELLI: Framework to effectively integrate LLMs for quality software generation](https://arxiv.org/abs/2602.10808v1)
- [Integrating Generative AI-enhanced Cognitive Systems in Higher Education: From Stakeholder Perceptions to a Conceptual Framework considering the EU AI Act](https://arxiv.org/abs/2602.10802v1)
- [Deep Learning-based Method for Expressing Knowledge Boundary of Black-Box LLM](https://arxiv.org/abs/2602.10801v1)
- [RSHallu: Dual-Mode Hallucination Evaluation for Remote-Sensing Multimodal Large Language Models with Domain-Tailored Mitigation](https://arxiv.org/abs/2602.10799v1)
- [PRISM: Parallel Residual Iterative Sequence Model](https://arxiv.org/abs/2602.10796v1)
- [Transport, Don't Generate: Deterministic Geometric Flows for Combinatorial Optimization](https://arxiv.org/abs/2602.10794v1)
- [Semi-Supervised Cross-Domain Imitation Learning](https://arxiv.org/abs/2602.10793v1)
- [Bayesian Signal Component Decomposition via Diffusion-within-Gibbs Sampling](https://arxiv.org/abs/2602.10792v1)
- [VulReaD: Knowledge-Graph-guided Software Vulnerability Reasoning and Detection](https://arxiv.org/abs/2602.10787v1)
- [Kill it with FIRE: On Leveraging Latent Space Directions for Runtime Backdoor Mitigation in Deep Neural Networks](https://arxiv.org/abs/2602.10780v1)
- [LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers](https://arxiv.org/abs/2602.10770v1)
- [Collaborative Threshold Watermarking](https://arxiv.org/abs/2602.10765v1)
- [Exploring the impact of adaptive rewiring in Graph Neural Networks](https://arxiv.org/abs/2602.10754v1)
- [Predicting integers from continuous parameters](https://arxiv.org/abs/2602.10751v1)
- [SecureScan: An AI-Driven Multi-Layer Framework for Malware and Phishing Detection Using Logistic Regression and Threat Intelligence Integration](https://arxiv.org/abs/2602.10750v1)
- [Spectral-Spatial Contrastive Learning Framework for Regression on Hyperspectral Data](https://arxiv.org/abs/2602.10745v1)
- [Self-Supervised Image Super-Resolution Quality Assessment based on Content-Free Multi-Model Oriented Representation Learning](https://arxiv.org/abs/2602.10744v1)
- [Kalman Linear Attention: Parallel Bayesian Filtering For Efficient Language Modelling and State Tracking](https://arxiv.org/abs/2602.10743v1)
- [Reinforced Curriculum Pre-Alignment for Domain-Adaptive VLMs](https://arxiv.org/abs/2602.10740v1)
- [Calliope: A TTS-based Narrated E-book Creator Ensuring Exact Synchronization, Privacy, and Layout Fidelity](https://arxiv.org/abs/2602.10735v1)
- [Macaron: Controlled, Human-Written Benchmark for Multilingual and Multicultural Reasoning via Template-Filling](https://arxiv.org/abs/2602.10732v1)
- [Rising Multi-Armed Bandits with Known Horizons](https://arxiv.org/abs/2602.10727v1)
- [A Diffusion-Based Generative Prior Approach to Sparse-view Computed Tomography](https://arxiv.org/abs/2602.10722v1)
- [SnapMLA: Efficient Long-Context MLA Decoding via Hardware-Aware FP8 Quantized Pipelining](https://arxiv.org/abs/2602.10718v1)
- [RE-LLM: Refining Empathetic Speech-LLM Responses by Integrating Emotion Nuance](https://arxiv.org/abs/2602.10716v1)
- [Locomo-Plus: Beyond-Factual Cognitive Memory Evaluation Framework for LLM Agents](https://arxiv.org/abs/2602.10715v1)
- [A Non-asymptotic Analysis for Learning and Applying a Preconditioner in MCMC](https://arxiv.org/abs/2602.10714v1)
- [Cross-Sectional Asset Retrieval via Future-Aligned Soft Contrastive Learning](https://arxiv.org/abs/2602.10711v1)
- [Interpretable Graph-Level Anomaly Detection via Contrast with Normal Prototypes](https://arxiv.org/abs/2602.10708v1)
- [Reducing Estimation Uncertainty Using Normalizing Flows and Stratification](https://arxiv.org/abs/2602.10706v1)
- [A Unified Experimental Architecture for Informative Path Planning: from Simulation to Deployment with GuadalPlanner](https://arxiv.org/abs/2602.10702v1)
- [Spend Search Where It Pays: Value-Guided Structured Sampling and Optimization for Generative Recommendation](https://arxiv.org/abs/2602.10699v1)
- [AugVLA-3D: Depth-Driven Feature Augmentation for Vision-Language-Action Models](https://arxiv.org/abs/2602.10698v1)
- [Fast and Large-Scale Unbalanced Optimal Transport via its Semi-Dual and Adaptive Gradient Methods](https://arxiv.org/abs/2602.10697v1)
- [Robust Assortment Optimization from Observational Data](https://arxiv.org/abs/2602.10696v1)
- [VESPO: Variational Sequence-Level Soft Policy Optimization for Stable Off-Policy LLM Training](https://arxiv.org/abs/2602.10693v1)
- [Convergence Rates for Distribution Matching with Sliced Optimal Transport](https://arxiv.org/abs/2602.10691v1)
- [OmniVL-Guard: Towards Unified Vision-Language Forgery Detection and Grounding via Balanced RL](https://arxiv.org/abs/2602.10687v1)
- [Beyond Task Performance: A Metric-Based Analysis of Sequential Cooperation in Heterogeneous Multi-Agent Destructive Foraging](https://arxiv.org/abs/2602.10685v1)
- [A solvable high-dimensional model where nonlinear autoencoders learn structure invisible to PCA while test loss misaligns with generalization](https://arxiv.org/abs/2602.10680v1)
- [TwiFF (Think With Future Frames): A Large-Scale Dataset for Dynamic Visual Reasoning](https://arxiv.org/abs/2602.10675v1)
- [Domain Knowledge Guided Bayesian Optimization For Autonomous Alignment Of Complex Scientific Instruments](https://arxiv.org/abs/2602.10670v1)
- [From Diet to Free Lunch: Estimating Auxiliary Signal Properties using Dynamic Pruning Masks in Speech Enhancement Networks](https://arxiv.org/abs/2602.10666v1)
- [Targeted Syntactic Evaluation of Language Models on Georgian Case Alignment](https://arxiv.org/abs/2602.10661v1)
- [Benchmarks Are Not That Out of Distribution: Word Overlap Predicts Performance](https://arxiv.org/abs/2602.10657v1)
- [UMEM: Unified Memory Extraction and Management Framework for Generalizable Memory](https://arxiv.org/abs/2602.10652v1)
- [Evaluation metrics for temporal preservation in synthetic longitudinal patient data](https://arxiv.org/abs/2602.10643v1)
- [Beyond Kemeny Medians: Consensus Ranking Distributions Definition, Properties and Statistical Learning](https://arxiv.org/abs/2602.10640v1)
- [OmniSapiens: A Foundation Model for Social Behavior Processing via Heterogeneity-Aware Relative Policy Optimization](https://arxiv.org/abs/2602.10635v1)
- [The Neurosymbolic Frontier of Nonuniform Ellipticity: Formalizing Sharp Schauder Theory via Topos-Theoretic Reasoning Models](https://arxiv.org/abs/2602.10632v1)
- [Generative clinical time series models trained on moderate amounts of patient data are privacy preserving](https://arxiv.org/abs/2602.10631v1)
- [To Think or Not To Think, That is The Question for Large Reasoning Models in Theory of Mind Tasks](https://arxiv.org/abs/2602.10625v1)
- [A Vision-Language Foundation Model for Zero-shot Clinical Collaboration and Automated Concept Discovery in Dermatology](https://arxiv.org/abs/2602.10624v1)
- [Mitigating Reward Hacking in RLHF via Bayesian Non-negative Reward Modeling](https://arxiv.org/abs/2602.10623v1)
- [How Do Decoder-Only LLMs Perceive Users? Rethinking Attention Masking for User Representation Learning](https://arxiv.org/abs/2602.10622v1)
- [ISD-Agent-Bench: A Comprehensive Benchmark for Evaluating LLM-based Instructional Design Agents](https://arxiv.org/abs/2602.10620v1)
- [Pupillometry and Brain Dynamics for Cognitive Load in Working Memory](https://arxiv.org/abs/2602.10614v1)
- [Highly Adaptive Principal Component Regression](https://arxiv.org/abs/2602.10613v1)
- [Online Causal Kalman Filtering for Stable and Effective Policy Optimization](https://arxiv.org/abs/2602.10609v1)
- [Bayesian Inference of Contextual Bandit Policies via Empirical Likelihood](https://arxiv.org/abs/2602.10608v1)
- [Hierarchical Zero-Order Optimization for Deep Neural Networks](https://arxiv.org/abs/2602.10607v1)
- [Step 3.5 Flash: Open Frontier-Level Intelligence with 11B Active Parameters](https://arxiv.org/abs/2602.10604v1)
- [dnaHNet: A Scalable and Hierarchical Foundation Model for Genomic Sequence Learning](https://arxiv.org/abs/2602.10603v1)
- [Learning Mixture Density via Natural Gradient Expectation Maximization](https://arxiv.org/abs/2602.10602v1)
- [Neuro-symbolic Action Masking for Deep Reinforcement Learning](https://arxiv.org/abs/2602.10598v1)
- [Roughness-Informed Federated Learning](https://arxiv.org/abs/2602.10595v1)
- [Flow-Enabled Generalization to Human Demonstrations in Few-Shot Imitation Learning](https://arxiv.org/abs/2602.10594v1)
- [TRACE: Theoretical Risk Attribution under Covariate-shift Effects](https://arxiv.org/abs/2602.10588v1)
- [Deep Bootstrap](https://arxiv.org/abs/2602.10587v1)
- [Neural Additive Experts: Context-Gated Experts for Controllable Model Additivity](https://arxiv.org/abs/2602.10585v1)
- [When Gradient Clipping Becomes a Control Mechanism for Differential Privacy in Deep Learning](https://arxiv.org/abs/2602.10584v1)
- [Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets](https://arxiv.org/abs/2602.10583v1)
- [LLM-Based Scientific Equation Discovery via Physics-Informed Token-Regularized Policy Optimization](https://arxiv.org/abs/2602.10576v1)
- [MetaphorStar: Image Metaphor Understanding and Reasoning with End-to-End Visual Reinforcement Learning](https://arxiv.org/abs/2602.10575v1)
- [Gauss-Newton Unlearning for the LLM Era](https://arxiv.org/abs/2602.10568v1)
- [Online Min-Max Optimization: From Individual Regrets to Cumulative Saddle Points](https://arxiv.org/abs/2602.10565v1)
- [When to Memorize and When to Stop: Gated Recurrent Memory for Long-Context Reasoning](https://arxiv.org/abs/2602.10560v1)
- [LAP: Language-Action Pre-Training Enables Zero-shot Cross-Embodiment Transfer](https://arxiv.org/abs/2602.10556v1)
- [Contrastive Learning for Multi Label ECG Classification with Jaccard Score Based Sigmoid Loss](https://arxiv.org/abs/2602.10553v1)
- [C^2ROPE: Causal Continuous Rotary Positional Encoding for 3D Large Multimodal-Models Reasoning](https://arxiv.org/abs/2602.10551v1)
- [Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Text Guidance](https://arxiv.org/abs/2602.10549v1)
- [RealHD: A High-Quality Dataset for Robust Detection of State-of-the-Art AI-Generated Images](https://arxiv.org/abs/2602.10546v1)
- [$μ$pscaling small models: Principled warm starts and hyperparameter transfer](https://arxiv.org/abs/2602.10545v1)
- [Bridging the Compression-Precision Paradox: A Hybrid Architecture for Clinical EEG Report Generation with Guaranteed Measurement Accuracy](https://arxiv.org/abs/2602.10544v1)
- [Predictive-State Communication: Innovation Coding and Reconciliation under Delay](https://arxiv.org/abs/2602.10542v1)
- [Solving PDEs in One Shot via Fourier Features with Exact Analytical Derivatives](https://arxiv.org/abs/2602.10541v1)
- [What Makes Value Learning Efficient in Residual Reinforcement Learning?](https://arxiv.org/abs/2602.10539v1)
- [Why Agentic Theorem Prover Works: A Statistical Provability Theory of Mathematical Reasoning Models](https://arxiv.org/abs/2602.10538v1)

_…and 13 more._

</details>

<details><summary>Search definitions</summary>

- Materials & Physics (cond-mat / comp-ph / chem-ph) — `cat:cond-mat.mtrl-sci OR cat:physics.comp-ph OR cat:physics.chem-ph`
- AI/ML (cs.AI/cs.LG/stat.ML/cs.CL) — `cat:cs.AI OR cat:cs.LG OR cat:stat.ML OR cat:cs.CL`

</details>
