---
title: "Daily Issue (JST): 2026-02-08"
---

# Daily Issue (JST): 2026-02-08

## What's New Today

**AI Advances in Continual Learning, Multi-Agent Systems, and Efficient Inference**

Research today focuses on overcoming key bottlenecks in AI deployment, from catastrophic forgetting in large models to the computational cost of inference. New methods for parameter-efficient continual learning, dynamic multi-agent coordination, and parallel decoding architectures show promising paths forward. Meanwhile, foundational studies probe the generalization of diffusion models and the structural biases of transformers at initialization.

**Highlights**
- Share method enables strict continual learning for large models via a single, shared low-rank subspace.
- DyTopo framework improves multi-agent reasoning with dynamic, semantic-matching communication graphs.
- DFlash speculative decoding uses a block diffusion model for parallel drafting to reduce LLM latency.
- Study reveals transformers exhibit strong, systematic token preferences even at random initialization.
- Diffusion model generalization is characterized by a data-dependent ridge manifold alignment process.
- Orthogonal Self-Attention proposed to combat rank collapse in skipless transformer architectures.

**Themes**
- AI: Efficient Model Adaptation & Continual Learning
- AI: Multi-Agent Reasoning & Coordination
- AI: Accelerated Inference & Speculative Decoding
- AI: Foundational Model Analysis & Mechanics
- AI: Novel Architectures & Training Methods
- Materials: Nanoscale Measurement & Epitaxy Studies

**Keywords**
- continual learning
- multi-agent systems
- speculative decoding
- transformer mechanics
- diffusion models
- parameter-efficient tuning
- low-rank adaptation
- reinforcement learning
- out-of-distribution detection
- model merging
- van der Waals epitaxy
- thermal transport

**Total new papers:** 124

## Featured Papers

### [Shared LoRA Subspaces for almost Strict Continual Learning](https://arxiv.org/abs/2602.06043v1)
**Authors:** Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille
**Published:** 2026-02-05
**Summary:** Share enables strict continual learning by dynamically updating a single shared low-rank subspace across tasks, reducing parameters and memory while preventing catastrophic forgetting.

**What's new**
- Proposes a single shared low-rank subspace that evolves across tasks, enabling forward knowledge transfer without data replay.
- Achieves up to 100x parameter reduction and 281x memory savings compared to traditional LoRA methods.
- Validated across diverse modalities including image classification, NLP, 3D pose estimation, and text-to-image generation.
- Replaces hundreds of task-specific LoRA adapters with one model, supporting scalable asynchronous continual learning.

### [DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching](https://arxiv.org/abs/2602.06039v1)
**Authors:** Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao
**Published:** 2026-02-05
**Summary:** DyTopo is a multi-agent reasoning framework that dynamically reconstructs a sparse communication graph each round via semantic matching of agent needs and offers, guided by a manager.

**What's new**
- Dynamic per-round routing replaces fixed communication patterns with sparse directed graphs matched to stage-dependent needs.
- Agents output lightweight natural-language need/offer descriptors; semantic matching routes private messages along induced edges.
- Consistently outperforms strongest baselines across code generation and mathematical reasoning benchmarks (avg. +6.2).
- Provides interpretable coordination traces via evolving communication graphs, enabling inspection of pathway reconfiguration.

### [DFlash: Block Diffusion for Flash Speculative Decoding](https://arxiv.org/abs/2602.06036v1)
**Authors:** Jian Chen, Yesheng Liang, Zhijian Liu
**Published:** 2026-02-05
**Summary:** DFlash accelerates LLM inference via speculative decoding using a parallel block diffusion draft model, achieving over 6x speedup.

**What's new**
- Introduces a block diffusion model for parallel drafting, replacing sequential autoregressive draft models.
- Conditions the draft model on target model features to improve output quality and acceptance rates.
- Achieves over 6x lossless acceleration, outperforming state-of-the-art methods like EAGLE-3.

### [Diffusion Model's Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold](https://arxiv.org/abs/2602.06021v1)
**Authors:** Ye He, Yitong Qiu, Molei Tao
**Published:** 2026-02-05
**Summary:** This paper characterizes diffusion model generalization by showing generated data follows a 'reach-align-slide' process around a data-dependent ridge manifold, linking training errors to specific normal/tangent motions.

**What's new**
- Proposes a log-density ridge manifold to quantify what diffusion models generate beyond memorization.
- Identifies a 'reach-align-slide' inference process: trajectories approach, orient toward, and slide along the ridge manifold.
- Links training errors to specific normal (push/pull) and tangent (sliding) motions of the generated data.
- Shows how inductive bias emerges from architecture and training accuracy, illustrated via a random feature model.
- Validates theory with experiments on synthetic multimodal data and MNIST latent diffusion in low/high dimensions.

### [Orthogonal Self-Attention](https://arxiv.org/abs/2602.05996v1)
**Authors:** Leo Zhang, James Martens
**Published:** 2026-02-05
**Summary:** Orthogonal Self-Attention (OSA) replaces softmax attention with an orthogonal attention matrix via matrix exponential to prevent rank collapse and improve training stability in skipless Transformers.

**What's new**
- Replaces softmax with orthogonal attention via matrix exponential of a skew-symmetric matrix
- Prevents rank collapse and poor conditioning in Transformers without skip connections or normalization
- Achieves linear complexity with sequence length by exploiting low-rank structure of query-key values
- Provides an initialization scheme proven to ensure well-conditioned Jacobians

### [Diamond Maps: Efficient Reward Alignment via Stochastic Flow Maps](https://arxiv.org/abs/2602.05993v1)
**Authors:** Peter Holderrieth, Douglas Chen, Luca Eyring, Ishin Shah, Giri Anantharaman, Yutong He, Zeynep Akata, Tommi Jaakkola, Nicholas Matthew Boffi, Max Simchowitz
**Published:** 2026-02-05
**Summary:** Diamond Maps are stochastic flow map models designed for efficient post-training reward alignment, enabling scalable adaptation to arbitrary preferences at inference time.

**What's new**
- Introduces stochastic flow map models that amortize many simulation steps into a single-step sampler while preserving stochasticity.
- Enables efficient and consistent estimation of the value function, making search, SMC, and guidance scalable for reward alignment.
- Achieves stronger reward alignment performance and scales better than existing methods via distillation from GLASS Flows.

### [Layer-wise LoRA fine-tuning: a similarity metric approach](https://arxiv.org/abs/2602.05988v1)
**Authors:** Keith Ando Ogawa, Bruno Lopes Yamamoto, Lucas Lauton de Alcantara, Lucas Pellicer, Rosimeire Pereira Costa, Edson Bollis, Anna Helena Reali Costa, Artur Jordao
**Published:** 2026-02-05
**Summary:** A method to reduce trainable parameters in LoRA fine-tuning by selecting only the most relevant layers based on changes in internal representations, maintaining performance.

**What's new**
- Systematically selects a few layers for LoRA fine-tuning using similarity metrics on internal representations.
- Reduces trainable parameters by up to 50% compared to standard LoRA while preserving predictive performance.
- Shows negligible performance drop on GLUE benchmarks for encoder-only models and improvements on some decoder-only tasks.
- Extends effectively to multimodal models, achieving competitive results with fewer fine-tuned layers.

### [Self-Improving Multilingual Long Reasoning via Translation-Reasoning Integrated Training](https://arxiv.org/abs/2602.05940v1)
**Authors:** Junxiao Liu, Zhijun Wang, Yixiao Li, Zhejian Lai, Liqian Huang, Xin Huang, Xue Han, Junlan Feng, Shujian Huang
**Published:** 2026-02-05
**Summary:** A self-improving framework that integrates translation training into multilingual reasoning to enhance both question understanding and response generation without external data.

**What's new**
- Proposes TRIT, integrating translation training directly into multilingual reasoning model training
- Improves multilingual reasoning accuracy by 7 percentage points on MMATH without extra data
- Enhances cross-lingual question alignment by over 10 percentage points
- Boosts translation quality for both math questions and general text up to 8.4 COMET points

### [Approximation of Log-Partition Function in Policy Mirror Descent Induces Implicit Regularization for LLM Post-Training](https://arxiv.org/abs/2602.05933v1)
**Authors:** Zhenghao Xu, Qin Lu, Changlong Yu, Tuo Zhao
**Published:** 2026-02-05
**Summary:** PMD-mean approximates the log-partition function in policy mirror descent with mean reward, inducing implicit mixed KL-chi^2 regularization for more stable and efficient LLM post-training.

**What's new**
- Approximates log-partition function using mean reward under sampling policy, avoiding costly estimation
- Reveals implicit adaptive mixed KL-chi^2 regularization that constrains large probability changes
- Produces more conservative updates when expected rewards are low, enhancing robustness
- Demonstrates superior performance, stability, and time efficiency on math reasoning tasks

### [Transformers Are Born Biased: Structural Inductive Biases at Random Initialization and Their Practical Consequences](https://arxiv.org/abs/2602.05927v1)
**Authors:** Siquan Li, Yao Tong, Haonan Wang, Tianyang Hu
**Published:** 2026-02-05
**Summary:** Randomly initialized transformers exhibit strong, systematic token preferences due to architectural biases, which persist through training and enable model fingerprinting.

**What's new**
- Shows untrained transformers have extreme, seed-dependent token prediction biases from hidden representation contraction.
- Mechanistically explains bias via MLP activation asymmetry and self-attention amplification at initialization.
- Introduces SeedPrint, a method to fingerprint models based on persistent initialization biases after training.
- Links attention's positional contraction bias to the causal emergence of attention sinks, suggesting control methods.

### [Platform and Framework for Time-Resolved Nanoscale Thermal Transport Measurements in STEM](https://arxiv.org/abs/2602.05911v1)
**Authors:** Mairi McCauley, Joel Martis, Ondrej L. Krivanek, Ben Plotkin-Swing, Andreas Mittelberger, Tolga Wagner, Hüseyin Çelik, Grigory Kornilov, Meng Zhao, Matthias Meffert, Luca Piazza, Tracy C. Lovejoy, Guillaume Radtke, Christoph Koch, Benedikt Haas
**Published:** 2026-02-05
**Summary:** A laser-excitation system integrated into a STEM enables time-resolved nanoscale thermal transport measurements via EELS, with ~50 ns temporal resolution.

**What's new**
- Integrated fiber-coupled laser via modified aperture, enabling flexible holder geometries and large tilt angles without polepiece optics.
- Synchronized pulsed laser with gated electron detector for ~50 ns temporal resolution at <10 meV energy resolution.
- Extracts thermal parameters via detailed balance and heat diffusion modeling, demonstrated on amorphous carbon films.

### [Codified Finite-state Machines for Role-playing](https://arxiv.org/abs/2602.05905v1)
**Authors:** Letian Peng, Yupeng Hou, Kun Zhou, Jingbo Shang
**Published:** 2026-02-05
**Summary:** A framework that uses LLMs to automatically convert character profiles into finite-state machines, improving consistency in role-playing by modeling latent states and probabilistic transitions.

**What's new**
- Introduces Codified Finite-State Machines (CFSMs) that automatically generate FSMs from text profiles using LLM-based coding.
- Extends CFSMs to probabilistic versions (CPFSMs) to model uncertain transitions as probability distributions.
- Demonstrates effectiveness in both structured tasks and open-ended role-playing scenarios, outperforming baseline methods.

### [Universal approximation with signatures of non-geometric rough paths](https://arxiv.org/abs/2602.05898v1)
**Authors:** Mihriban Ceylan, Anna P. Kwossek, David J. Prömel
**Published:** 2026-02-05
**Summary:** Extends universal approximation to signatures of non-geometric rough paths by augmenting paths with time and bracket terms, enabling approximation of continuous functionals on rough path spaces.

**What's new**
- Proves universal approximation for signatures of non-weakly geometric rough paths via time and bracket augmentation
- Constructs signatures for paths extended by quadratic variation using Föllmer-type pathwise integration
- Obtains approximation results for signatures of continuous semimartingales with Itô quadratic variation
- Enables pathwise Itô, Stratonovich, and backward Itô integration frameworks in signature methods
- Demonstrates applications in mathematical finance for model calibration and option pricing

### [Large-scale Score-based Variational Posterior Inference for Bayesian Deep Neural Networks](https://arxiv.org/abs/2602.05873v1)
**Authors:** Minyoung Kim
**Published:** 2026-02-05
**Summary:** A scalable score-based variational inference method for Bayesian neural networks that avoids reparametrized sampling and works with large models like Vision Transformers.

**What's new**
- Proposes a score-based variational inference alternative to ELBO for Bayesian neural networks, avoiding reparametrized sampling.
- Uses a learning objective combining score matching loss and proximal penalty, enabling noisy unbiased mini-batch scores via stochastic gradients.
- Scalable to large networks like Vision Transformers and supports richer variational density families.
- Demonstrates effectiveness on visual recognition and time-series forecasting benchmarks with large-scale deep networks.

### [AMDAT: An Open-Source Molecular Dynamics Analysis Toolkit for Supercooled Liquids, Glass-Forming Materials, and Complex Fluids](https://arxiv.org/abs/2602.05865v1)
**Authors:** Pierre Kawak, William F. Drayer, David S. Simmons
**Published:** 2026-02-05
**Summary:** AMDAT is an open-source C++ toolkit for high-performance analysis of molecular dynamics trajectories in amorphous, glassy, and polymeric materials.

**What's new**
- Open-source C++ toolkit focused on amorphous, glassy, and polymer materials analysis.
- Enables high-performance, long-timescale analysis via in-memory trajectory handling.
- Implements exponential time sampling for efficient dynamic analysis.
- Provides workflows for key observables like structure factors and scattering functions.

### [CFRecs: Counterfactual Recommendations on Real Estate User Listing Interaction Graphs](https://arxiv.org/abs/2602.05861v1)
**Authors:** Seyedmasoud Mousavi, Ruomeng Xu, Xiaojing Zhu
**Published:** 2026-02-05
**Summary:** CFRecs is a framework that uses counterfactual graph learning to provide actionable recommendations for real estate by suggesting minimal changes to user-listing interactions to achieve desired outcomes.

**What's new**
- Transforms counterfactual graph explanations into actionable insights for recommender systems.
- Uses a two-stage GNN and Graph-VAE architecture to propose minimal structural and attribute changes.
- Applies the framework to Zillow's user-listing interaction graph for real estate recommendations.
- Demonstrates effectiveness on real-world data, offering a new perspective via counterfactual reasoning.

### [BABE: Biology Arena BEnchmark](https://arxiv.org/abs/2602.05857v1)
**Authors:** Junting Zhou, Jin Chen, Linfeng Hao, Denghui Cao, Zheyu Wang, Qiguang Chen, Chaoyou Fu, Jiaze Chen, Yuchen Wu, Ge Zhang, Mingxuan Wang, Wenhao Huang, Tong Yang
**Published:** 2026-02-05
**Summary:** BABE is a benchmark for evaluating AI's ability to reason like biologists by integrating experimental results and contextual knowledge from real research papers.

**What's new**
- Assesses AI's experimental reasoning in biology, not just basic knowledge or dialogue.
- Built from peer-reviewed papers and real studies for authentic scientific complexity.
- Focuses on causal reasoning and cross-scale inference tasks.
- Measures ability to integrate experimental data with background knowledge.

### [Visualizing the loss landscapes of physics-informed neural networks](https://arxiv.org/abs/2602.05849v1)
**Authors:** Conor Rowan, Finn Murphy-Blanchard
**Published:** 2026-02-05
**Summary:** This paper visualizes and compares the loss landscapes of physics-informed neural networks, finding they are often smooth and convex near solutions, similar to traditional ML problems.

**What's new**
- Extends loss landscape visualization techniques from image classification to physics-informed machine learning.
- Empirically compares landscapes from Deep Ritz and squared residual (strong form) physics loss formulations.
- Finds physics-informed loss landscapes are often smooth, well-conditioned, and convex near solutions.
- Challenges prevailing intuitions about the complexity of optimization in scientific machine learning.

### [Reinforcement World Model Learning for LLM-based Agents](https://arxiv.org/abs/2602.05842v1)
**Authors:** Xiao Yu, Baolin Peng, Ruize Xu, Yelong Shen, Pengcheng He, Suman Nath, Nikhil Singh, Jiangfeng Gao, Zhou Yu
**Published:** 2026-02-05
**Summary:** A self-supervised method that trains LLM-based agents to learn action-conditioned world models by aligning simulated next states with real environment states in an embedding space.

**What's new**
- Uses sim-to-real gap rewards to align simulated and observed next states in a pre-trained embedding space
- Focuses on semantic equivalence over token-level fidelity to avoid model collapse
- More robust to reward hacking compared to LLM-as-a-judge approaches
- Combined with task rewards, outperforms direct task-success reward RL by significant margins
- Achieves strong performance on ALFWorld and τ² Bench without expert data

### [Principled Confidence Estimation for Deep Computed Tomography](https://arxiv.org/abs/2602.05812v1)
**Authors:** Matteo Gätzner, Johannes Kirschner
**Published:** 2026-02-05
**Summary:** A framework for confidence estimation in deep-learning CT reconstruction with theoretical coverage guarantees, using a realistic Poisson noise model.

**What's new**
- Provides confidence regions with theoretical coverage guarantees for deep CT reconstructions.
- Applies to U-Nets, ensembles, and diffusion models under a realistic Poisson noise forward model.
- Shows deep methods yield tighter confidence regions than classical ones without losing coverage.
- Enables detection of hallucinations and offers interpretable visualizations of uncertainty.

### [Non-Stationary Inventory Control with Lead Times](https://arxiv.org/abs/2602.05799v1)
**Authors:** Nele H. Amiri, Sean R. Sinclair, Maximiliano Udenio
**Published:** 2026-02-05
**Summary:** Online algorithms for non-stationary inventory control with unknown demand, showing sharp performance separation between backlogging/lost-sales with zero lead time vs. lost-sales with positive lead times.

**What's new**
- Proposes adaptive online algorithms optimizing base-stock policies for non-stationary demand with performance guarantees via dynamic regret
- Reveals sharp separation: backlogging and zero-lead-time lost-sales systems adapt without extra loss, unlike lost-sales with positive lead times
- Leverages convexity and one-sided feedback to enable counterfactual policy evaluation despite demand censoring
- Establishes fundamental limitations from delayed replenishment and censored feedback in lost-sales systems with lead times
- Simulations show significant outperformance over existing benchmarks across inventory models

### [Price of universality in vector quantization is at most 0.11 bit](https://arxiv.org/abs/2602.05790v1)
**Authors:** Alina Harbuzova, Or Ordentlich, Yury Polyanskiy
**Published:** 2026-02-05
**Summary:** A universal vector quantization codebook exists that is near-optimal for all data statistics, losing at most 0.11 bit per dimension compared to a data-adapted optimal codebook.

**What's new**
- Proves existence of a universal codebook for weight quantization, independent of data statistics.
- Shows this universal codebook is within 0.11 bit per dimension of the optimal data-adapted waterfilling solution.
- Result implies existence of a net in R^n that is a nearly-optimal covering for all Hilbert norms.
- The proof is non-constructive, leaving open the challenge of designing such a universal codebook.

### [Variational Speculative Decoding: Rethinking Draft Training from Token Likelihood to Sequence Acceptance](https://arxiv.org/abs/2602.05774v1)
**Authors:** Xiandong Zou, Jianshu Li, Jing Huang, Pan Zhou
**Published:** 2026-02-05
**Summary:** VSD reformulates speculative decoding draft training as variational inference to maximize acceptance of draft paths, boosting decoding speedup.

**What's new**
- Formulates draft training as variational inference over latent draft paths to maximize target-model acceptance probability.
- Introduces an EM procedure with MCMC sampling from an oracle-filtered posterior and weighted likelihood maximization.
- Uses Adaptive Rejection Weighting and Confidence-Aware Regularization to enhance proposal quality and reduce variance.
- Achieves up to 9.6% speedup over prior methods like EAGLE-3 in experiments across LLMs and MLLMs.

### [Pseudo-Invertible Neural Networks](https://arxiv.org/abs/2602.06042v1)
**Authors:** Yamit Ehrlich, Nimrod Berman, Assaf Shocher
**Published:** 2026-02-05
**Summary:** A neural network architecture with a tractable pseudo-inverse, enabling zero-shot solution of nonlinear inverse problems via geometric back-projection.

**What's new**
- Introduces Surjective Pseudo-invertible Neural Networks (SPNN) with a defined nonlinear pseudo-inverse.
- Formalizes Non-Linear Back-Projection (NLBP) to enforce consistency for nonlinear mappings.
- Extends zero-shot inverse problem solving from linear to complex nonlinear degradations.
- Enables semantic control over generative outputs without retraining the diffusion prior.

<details><summary>More papers (100)</summary>

**Materials & Physics (cond-mat / comp-ph / chem-ph)**
- [Topological piezomagnetic effect in two-dimensional Dirac quadrupole altermagnets](https://arxiv.org/abs/2602.05894v1)
- [Low-temperature spin dynamics in LAFO thin films: from cubic anisotropy to TLS-limited coherence](https://arxiv.org/abs/2602.05889v1)
- [Generalized Path Reweighting and History-Dependent Free Energies](https://arxiv.org/abs/2602.05793v1)
- [The near-continuum mechanism for extended Boltzmann theory: the non-equilibrium relaxation](https://arxiv.org/abs/2602.05775v1)
- [Questionable van der Waals Epitaxy of Non-Layered Materials on Fluorophlogopite Mica: The Case of ScN](https://arxiv.org/abs/2602.05741v1)

**AI/ML (cs.AI/cs.LG/stat.ML/cs.CL)**
- [CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction](https://arxiv.org/abs/2602.06038v1)
- [Can vision language models learn intuitive physics from interaction?](https://arxiv.org/abs/2602.06033v1)
- [AP-OOD: Attention Pooling for Out-of-Distribution Detection](https://arxiv.org/abs/2602.06031v1)
- [PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling](https://arxiv.org/abs/2602.06030v1)
- [Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference](https://arxiv.org/abs/2602.06029v1)
- [Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory](https://arxiv.org/abs/2602.06025v1)
- [Learning Event-Based Shooter Models from Virtual Reality Experiments](https://arxiv.org/abs/2602.06023v1)
- [Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering](https://arxiv.org/abs/2602.06022v1)
- [Mechanisms of AI Protein Folding in ESMFold](https://arxiv.org/abs/2602.06020v1)
- [Multi-Token Prediction via Self-Distillation](https://arxiv.org/abs/2602.06019v1)
- [A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies](https://arxiv.org/abs/2602.06015v1)
- [Optimism Stabilizes Thompson Sampling for Adaptive Inference](https://arxiv.org/abs/2602.06014v1)
- [GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?](https://arxiv.org/abs/2602.06013v1)
- [AgenticPay: A Multi-Agent LLM Negotiation System for Buyer-Seller Transactions](https://arxiv.org/abs/2602.06008v1)
- [Speech Emotion Recognition Leveraging OpenAI's Whisper Representations and Attentive Pooling Methods](https://arxiv.org/abs/2602.06000v1)
- [On Computation and Reinforcement Learning](https://arxiv.org/abs/2602.05999v1)
- [Causal Inference on Stopped Random Walks in Online Advertising](https://arxiv.org/abs/2602.05997v1)
- [DSB: Dynamic Sliding Block Scheduling for Diffusion LLMs](https://arxiv.org/abs/2602.05992v1)
- [RISE-Video: Can Video Generators Decode Implicit World Rules?](https://arxiv.org/abs/2602.05986v1)
- [Geographically-aware Transformer-based Traffic Forecasting for Urban Motorway Digital Twins](https://arxiv.org/abs/2602.05983v1)
- [Clifford Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.05977v1)
- [SAGE: Benchmarking and Improving Retrieval for Deep Research Agents](https://arxiv.org/abs/2602.05975v1)
- [Characterizing Human Semantic Navigation in Concept Production as Trajectories in Embedding Space](https://arxiv.org/abs/2602.05971v1)
- [Inverse Depth Scaling From Most Layers Being Similar](https://arxiv.org/abs/2602.05970v1)
- [A Hybrid Data-Driven Algorithm for Real-Time Friction Force Estimation in Hydraulic Cylinders](https://arxiv.org/abs/2602.05967v1)
- [LSA: Localized Semantic Alignment for Enhancing Temporal Consistency in Traffic Video Generation](https://arxiv.org/abs/2602.05966v1)
- [Learning to Share: Selective Memory for Efficient Parallel Agentic Systems](https://arxiv.org/abs/2602.05965v1)
- [Discrete diffusion samplers and bridges: Off-policy algorithms and applications in latent spaces](https://arxiv.org/abs/2602.05961v1)
- [Better Source, Better Flow: Learning Condition-Dependent Source Distribution for Flow Matching](https://arxiv.org/abs/2602.05951v1)
- [Breaking Symmetry Bottlenecks in GNN Readouts](https://arxiv.org/abs/2602.05950v1)
- [$f$-GRPO and Beyond: Divergence-Based Reinforcement Learning Algorithms for General LLM Alignment](https://arxiv.org/abs/2602.05946v1)
- [Orthogonal Model Merging](https://arxiv.org/abs/2602.05943v1)
- [Dimensionality Reduction on Riemannian Manifolds in Data Analysis](https://arxiv.org/abs/2602.05936v1)
- [Tuning Out-of-Distribution (OOD) Detectors Without Given OOD Data](https://arxiv.org/abs/2602.05935v1)
- [Polyglots or Multitudes? Multilingual LLM Answers to Value-laden Multiple-Choice Questions](https://arxiv.org/abs/2602.05932v1)
- [Compound Deception in Elite Peer Review: A Failure Mode Taxonomy of 100 Fabricated Citations at NeurIPS 2025](https://arxiv.org/abs/2602.05930v1)
- [KV-CoRE: Benchmarking Data-Dependent Low-Rank Compressibility of KV-Caches in LLMs](https://arxiv.org/abs/2602.05929v1)
- [Quantum Reinforcement Learning with Transformers for the Capacitated Vehicle Routing Problem](https://arxiv.org/abs/2602.05920v1)
- [Chunky Post-Training: Data Driven Failures of Generalization](https://arxiv.org/abs/2602.05910v1)
- [Verification of the Implicit World Model in a Generative Model via Adversarial Sequences](https://arxiv.org/abs/2602.05903v1)
- [Regularized Calibration with Successive Rounding for Post-Training Quantization](https://arxiv.org/abs/2602.05902v1)
- [Stop Rewarding Hallucinated Steps: Faithfulness-Aware Step-Level Reinforcement Learning for Small Reasoning Models](https://arxiv.org/abs/2602.05897v1)
- [Parity, Sensitivity, and Transformers](https://arxiv.org/abs/2602.05896v1)
- [ContextBench: A Benchmark for Context Retrieval in Coding Agents](https://arxiv.org/abs/2602.05892v1)
- [DFPO: Scaling Value Modeling via Distributional Flow towards Robust and Generalizable LLM Post-Training](https://arxiv.org/abs/2602.05890v1)
- [Metric Hedonic Games on the Line](https://arxiv.org/abs/2602.05888v1)
- [Escaping Local Minima Provably in Non-convex Matrix Sensing: A Deterministic Framework via Simulated Lifting](https://arxiv.org/abs/2602.05887v1)
- [Dr. Kernel: Reinforcement Learning Done Right for Triton Kernel Generations](https://arxiv.org/abs/2602.05885v1)
- [Neural Implicit 3D Cardiac Shape Reconstruction from Sparse CT Angiography Slices Mimicking 2D Transthoracic Echocardiography Views](https://arxiv.org/abs/2602.05884v1)
- [A Guide to Large Language Models in Modeling and Simulation: From Core Techniques to Critical Challenges](https://arxiv.org/abs/2602.05883v1)
- [EuroLLM-22B: Technical Report](https://arxiv.org/abs/2602.05879v1)
- [Agent2Agent Threats in Safety-Critical LLM Assistants: A Human-Centric Taxonomy](https://arxiv.org/abs/2602.05877v1)
- [Beyond Manual Planning: Seating Allocation for Large Organizations](https://arxiv.org/abs/2602.05875v1)
- [xList-Hate: A Checklist-Based Framework for Interpretable and Generalizable Hate Speech Detection](https://arxiv.org/abs/2602.05874v1)
- [Wedge Sampling: Efficient Tensor Completion with Nearly-Linear Sample Complexity](https://arxiv.org/abs/2602.05869v1)
- [Constrained Group Relative Policy Optimization](https://arxiv.org/abs/2602.05863v1)
- [Distribution-free two-sample testing with blurred total variation distance](https://arxiv.org/abs/2602.05862v1)
- [DLM-Scope: Mechanistic Interpretability of Diffusion Language Models via Sparse Autoencoders](https://arxiv.org/abs/2602.05859v1)
- [A Hybrid Autoencoder for Robust Heightmap Generation from Fused Lidar and Depth Data for Humanoid Robot Locomotion](https://arxiv.org/abs/2602.05855v1)
- [RRAttention: Dynamic Block Sparse Attention via Per-Head Round-Robin Shifts for Long-Context Inference](https://arxiv.org/abs/2602.05853v1)
- [Exact Recovery in the Data Block Model](https://arxiv.org/abs/2602.05852v1)
- [DARWIN: Dynamic Agentically Rewriting Self-Improving Network](https://arxiv.org/abs/2602.05848v1)
- [OmniVideo-R1: Reinforcing Audio-visual Reasoning with Query Intention and Modality Attention](https://arxiv.org/abs/2602.05847v1)
- [Optimal scaling laws in learning hierarchical multi-index models](https://arxiv.org/abs/2602.05846v1)
- [OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://arxiv.org/abs/2602.05843v1)
- [FHAIM: Fully Homomorphic AIM For Private Synthetic Data Generation](https://arxiv.org/abs/2602.05838v1)
- [Synthesizing Realistic Test Data without Breaking Privacy](https://arxiv.org/abs/2602.05833v1)
- [Learning Compact Boolean Networks](https://arxiv.org/abs/2602.05830v1)
- [TKG-Thinker: Towards Dynamic Reasoning over Temporal Knowledge Graphs via Agentic Reinforcement Learning](https://arxiv.org/abs/2602.05818v1)
- [Interpreting Manifolds and Graph Neural Embeddings from Internet of Things Traffic Flows](https://arxiv.org/abs/2602.05817v1)
- [Where Does Warm-Up Come From? Adaptive Scheduling for Norm-Constrained Optimizers](https://arxiv.org/abs/2602.05813v1)
- [STProtein: predicting spatial protein expression from multi-omics data](https://arxiv.org/abs/2602.05811v1)
- [Bifrost: Steering Strategic Trajectories to Bridge Contextual Gaps for Self-Improving Agents](https://arxiv.org/abs/2602.05810v1)
- [NEX: Neuron Explore-Exploit Scoring for Label-Free Chain-of-Thought Selection and Model Ranking](https://arxiv.org/abs/2602.05805v1)
- [Learning False Discovery Rate Control via Model-Based Neural Networks](https://arxiv.org/abs/2602.05798v1)
- [Classification Under Local Differential Privacy with Model Reversal and Model Averaging](https://arxiv.org/abs/2602.05797v1)
- [FiMI: A Domain-Specific Language Model for Indian Finance Ecosystem](https://arxiv.org/abs/2602.05794v1)
- [Allocentric Perceiver: Disentangling Allocentric Reasoning from Egocentric Visual Priors via Frame Instantiation](https://arxiv.org/abs/2602.05789v1)
- [Bagging-Based Model Merging for Robust General Text Embeddings](https://arxiv.org/abs/2602.05787v1)
- [Selecting Hyperparameters for Tree-Boosting](https://arxiv.org/abs/2602.05786v1)
- [ReText: Text Boosts Generalization in Image-Based Person Re-identification](https://arxiv.org/abs/2602.05785v1)
- [Distributional Reinforcement Learning with Diffusion Bridge Critics](https://arxiv.org/abs/2602.05783v1)
- [Automated Customization of LLMs for Enterprise Code Repositories Using Semantic Scopes](https://arxiv.org/abs/2602.05780v1)
- [How Controlling the Variance can Improve Training Stability of Sparsely Activated DNNs and CNNs](https://arxiv.org/abs/2602.05779v1)
- [Cross-Domain Offline Policy Adaptation via Selective Transition Correction](https://arxiv.org/abs/2602.05776v1)
- [Different Time, Different Language: Revisiting the Bias Against Non-Native Speakers in GPT Detectors](https://arxiv.org/abs/2602.05769v1)
- [PMT Waveform Simulation and Reconstruction with Conditional Diffusion Network](https://arxiv.org/abs/2602.05767v1)
- [RL-VLA$^3$: Reinforcement Learning VLA Accelerating via Full Asynchronism](https://arxiv.org/abs/2602.05765v1)
- [RocqSmith: Can Automatic Optimization Forge Better Proof Agents?](https://arxiv.org/abs/2602.05762v1)
- [LongR: Unleashing Long-Context Reasoning via Reinforcement Learning with Dense Utility Rewards](https://arxiv.org/abs/2602.05758v1)
- [TimelyFreeze: Adaptive Parameter Freezing Mechanism for Pipeline Parallelism](https://arxiv.org/abs/2602.05754v1)
- [How to Achieve the Intended Aim of Deep Clustering Now, without Deep Learning](https://arxiv.org/abs/2602.05749v1)
- [LeakBoost: Perceptual-Loss-Based Membership Inference Attack](https://arxiv.org/abs/2602.05748v1)
- [Learning to Inject: Automated Prompt Injection via Reinforcement Learning](https://arxiv.org/abs/2602.05746v1)
- [Fast Rates for Nonstationary Weighted Risk Minimization](https://arxiv.org/abs/2602.05742v1)

</details>

<details><summary>Search definitions</summary>

- Materials & Physics (cond-mat / comp-ph / chem-ph) — `cat:cond-mat.mtrl-sci OR cat:physics.comp-ph OR cat:physics.chem-ph`
- AI/ML (cs.AI/cs.LG/stat.ML/cs.CL) — `cat:cs.AI OR cat:cs.LG OR cat:stat.ML OR cat:cs.CL`

</details>
