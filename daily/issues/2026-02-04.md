---
title: "Daily Issue (JST): 2026-02-04"
---

# Daily Issue (JST): 2026-02-04

## What's New Today

**AI Advances in Reasoning, Alignment, and Agents; Physics Insights on Materials and Symmetry**

Today's research spans robust AI agent frameworks and novel methods for aligning language models with complex human preferences. In parallel, materials science reveals critical measurement errors in thin films and explores the interplay between AI and physical symmetries. Cross-domain work includes generative models for molecular transition states and a physical interpretation of backpropagation.

**Highlights**
- RACO framework aligns LLMs with conflicting objectives without explicit reward models.
- Re-TRAC agent framework uses structured state summaries to improve search over ReAct.
- Measurement of thin film conductivity can have unbounded systematic error from finite electrode conductivity.
- FragmentFlow scales transition state generation for large molecules via a divide-and-conquer approach.
- Dyadic Backpropagation is derived as the finite-time relaxation of a physical dynamical system.
- Review explores how AI representation learning can identify, encode, or diagnose symmetry-induced constraints.

**Themes**
- AI: Multi-Objective Alignment & Preference Optimization
- AI: Agentic Frameworks & Memory
- Materials/Physics: Measurement & Simulation
- Cross-Domain: AI for Scientific Discovery
- AI: Reasoning & Model Interpretability
- AI: Efficient Training & Inference

**Keywords**
- reward-free alignment
- agent memory
- KV cache compression
- grain boundary migration
- transition state generation
- symmetry learning
- reasoning evaluation
- neural operators
- LLM quantization
- federated learning
- self-supervised learning
- uncertainty quantification

**Total new papers:** 234

## Featured Papers

### [Reward-free Alignment for Conflicting Objectives](https://arxiv.org/abs/2602.02495v1)
**Authors:** Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin
**Published:** 2026-02-02
**Summary:** A reward-free alignment method (RACO) uses clipped conflict-averse gradient descent on pairwise preferences to handle multiple conflicting objectives in LLMs without explicit reward models.

**What's new**
- Proposes RACO, a reward-free framework using pairwise preference data for multi-objective LLM alignment.
- Introduces a clipped variant of conflict-averse gradient descent to resolve gradient conflicts.
- Provides convergence guarantees to Pareto-critical points respecting user-specified objective weights.
- Shows clipping can strictly improve convergence rate in the two-objective setting.
- Demonstrates better Pareto trade-offs on summarization and safety tasks across multiple LLM families.

### [RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents](https://arxiv.org/abs/2602.02486v1)
**Authors:** Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, Ruichun Ma, Bei Liu, Yifan Yang, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Xin Geng, Baining Guo
**Published:** 2026-02-02
**Summary:** Re-TRAC is a framework for LLM-based research agents that compresses search trajectories into structured states to enable cross-trajectory exploration, reducing redundancy and improving performance.

**What's new**
- Introduces structured state representation summarizing evidence, uncertainties, failures, and future plans after each search trajectory.
- Enables cross-trajectory exploration by conditioning subsequent searches on prior states, allowing revisiting and branching.
- Shows 15-20% performance gains over ReAct on BrowseComp with frontier LLMs and reduces tool calls and tokens across rounds.
- Proposes Re-TRAC-aware supervised fine-tuning for smaller models, achieving state-of-the-art results at comparable scales.

### [Unbounded Systematic Error in Thin Film Conductivity Measurements](https://arxiv.org/abs/2602.02418v1)
**Authors:** Yongyi Gao, Hio-Ieng Un, Yuxuan Huang, Henning Sirringhaus, Ian E. Jacobs
**Published:** 2026-02-02
**Summary:** Four-point conductivity measurements in thin films can produce unbounded systematic errors due to finite metal electrode conductivity, leading to arbitrarily high reported values.

**What's new**
- Identifies unbounded systematic errors in four-point thin film conductivity measurements from finite metal electrode conductivity.
- Shows errors can cause arbitrarily high measured conductivity at modest true values, relevant for conducting polymers.
- Explains likely cause of literature reports of extremely high conductivities in conducting polymers.
- Characterizes geometric factors controlling errors and provides guidelines for accurate device architectures.

### [ROG: Retrieval-Augmented LLM Reasoning for Complex First-Order Queries over Knowledge Graphs](https://arxiv.org/abs/2602.02382v1)
**Authors:** Ziyan Zhang, Chao Wang, Zhuo Chen, Chiyi Li, Kai Song
**Published:** 2026-02-02
**Summary:** ROG is a retrieval-augmented LLM framework that decomposes complex first-order logic queries over knowledge graphs into stepwise, evidence-grounded sub-queries for more accurate reasoning.

**What's new**
- Decomposes multi-operator queries into single-operator sub-queries grounded in retrieved neighborhood evidence.
- Caches and reuses intermediate answer sets across reasoning steps to improve consistency.
- Replaces learned embedding operators with retrieval-grounded, step-wise LLM inference.
- Shows largest gains on high-complexity and negation-heavy query types versus embedding baselines.

### [EvalQReason: A Framework for Step-Level Reasoning Evaluation in Large Language Models](https://arxiv.org/abs/2602.02295v1)
**Authors:** Shaima Ahmad Freja, Ferhat Ozgur Catak, Betul Yurdem, Chunming Rong
**Published:** 2026-02-02
**Summary:** EvalQReason is a framework that evaluates LLM reasoning quality by analyzing step-level probability distributions, using algorithms to measure local coherence and global alignment without human annotation.

**What's new**
- Introduces Consecutive Step Divergence (CSD) and Step-to-Final Convergence (SFC) algorithms to quantify reasoning via step-level probability distributions.
- Shows CSD features enable strong correctness classification, with sequential neural models achieving F1=0.88 and ROC-AUC=0.97.
- Reveals reasoning dynamics are domain-specific, with clear patterns in math but minimal signals in medical tasks.
- Provides a scalable, annotation-free method for process-aware evaluation of LLM reasoning reliability.

### [Backpropagation as Physical Relaxation: Exact Gradients in Finite Time](https://arxiv.org/abs/2602.02281v1)
**Authors:** Antonino Emanuele Scurria
**Published:** 2026-02-02
**Summary:** Shows backpropagation emerges exactly as finite-time relaxation of a physical dynamical system, deriving exact gradients in 2L steps for an L-layer network.

**What's new**
- Formulates backpropagation as saddle-point dynamics of a global energy functional on a doubled state space
- Proves unit-step Euler discretization recovers standard backpropagation exactly in 2L steps with no approximations
- Unlike prior methods, guarantees exact gradients in finite time without requiring symmetric weights or asymptotic convergence
- Establishes backpropagation as digitally optimized shadow of continuous physical relaxation for analog substrates

### [Artificial Intelligence and Symmetries: Learning, Encoding, and Discovering Structure in Physical Data](https://arxiv.org/abs/2602.02351v1)
**Authors:** Veronica Sanz
**Published:** 2026-02-02
**Summary:** This review explores how machine learning, especially representation learning, can identify, encode, and diagnose symmetry structures in physical data without imposing them as prior constraints.

**What's new**
- Focuses on data-driven methods and latent representation learning, particularly variational autoencoders, rather than architectures with built-in symmetries.
- Discusses how symmetries reduce dataset dimensionality and how this manifests in the self-organization of generative model latent spaces.
- Reviews case studies from geometric systems and particle physics to analyze the limits of inferring symmetries without explicit inductive biases.

### [Training LLMs for Divide-and-Conquer Reasoning Elevates Test-Time Scalability](https://arxiv.org/abs/2602.02477v1)
**Authors:** Xiao Liang, Zhong-Zhi Li, Zhenghao Lin, Eric Hancheng Jiang, Hengyuan Zhang, Yelong Shen, Kai-Wei Chang, Ying Nian Wu, Yeyun Gong, Weizhu Chen
**Published:** 2026-02-02
**Summary:** An RL framework trains LLMs to use divide-and-conquer reasoning, improving performance and test-time scalability on complex tasks compared to chain-of-thought.

**What's new**
- Identifies misalignment between standard LLM training and divide-and-conquer inference, limiting its potential.
- Proposes end-to-end RL framework to train decomposition and solution steps jointly.
- Enables models to break problems into subproblems, solve them, then integrate solutions.
- Achieves significant gains over chain-of-thought on competition benchmarks (e.g., +8.6% Pass@1).
- Demonstrates higher performance ceiling and stronger test-time scalability.

### [Cross-Lingual Stability of LLM Judges Under Controlled Generation: Evidence from Finno-Ugric Languages](https://arxiv.org/abs/2602.02287v1)
**Authors:** Isaac Chung, Linda Freienthal
**Published:** 2026-02-02
**Summary:** Controlled generation experiments show LLM evaluation judges produce unstable rankings across Finno-Ugric languages for pragmatic tasks, revealing measurement unreliability rather than true model differences.

**What's new**
- Introduces a controlled generation protocol to isolate measurement variance from true model performance differences in cross-lingual evaluation
- Finds systematic ranking instabilities: surface metrics are stable but pragmatic judgments show rank inversions across Estonian, Finnish, Hungarian
- Demonstrates zero-shot judge transfer fails for discourse-level assessment in morphologically rich languages
- Provides diagnostic probe: evaluation methods failing under identical generation conditions signal transfer failure before deployment
- Releases generation protocol, synthetic data, and framework for replication across language families

### [OmniCode: A Benchmark for Evaluating Software Engineering Agents](https://arxiv.org/abs/2602.02262v1)
**Authors:** Atharv Sonwane, Eng-Shen Tu, Wei-Chung Lu, Claas Beger, Carter Larsen, Debjit Dhar, Rachel Chen, Ronit Pattanayak, Tuan Anh Dang, Guohao Chen, Gloria Geng, Kevin Ellis, Saikat Dutta
**Published:** 2026-02-02
**Summary:** OmniCode is a new benchmark for evaluating LLM-powered coding agents across diverse software engineering tasks like bug fixing, test generation, code review, and style fixing in Python, Java, and C++.

**What's new**
- Introduces 1794 tasks spanning bug fixing, test generation, code review fixing, and style fixing across Python, Java, and C++.
- Tasks are manually validated to eliminate ill-defined problems and synthetically crafted to avoid data leakage.
- Reveals that current agents like SWE-Agent perform poorly on tasks like Test Generation and in languages like Java and C++.
- Provides a new framework for synthetically generating diverse software tasks from limited real-world data.

### [Live-Evo: Online Evolution of Agentic Memory from Continuous Feedback](https://arxiv.org/abs/2602.02369v1)
**Authors:** Yaolun Zhang, Yiran Wu, Yijiong Yu, Qingyun Wu, Huazheng Wang
**Published:** 2026-02-02
**Summary:** Live-Evo is an online self-evolving memory system for LLM agents that continuously updates memory from feedback, improving performance on dynamic tasks.

**What's new**
- Introduces an online evolution pipeline for agent memory using continuous feedback, unlike static train/test approaches.
- Decouples memory into an Experience Bank and a Meta-Guideline Bank for adaptive task-solving.
- Manages memory via experience weights reinforced by helpful feedback and decayed if stale or misleading.
- Demonstrates significant gains on live benchmarks over 10 weeks and transfers well to deep-research tasks.

### [HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos](https://arxiv.org/abs/2602.02473v1)
**Authors:** Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui, Qifeng Chen, Jingbo Wang, Jiangmiao Pang, Ping Tan
**Published:** 2026-02-02
**Summary:** HumanX is a framework that converts human videos into generalizable interaction skills for humanoid robots without task-specific rewards, enabling agile tasks like sports maneuvers.

**What's new**
- Generates diverse robot interaction data from videos via XGen, enabling scalable augmentation without task-specific reward engineering.
- Learns generalizable skills via XMimic, acquiring 10 skills across domains like basketball and football from single video demos.
- Achieves zero-shot transfer to a physical Unitree G1 humanoid, performing complex maneuvers like fadeaway jumpshots without external perception.
- Shows over 8x higher generalization success than prior methods in interactive tasks like sustained human-robot passing sequences.

### [Large Language Models for Mental Health: A Multilingual Evaluation](https://arxiv.org/abs/2602.02440v1)
**Authors:** Nishat Raihan, Sadiya Sayara Chowdhury Puspo, Ana-Maria Bucur, Stevie Chancellor, Marcos Zampieri
**Published:** 2026-02-02
**Summary:** This paper evaluates proprietary and open-source LLMs on multilingual mental health datasets, finding strong performance on original data but declines on machine-translated versions, with variation by language.

**What's new**
- Evaluates LLMs on eight multilingual mental health datasets, including machine-translated versions, across zero-shot, few-shot, and fine-tuned settings.
- Finds proprietary and fine-tuned open-source LLMs often surpass conventional NLP baselines and achieve state-of-the-art results on original data.
- Shows performance on machine-translated data is generally lower, with decline severity varying by language family and typology.
- Highlights LLMs' strengths in non-English mental health tasks and limitations from translation-induced structural or lexical mismatches.

### [Indications of Belief-Guided Agency and Meta-Cognitive Monitoring in Large Language Models](https://arxiv.org/abs/2602.02467v1)
**Authors:** Noam Steinmetz Yalon, Ariel Goldstein, Liad Mudrik, Mor Geva
**Published:** 2026-02-02
**Summary:** This paper provides empirical evidence that large language models exhibit belief-guided agency and meta-cognitive monitoring, aligning with a key indicator of consciousness in AI systems.

**What's new**
- Introduces a metric to quantify the dominance of latent beliefs during model generation.
- Shows external manipulations systematically modulate the model's internal belief formation.
- Demonstrates belief formation causally drives the model's action selection.
- Finds models can monitor and report their own internal belief states.
- Lays methodological groundwork for studying agency and meta-cognition in LLMs.

### [Segment to Focus: Guiding Latent Action Models in the Presence of Distractors](https://arxiv.org/abs/2602.02259v1)
**Authors:** Hamza Adnan, Matthew T. Jackson, Alexey Zakharov
**Published:** 2026-02-02
**Summary:** MaskLAM improves Latent Action Models by using segmentation masks to focus reconstruction loss on the agent, filtering out distracting background noise for better performance.

**What's new**
- Introduces MaskLAM, a lightweight training modification for Latent Action Models (LAMs) using segmentation masks.
- Uses masks from pretrained models to weight reconstruction loss, prioritizing agent over background without architectural changes.
- Demonstrates up to 4x reward increase and 3x latent action quality improvement on MuJoCo tasks with distracting noise.
- Addresses LAMs' critical challenge of disentangling action-relevant features from action-correlated background distractors.

### [SurvKAN: A Fully Parametric Survival Model Based on Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02179v1)
**Authors:** Marina Mastroleo, Alberto Archetti, Federico Mastroleo, Matteo Matteucci
**Published:** 2026-02-02
**Summary:** SurvKAN is a fully parametric survival model using Kolmogorov-Arnold Networks to predict time-to-event outcomes without the proportional hazards assumption, offering both high accuracy and interpretability.

**What's new**
- Introduces a fully parametric survival model that removes the proportional hazards constraint of classical Cox models.
- Treats time as an explicit input to a KAN, enabling direct prediction of the log-hazard function via end-to-end training.
- Preserves interpretability through learnable univariate functions that show how individual features influence risk over time.
- Demonstrates competitive or superior performance to classical and state-of-the-art baselines in concordance and calibration.

### [Transfer Learning Through Conditional Quantile Matching](https://arxiv.org/abs/2602.02358v1)
**Authors:** Yikun Zhang, Steven Wilkins-Reeves, Wesley Lee, Aude Hofleitner
**Published:** 2026-02-02
**Summary:** A transfer learning method for regression that aligns source and target domains via conditional quantile matching to augment scarce target data, improving prediction with theoretical guarantees.

**What's new**
- Proposes conditional quantile matching to calibrate source-generated responses to a target domain without restrictive shift assumptions.
- Provides theoretical excess risk bounds showing improved performance over target-only empirical risk minimization.
- Establishes new convergence rates for the quantile matching estimator, governing transfer bias-variance tradeoff.
- Demonstrates consistent accuracy gains over baselines in simulations and real data applications.

### [Structure Enables Effective Self-Localization of Errors in LLMs](https://arxiv.org/abs/2602.02416v1)
**Authors:** Ankur Samanta, Akshayaa Magesh, Ayush Jain, Kavosh Asadi, Youliang Yu, Daniel Jiang, Boris Vidolov, Kaveh Hassani, Paul Sajda, Jalaj Bhandari, Yonathan Efroni
**Published:** 2026-02-02
**Summary:** A prompting method structures reasoning into discrete steps, enabling LLMs to localize errors and self-correct via iterative backtracking and resampling.

**What's new**
- Introduces structured reasoning as discrete, coherent thought steps for reliable error localization, unlike unstructured chain-of-thought.
- Proposes Thought-ICS, a self-correction framework that iteratively generates and verifies thoughts, backtracking from errors to resample alternatives.
- Achieves 20-40% self-correction lift with oracle verification and outperforms baselines in fully autonomous settings without external feedback.

### [More Than a Quick Glance: Overcoming the Greedy Bias in KV-Cache Compression](https://arxiv.org/abs/2602.02199v1)
**Authors:** Aryan Sood, Tanvi Sharma, Vansh Agrawal
**Published:** 2026-02-02
**Summary:** LASER-KV is a new KV-cache compression method that uses a block-wise accumulation strategy to maintain long-context performance, outperforming prior methods by up to 10%.

**What's new**
- Introduces LASER-KV, a framework using Layer Accumulated Selection with Exact-LSH Recall for KV compression.
- Employs a block-wise accumulation strategy with a protection divisor, isolating compression effects from sliding window artifacts.
- Shows prior methods degrade 15-30% on long-context tasks, while LASER-KV maintains stable performance with up to 10% higher accuracy.
- Challenges the assumption that attention scores alone are a sufficient proxy for token utility in compression.

### [Implicit neural representation of textures](https://arxiv.org/abs/2602.02354v1)
**Authors:** Albert Kwok, Zheyuan Hu, Dounia Hammou
**Published:** 2026-02-02
**Summary:** Proposes using implicit neural representations (INRs) as continuous, memory-efficient texture maps, analyzing their quality/speed trade-offs and applications like mipmapping.

**What's new**
- Introduces neural networks as continuous texture INRs operating on UV space, replacing discrete representations.
- Demonstrates strong image quality with favorable memory usage and rendering inference times.
- Analyzes the trade-off balance between quality, memory, and speed objectives.
- Explores applications in real-time rendering and downstream tasks like mipmap fitting.
- Investigates INR-space generation for texture-related operations.

### [Self-Supervised Learning from Structural Invariance](https://arxiv.org/abs/2602.02381v1)
**Authors:** Yipeng Zhang, Hafez Ghaemi, Jungyoon Lee, Shahab Bakhtiari, Eilif B. Muller, Laurent Charlin
**Published:** 2026-02-02
**Summary:** AdaSSL improves self-supervised learning by modeling conditional uncertainty in one-to-many data mappings via a variational latent variable, applicable to both contrastive and distillation objectives.

**What's new**
- Introduces a latent variable to capture conditional uncertainty in one-to-many data mappings common in generative processes like video frames.
- Derives a variational lower bound on mutual information between paired embeddings, yielding a simple regularization term for SSL objectives.
- Proposes AdaSSL, a method applicable to both contrastive and distillation-based SSL frameworks.
- Demonstrates versatility in causal representation learning, fine-grained image understanding, and video world modeling.
- Addresses limitations of existing SSL methods that struggle to flexibly capture uncertainty when data pairs have multiple valid targets.

### [MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training](https://arxiv.org/abs/2602.02494v1)
**Authors:** Dulhan Jayalath, Oiwi Parker Jones
**Published:** 2026-02-02
**Summary:** MEG-XL is a brain-to-text model pre-trained with 2.5 minutes of neural context per sample, enabling data-efficient word decoding from MEG data with far less training data.

**What's new**
- Pre-trains with 2.5 minutes of MEG context per sample, 5-300x longer than prior methods.
- Matches supervised word decoding performance using drastically less data (e.g., 1 hour vs 50 hours).
- Outperforms existing brain foundation models in data-efficient generalization.
- Shows that longer pre-training contexts yield better transferable representations for decoding.

### [PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss](https://arxiv.org/abs/2602.02493v1)
**Authors:** Zehong Ma, Ruihan Xu, Shiliang Zhang
**Published:** 2026-02-02
**Summary:** PixelGen is a pixel-space diffusion model that uses perceptual losses to outperform latent diffusion models, achieving state-of-the-art image generation without VAEs or latent representations.

**What's new**
- Introduces perceptual supervision via LPIPS and DINO losses to guide diffusion on a meaningful manifold, not the full pixel space.
- Achieves FID of 5.11 on ImageNet-256 without classifier-free guidance in only 80 training epochs.
- Demonstrates strong scaling for text-to-image generation with a GenEval score of 0.79.
- Provides a simpler, end-to-end generative paradigm that avoids VAEs, latent bottlenecks, and auxiliary stages.

### [New explanations and inference for least angle regression](https://arxiv.org/abs/2602.02491v1)
**Authors:** Karl B. Gregory, Daniel J. Nordman
**Published:** 2026-02-02
**Summary:** A new framework provides inference and mathematical properties for least angle regression (LAR), explaining variable ordering via population correlations and offering a formal stopping rule.

**What's new**
- Interprets LAR as estimating a population path where variables are ordered by decreasing correlation parameters, with zero correlations indicating unimportant variables.
- Shows estimates of non-zero population correlations are independent normals for inference, while zero-correlation estimates have a non-normal joint distribution.
- Provides a formal rule for stopping the LAR algorithm based on these distributional properties.
- Introduces a modified bootstrap method for valid inference on variable entrance and estimation uncertainty, addressing standard bootstrap failures.

<details><summary>More papers (210)</summary>

**Materials & Physics (cond-mat / comp-ph / chem-ph)**
- [Numerically optimized FROG results for the study of red-shifted spectra in multi-frequency Raman generation](https://arxiv.org/abs/2602.02463v1)
- [Mechanics of incompatible asymmetric grain boundary migration](https://arxiv.org/abs/2602.02387v1)
- [FragmentFlow: Scalable Transition State Generation for Large Molecules](https://arxiv.org/abs/2602.02310v1)
- [Energy-Transfer-Enhanced Emission and Quantum Sensing of VB- Defects in hBN-PbI2 Heterostructures](https://arxiv.org/abs/2602.02256v1)
- [Sampling two-dimensional isometric tensor network states](https://arxiv.org/abs/2602.02245v1)
- [Symmetry-restricted energy landscapes as a benchmark for machine learned interatomic potentials](https://arxiv.org/abs/2602.02237v1)
- [Enabling AI Deep Potentials for Ab Initio-quality Molecular Dynamics Simulations in GROMACS](https://arxiv.org/abs/2602.02234v1)
- [Triplet Envelope Functions for increasing machine learning interatomic potential efficiency and stability](https://arxiv.org/abs/2602.02228v1)
- [Direct Observation of Unidirectional Density Wave and Band splitting in a Single-Domain Trilayer Nickelate Pr$_4$Ni$_3$O$_{10}$](https://arxiv.org/abs/2602.02127v1)
- [The Entropic Barrier around the Conical Intersection Seam](https://arxiv.org/abs/2602.02115v1)
- [Intersubband electric dipole spin resonance in transition metal dichalcogenide heterobilayers](https://arxiv.org/abs/2602.02111v1)
- [Optical properties of Fermi polarons in a GaInP/MoSe2 monolayer heterostructure](https://arxiv.org/abs/2602.01964v1)
- [FluxNet: Learning Capacity-Constrained Local Transport Operators for Conservative and Bounded PDE Surrogates](https://arxiv.org/abs/2602.01941v1)
- [Fe-DCA Metal-Organic Frameworks on the Bi2Se3(0001) Topological Insulator Surface](https://arxiv.org/abs/2602.01940v1)
- [A Flux-Correction Form of the Third-Order Edge-Based Scheme for a General Numerical Flux Function](https://arxiv.org/abs/2602.01938v1)
- [Internal Trajectories and Observation Effects in Langevin Splitting Schemes](https://arxiv.org/abs/2602.01923v1)
- [Thermophysical properties of spark plasma sintered UCo: a comparison with machine learning predictions](https://arxiv.org/abs/2602.01896v1)
- [Multigrid Poisson Solver for Complex Geometries Using Finite Difference Method](https://arxiv.org/abs/2602.01888v1)
- [Bayesian Parameter Estimation for Predictive Modeling of Illumination-Dependent Current-Voltage Curves](https://arxiv.org/abs/2602.01859v1)
- [Weyl-Dirac nodal line phonons with type-selective surface states](https://arxiv.org/abs/2602.01841v1)
- [Facilitating electrical and laser-induced skyrmion nucleation with a dipolar-field enhanced effective DMI](https://arxiv.org/abs/2602.01818v1)
- [Morphological Evolution of Nickel-Fullerene Thin Film Mixtures](https://arxiv.org/abs/2602.01788v1)
- [Degenerate Soft Modes and Selective Condensation in BaAl$_2$O$_4$ via Inelastic X-ray Scattering](https://arxiv.org/abs/2602.01732v1)
- [Machine learning determines the Mg2SiO4 P-T phase diagram](https://arxiv.org/abs/2602.01730v1)

**AI/ML (cs.AI/cs.LG/stat.ML/cs.CL)**
- [RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System](https://arxiv.org/abs/2602.02488v1)
- [Expanding the Capabilities of Reinforcement Learning via Text Feedback](https://arxiv.org/abs/2602.02482v1)
- [Flow Policy Gradients for Robot Control](https://arxiv.org/abs/2602.02481v1)
- [AgentRx: Diagnosing AI Agent Failures from Execution Trajectories](https://arxiv.org/abs/2602.02475v1)
- [MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents](https://arxiv.org/abs/2602.02474v1)
- [SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning](https://arxiv.org/abs/2602.02472v1)
- [Multi-head automated segmentation by incorporating detection head into the contextual layer neural network](https://arxiv.org/abs/2602.02471v1)
- [Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge](https://arxiv.org/abs/2602.02470v1)
- [Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation](https://arxiv.org/abs/2602.02469v1)
- [Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts](https://arxiv.org/abs/2602.02468v1)
- [MentisOculi: Revealing the Limits of Reasoning with Mental Imagery](https://arxiv.org/abs/2602.02465v1)
- [From Directions to Regions: Decomposing Activations in Language Models via Local Geometry](https://arxiv.org/abs/2602.02464v1)
- [Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models](https://arxiv.org/abs/2602.02462v1)
- [Conflict-Aware Client Selection for Multi-Server Federated Learning](https://arxiv.org/abs/2602.02458v1)
- [Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction](https://arxiv.org/abs/2602.02455v1)
- [World-Gymnast: Training Robots with Reinforcement Learning in a World Model](https://arxiv.org/abs/2602.02454v1)
- [Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling](https://arxiv.org/abs/2602.02453v1)
- [Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization](https://arxiv.org/abs/2602.02451v1)
- [Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation](https://arxiv.org/abs/2602.02445v1)
- [Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE](https://arxiv.org/abs/2602.02443v1)
- [Energy-Efficient Neuromorphic Computing for Edge AI: A Framework with Adaptive Spiking Neural Networks and Hardware-Aware Optimization](https://arxiv.org/abs/2602.02439v1)
- [UniReason 1.0: A Unified Reasoning Framework for World Knowledge Aligned Image Generation and Editing](https://arxiv.org/abs/2602.02437v1)
- [Maximizing Reliability with Bayesian Optimization](https://arxiv.org/abs/2602.02432v1)
- [Full-Batch Gradient Descent Outperforms One-Pass SGD: Sample Complexity Separation in Single-Index Learning](https://arxiv.org/abs/2602.02431v1)
- [Embedding Perturbation may Better Reflect the Uncertainty in LLM Reasoning](https://arxiv.org/abs/2602.02427v1)
- [Repurposing Protein Language Models for Latent Flow-Based Fitness Optimization](https://arxiv.org/abs/2602.02425v1)
- [Poly-attention: a general scheme for higher-order self-attention](https://arxiv.org/abs/2602.02422v1)
- [SafeGround: Know When to Trust GUI Grounding Models via Uncertainty Calibration](https://arxiv.org/abs/2602.02419v1)
- [Trust Region Continual Learning as an Implicit Meta-Learner](https://arxiv.org/abs/2602.02417v1)
- [Active Transfer Bagging: A New Approach for Accelerated Active Learning Acquisition of Data by Combined Transfer Learning and Bagging Based Models](https://arxiv.org/abs/2602.02415v1)
- [Misconception Diagnosis From Student-Tutor Dialogue: Generate, Retrieve, Rerank](https://arxiv.org/abs/2602.02414v1)
- [Masked Autoencoders as Universal Speech Enhancer](https://arxiv.org/abs/2602.02413v1)
- [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/abs/2602.02408v1)
- [Provably Data-driven Multiple Hyper-parameter Tuning with Structured Loss Function](https://arxiv.org/abs/2602.02406v1)
- [Didactic to Constructive: Turning Expert Solutions into Learnable Reasoning](https://arxiv.org/abs/2602.02405v1)
- [SoMA: A Real-to-Sim Neural Simulator for Robotic Soft-body Manipulation](https://arxiv.org/abs/2602.02402v1)
- [An Empirical Study on Noisy Data and LLM Pretraining Loss Divergence](https://arxiv.org/abs/2602.02400v1)
- [PRISM: Performer RS-IMLE for Single-pass Multisensory Imitation Learning](https://arxiv.org/abs/2602.02396v1)
- [David vs. Goliath: Verifiable Agent-to-Agent Jailbreaking via Reinforcement Learning](https://arxiv.org/abs/2602.02395v1)
- [Infinite-World: Scaling Interactive World Models to 1000-Frame Horizons via Pose-Free Hierarchical Memory](https://arxiv.org/abs/2602.02393v1)
- [Personalized Image Generation via Human-in-the-loop Bayesian Optimization](https://arxiv.org/abs/2602.02388v1)
- [Trust by Design: Skill Profiles for Transparent, Cost-Aware LLM Routing](https://arxiv.org/abs/2602.02386v1)
- [Transformers learn factored representations](https://arxiv.org/abs/2602.02385v1)
- [SLIME: Stabilized Likelihood Implicit Margin Enforcement for Preference Optimization](https://arxiv.org/abs/2602.02383v1)
- [From Sycophancy to Sensemaking: Premise Governance for Human-AI Decision Making](https://arxiv.org/abs/2602.02378v1)
- [Proof-RM: A Scalable and Generalizable Reward Model for Math Proof](https://arxiv.org/abs/2602.02377v1)
- [C-kNN-LSH: A Nearest-Neighbor Algorithm for Sequential Counterfactual Inference](https://arxiv.org/abs/2602.02371v1)
- [ReasonCACHE: Teaching LLMs To Reason Without Weight Updates](https://arxiv.org/abs/2602.02366v1)
- [SWE-Universe: Scale Real-World Verifiable Environments to Millions](https://arxiv.org/abs/2602.02361v1)
- [Automated Multiple Mini Interview (MMI) Scoring](https://arxiv.org/abs/2602.02360v1)
- [NAB: Neural Adaptive Binning for Sparse-View CT reconstruction](https://arxiv.org/abs/2602.02356v1)
- [Hierarchical Federated Learning with SignSGD: A Highly Communication-Efficient Approach](https://arxiv.org/abs/2602.02355v1)
- [Context Learning for Multi-Agent Discussion](https://arxiv.org/abs/2602.02350v1)
- [Why Steering Works: Toward a Unified View of Language Model Parameter Dynamics](https://arxiv.org/abs/2602.02343v1)
- [Rethinking Generative Recommender Tokenizer: Recsys-Native Encoding and Semantic Quantization Beyond LLMs](https://arxiv.org/abs/2602.02338v1)
- [Building a Correct-by-Design Lakehouse. Data Contracts, Versioning, and Transactional Pipelines for Humans and Agents](https://arxiv.org/abs/2602.02335v1)
- [VQ-Style: Disentangling Style and Content in Motion with Residual Quantized Representations](https://arxiv.org/abs/2602.02334v1)
- [TTT-Parkour: Rapid Test-Time Training for Perceptive Robot Parkour](https://arxiv.org/abs/2602.02331v1)
- [Language Steering for Multilingual In-Context Learning](https://arxiv.org/abs/2602.02326v1)
- [A Large-Scale Dataset for Molecular Structure-Language Description via a Rule-Regularized Method](https://arxiv.org/abs/2602.02320v1)
- [The Shape of Beliefs: Geometry, Dynamics, and Interventions along Representation Manifolds of Language Models' Posteriors](https://arxiv.org/abs/2602.02315v1)
- [Interpreting and Controlling LLM Reasoning through Integrated Policy Gradient](https://arxiv.org/abs/2602.02313v1)
- [Spark: Modular Spiking Neural Networks](https://arxiv.org/abs/2602.02306v1)
- [Position: Explaining Behavioral Shifts in Large Language Models Requires a Comparative Approach](https://arxiv.org/abs/2602.02304v1)
- [Advancing General-Purpose Reasoning Models with Modular Gradient Surgery](https://arxiv.org/abs/2602.02301v1)
- [Decoupling Generalizability and Membership Privacy Risks in Neural Networks](https://arxiv.org/abs/2602.02296v1)
- [Hallucination or Creativity: How to Evaluate AI-Generated Scientific Stories?](https://arxiv.org/abs/2602.02290v1)
- [An Optimization Method for Autoregressive Time Series Forecasting](https://arxiv.org/abs/2602.02288v1)
- [DFKI-Speech System for WildSpoof Challenge: A robust framework for SASV In-the-Wild](https://arxiv.org/abs/2602.02286v1)
- [Statistical Learning Theory in Lean 4: Empirical Processes from Scratch](https://arxiv.org/abs/2602.02285v1)
- [Choice-Model-Assisted Q-learning for Delayed-Feedback Revenue Management](https://arxiv.org/abs/2602.02283v1)
- [MoLF: Mixture-of-Latent-Flow for Pan-Cancer Spatial Gene Expression Prediction from Histology](https://arxiv.org/abs/2602.02282v1)
- [RACA: Representation-Aware Coverage Criteria for LLM Safety Testing](https://arxiv.org/abs/2602.02280v1)
- [Kimi K2.5: Visual Agentic Intelligence](https://arxiv.org/abs/2602.02276v1)
- [dziribot: rag based intelligent conversational agent for algerian arabic dialect](https://arxiv.org/abs/2602.02270v1)
- [Bridging the Sim-to-Real Gap with multipanda ros2: A Real-Time ROS2 Framework for Multimanual Systems](https://arxiv.org/abs/2602.02269v1)
- [HopFormer: Sparse Graph Transformers with Explicit Receptive Field Control](https://arxiv.org/abs/2602.02268v1)
- [OpenSeal: Good, Fast, and Cheap Construction of an Open-Source Southeast Asian LLM via Parallel Data](https://arxiv.org/abs/2602.02266v1)
- [Unsupervised Physics-Informed Operator Learning through Multi-Stage Curriculum Training](https://arxiv.org/abs/2602.02264v1)
- [Unlocking the Duality between Flow and Field Matching](https://arxiv.org/abs/2602.02261v1)
- [Learning Markov Decision Processes under Fully Bandit Feedback](https://arxiv.org/abs/2602.02260v1)
- [Alignment-Aware Model Adaptation via Feedback-Guided Optimization](https://arxiv.org/abs/2602.02258v1)
- [Well-Posed KL-Regularized Control via Wasserstein and Kalman-Wasserstein KL Divergences](https://arxiv.org/abs/2602.02250v1)
- [Learning While Staying Curious: Entropy-Preserving Supervised Fine-Tuning via Adaptive Self-Distillation for Large Reasoning Models](https://arxiv.org/abs/2602.02244v1)
- [Variational Entropic Optimal Transport](https://arxiv.org/abs/2602.02241v1)
- [Causal Inference for Preprocessed Outcomes with an Application to Functional Connectivity](https://arxiv.org/abs/2602.02240v1)
- [Interpretability in Deep Time Series Models Demands Semantic Alignment](https://arxiv.org/abs/2602.02239v1)
- [Geometry- and Relation-Aware Diffusion for EEG Super-Resolution](https://arxiv.org/abs/2602.02238v1)
- [Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL](https://arxiv.org/abs/2602.02236v1)
- [SEDformer: Event-Synchronous Spiking Transformers for Irregular Telemetry Time Series Forecasting](https://arxiv.org/abs/2602.02230v1)
- [Prediction-Powered Risk Monitoring of Deployed Models for Detecting Harmful Distribution Shifts](https://arxiv.org/abs/2602.02229v1)
- [Spectral Superposition: A Theory of Feature Geometry](https://arxiv.org/abs/2602.02224v1)
- [Using Correspondence Patterns to Identify Irregular Words in Cognate sets Through Leave-One-Out Validation](https://arxiv.org/abs/2602.02221v1)
- [Am I More Pointwise or Pairwise? Revealing Position Bias in Rubric-Based LLM-as-a-Judge](https://arxiv.org/abs/2602.02219v1)
- [Scientific Theory of a Black-Box: A Life Cycle-Scale XAI Framework Based on Constructive Empiricism](https://arxiv.org/abs/2602.02215v1)
- [Generating Physically Sound Designs from Text and a Set of Physical Constraints](https://arxiv.org/abs/2602.02213v1)
- [Towards AI Evaluation in Domain-Specific RAG Systems: The AgriHubi Case Study](https://arxiv.org/abs/2602.02208v1)
- [Sinhala Physical Common Sense Reasoning Dataset for Global PIQA](https://arxiv.org/abs/2602.02207v1)
- [Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning](https://arxiv.org/abs/2602.02206v1)
- [Cardinality-Preserving Structured Sparse Graph Transformers for Molecular Property Prediction](https://arxiv.org/abs/2602.02201v1)
- [Hierarchical Adaptive Eviction for KV Cache Management in Multimodal Language Models](https://arxiv.org/abs/2602.02197v1)
- [TIDE: Trajectory-based Diagnostic Evaluation of Test-Time Improvement in LLM Agents](https://arxiv.org/abs/2602.02196v1)
- [State Rank Dynamics in Linear Attention LLMs](https://arxiv.org/abs/2602.02195v1)
- [ECHO-2: A Large Scale Distributed Rollout Framework for Cost-efficient Reinforcement Learning](https://arxiv.org/abs/2602.02192v1)
- [PCA of probability measures: Sparse and Dense sampling regimes](https://arxiv.org/abs/2602.02190v1)
- [Reasoning in a Combinatorial and Constrained World: Benchmarking LLMs on Natural-Language Combinatorial Optimization](https://arxiv.org/abs/2602.02188v1)
- [Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://arxiv.org/abs/2602.02185v1)
- [Malware Detection Through Memory Analysis](https://arxiv.org/abs/2602.02184v1)
- [Evaluating Metalinguistic Knowledge in Large Language Models across the World's Languages](https://arxiv.org/abs/2602.02182v1)
- [STILL: Selecting Tokens for Intra-Layer Hybrid Attention to Linearize LLMs](https://arxiv.org/abs/2602.02180v1)
- [AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://arxiv.org/abs/2602.02178v1)
- [Generalized Optimal Classification Trees: A Mixed-Integer Programming Approach](https://arxiv.org/abs/2602.02173v1)
- [Self-Evolving Coordination Protocol in Multi-Agent AI Systems: An Exploratory Systems Feasibility Study](https://arxiv.org/abs/2602.02170v1)
- [Real-Time 2D LiDAR Object Detection Using Three-Frame RGB Scan Encoding](https://arxiv.org/abs/2602.02167v1)
- [Co-RedTeam: Orchestrated Security Discovery and Exploitation with LLM Agents](https://arxiv.org/abs/2602.02164v1)
- [Interpretable Tabular Foundation Models via In-Context Kernel Regression](https://arxiv.org/abs/2602.02162v1)
- [Generating Causal Temporal Interaction Graphs for Counterfactual Validation of Temporal Link Prediction](https://arxiv.org/abs/2602.02161v1)
- [D-CORE: Incentivizing Task Decomposition in Large Reasoning Models for Complex Tool Use](https://arxiv.org/abs/2602.02160v1)
- [Focus-dLLM: Accelerating Long-Context Diffusion LLM Inference via Confidence-Guided Context Focusing](https://arxiv.org/abs/2602.02159v1)
- [Traffic-Aware Navigation in Road Networks](https://arxiv.org/abs/2602.02158v1)
- [Efficient Neural Controlled Differential Equations via Attentive Kernel Smoothing](https://arxiv.org/abs/2602.02157v1)
- [Learning Beyond the Gaussian Data: Learning Dynamics of Neural Networks on an Expressive and Cumulant-Controllable Data Model](https://arxiv.org/abs/2602.02153v1)
- [Revisiting Adaptive Rounding with Vectorized Reparameterization for LLM Quantization](https://arxiv.org/abs/2602.02151v1)
- [ECHO: Entropy-Confidence Hybrid Optimization for Test-Time Reinforcement Learning](https://arxiv.org/abs/2602.02150v1)
- [Back to the Future: Look-ahead Augmentation and Parallel Self-Refinement for Time Series Forecasting](https://arxiv.org/abs/2602.02146v1)
- [Learning Generative Selection for Best-of-N](https://arxiv.org/abs/2602.02143v1)
- [Quantifying the Gap between Understanding and Generation within Unified Multimodal Models](https://arxiv.org/abs/2602.02140v1)
- [EvoMU: Evolutionary Machine Unlearning](https://arxiv.org/abs/2602.02139v1)
- [CAM: A Causality-based Analysis Framework for Multi-Agent Code Generation Systems](https://arxiv.org/abs/2602.02138v1)
- [DCoPilot: Generative AI-Empowered Policy Adaptation for Dynamic Data Center Operations](https://arxiv.org/abs/2602.02137v1)
- [Mitigating Safety Tax via Distribution-Grounded Refinement in Large Reasoning Models](https://arxiv.org/abs/2602.02136v1)
- [Understanding the Reversal Curse Mitigation in Masked Diffusion Models through Attention and Training Dynamics](https://arxiv.org/abs/2602.02133v1)
- [There Is More to Refusal in Large Language Models than a Single Direction](https://arxiv.org/abs/2602.02132v1)
- [Scalable Spatio-Temporal SE(3) Diffusion for Long-Horizon Protein Dynamics](https://arxiv.org/abs/2602.02128v1)
- [Two-Stage Grid Optimization for Group-wise Quantization of LLMs](https://arxiv.org/abs/2602.02126v1)
- [Toxicity Assessment in Preclinical Histopathology via Class-Aware Mahalanobis Distance for Known and Novel Anomalies](https://arxiv.org/abs/2602.02124v1)
- [The Maximum von Neumann Entropy Principle: Theory and Applications in Machine Learning](https://arxiv.org/abs/2602.02117v1)
- [Enhancing Diffusion-Based Quantitatively Controllable Image Generation via Matrix-Form EDM and Adaptive Vicinal Training](https://arxiv.org/abs/2602.02114v1)
- [Training-free score-based diffusion for parameter-dependent stochastic dynamical systems](https://arxiv.org/abs/2602.02113v1)
- [Unifying Masked Diffusion Models with Various Generation Orders and Beyond](https://arxiv.org/abs/2602.02112v1)
- [An Empirical Study of World Model Quantization](https://arxiv.org/abs/2602.02110v1)
- [Out of the Memory Barrier: A Highly Memory Efficient Training System for LLMs with Million-Token Contexts](https://arxiv.org/abs/2602.02108v1)
- [Dicta-LM 3.0: Advancing The Frontier of Hebrew Sovereign LLMs](https://arxiv.org/abs/2602.02104v1)
- [No Global Plan in Chain-of-Thought: Uncover the Latent Planning Horizon of LLMs](https://arxiv.org/abs/2602.02103v1)
- [The Verification Crisis: Expert Perceptions of GenAI Disinformation and the Case for Reproducible Provenance](https://arxiv.org/abs/2602.02100v1)
- [Think Dense, Not Long: Dynamic Decoupled Conditional Advantage for Efficient Reasoning](https://arxiv.org/abs/2602.02099v1)
- [Probabilistic Performance Guarantees for Multi-Task Reinforcement Learning](https://arxiv.org/abs/2602.02098v1)
- [WADEPre: A Wavelet-based Decomposition Model for Extreme Precipitation Nowcasting with Multi-Scale Learning](https://arxiv.org/abs/2602.02096v1)
- [LEC-KG: An LLM-Embedding Collaborative Framework for Domain-Specific Knowledge Graph Construction -- A Case Study on SDGs](https://arxiv.org/abs/2602.02090v1)
- [Efficient Swap Regret Minimization in Combinatorial Bandits](https://arxiv.org/abs/2602.02087v1)
- [Closing the Loop: Universal Repository Representation with RPG-Encoder](https://arxiv.org/abs/2602.02084v1)
- [Handling Covariate Mismatch in Federated Linear Prediction](https://arxiv.org/abs/2602.02083v1)
- [Active learning from positive and unlabeled examples](https://arxiv.org/abs/2602.02081v1)
- [Learning Half-Spaces from Perturbed Contrastive Examples](https://arxiv.org/abs/2602.02080v1)
- [AICD Bench: A Challenging Benchmark for AI-Generated Code Detection](https://arxiv.org/abs/2602.02079v1)
- [Calibrating Adaptive Smoothing Methods for Freeway Traffic Reconstruction](https://arxiv.org/abs/2602.02072v1)
- [BAPS: A Fine-Grained Low-Precision Scheme for Softmax in Attention via Block-Aware Precision reScaling](https://arxiv.org/abs/2602.02071v1)
- [Multi-View Stenosis Classification Leveraging Transformer-Based Multiple-Instance Learning Using Real-World Clinical Data](https://arxiv.org/abs/2602.02067v1)
- [See2Refine: Vision-Language Feedback Improves LLM-Based eHMI Action Designers](https://arxiv.org/abs/2602.02063v1)
- [Learning to Route and Schedule LLMs from User Retrials via Contextual Queueing Bandits](https://arxiv.org/abs/2602.02061v1)
- [FiLoRA: Focus-and-Ignore LoRA for Controllable Feature Reliance](https://arxiv.org/abs/2602.02060v1)
- [Ultrafast On-chip Online Learning via Spline Locality in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2602.02056v1)
- [FORLER: Federated Offline Reinforcement Learning with Q-Ensemble and Actor Rectification](https://arxiv.org/abs/2602.02055v1)
- [WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora](https://arxiv.org/abs/2602.02053v1)
- [SIDiffAgent: Self-Improving Diffusion Agent](https://arxiv.org/abs/2602.02051v1)
- [Rethinking the Role of Entropy in Optimizing Tool-Use Behaviors for Large Language Model Agents](https://arxiv.org/abs/2602.02050v1)
- [Dissecting Outlier Dynamics in LLM NVFP4 Pretraining](https://arxiv.org/abs/2602.02047v1)
- [On Stability and Robustness of Diffusion Posterior Sampling for Bayesian Inverse Problems](https://arxiv.org/abs/2602.02045v1)
- [Twinning Complex Networked Systems: Data-Driven Calibration of the mABCD Synthetic Graph Generator](https://arxiv.org/abs/2602.02044v1)
- [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/abs/2602.02043v1)
- [Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models](https://arxiv.org/abs/2602.02039v1)
- [Bandwidth-Efficient Multi-Agent Communication through Information Bottleneck and Vector Quantization](https://arxiv.org/abs/2602.02035v1)
- [Constrained Process Maps for Multi-Agent Generative AI Workflows](https://arxiv.org/abs/2602.02034v1)
- [One Size, Many Fits: Aligning Diverse Group-Wise Click Preferences in Large-Scale Advertising Image Generation](https://arxiv.org/abs/2602.02033v1)
- [Canonical Intermediate Representation for LLM-based optimization problem formulation and code generation](https://arxiv.org/abs/2602.02029v1)
- [Edit Knowledge, Not Just Facts via Multi-Step Reasoning over Background Stories](https://arxiv.org/abs/2602.02028v1)

_…and 10 more._

</details>

<details><summary>Search definitions</summary>

- Materials & Physics (cond-mat / comp-ph / chem-ph) — `cat:cond-mat.mtrl-sci OR cat:physics.comp-ph OR cat:physics.chem-ph`
- AI/ML (cs.AI/cs.LG/stat.ML/cs.CL) — `cat:cs.AI OR cat:cs.LG OR cat:stat.ML OR cat:cs.CL`

</details>
