---
title: "Daily Issue (JST): 2026-01-27"
---

# Daily Issue (JST): 2026-01-27

## What's New Today

**Advances in LLM Reasoning, Alignment, and Robustness Highlight Daily Research**

Today's research features significant progress in enhancing large language models through novel reinforcement learning and alignment techniques. Methods like PrefixRL and SOAR address exploration and curriculum generation for hard reasoning problems. Concurrently, frameworks for safety, interpretability, and efficient tool use are advancing agent reliability and cross-modal understanding.

**Highlights**
- PrefixRL reuses off-policy traces to bootstrap RL on hard LLM reasoning problems.
- SOAR framework enables LLMs to generate self-improvement curricula via meta-RL.
- HalluGuard provides a unified theoretical framework for data- and reasoning-driven hallucinations.
- AgentDoG introduces a diagnostic guardrail with a 3D taxonomy for agentic risks.
- AdaReasoner learns tool use as a general skill for iterative visual reasoning.
- TEA-Bench is the first interactive benchmark for tool-augmented emotional support agents.
- Embed-KCPD offers a training-free, theory-grounded method for unsupervised text segmentation.

**Themes**
- Reinforcement Learning for LLMs
- Model Alignment & Safety
- Reasoning & Tool Use
- Interpretability & Evaluation
- Multimodal & Cross-Modal Learning
- Efficiency & Scalability

**Keywords**
- reinforcement learning
- large language models
- reasoning
- alignment
- hallucination
- tool use
- multi-agent
- evaluation
- interpretability
- cross-modal
- efficient inference
- robustness

**Total new papers:** 142

## Featured Papers

### [Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes](https://arxiv.org/abs/2601.18795v1)
**Authors:** Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie
**Published:** 2026-01-26
**Summary:** PrefixRL improves RL for LLM reasoning by conditioning on prefixes from successful off-policy traces, boosting learning efficiency on hard problems.

**What's new**
- Introduces PrefixRL: conditions on off-policy trace prefixes, runs on-policy RL to complete them, avoiding off-policy instability.
- Proves PrefixRL objective is consistent with standard RL and more sample efficient, enabling difficulty modulation via prefix length.
- Discovers back-generalization: training on prefixed problems generalizes to unprefixed performance with novel strategies.
- Creates self-improvement loop via rejection sampling with base model, achieving 2x faster training and 3x higher final reward.
- Remains effective when off-policy traces come from different model families, showing practical flexibility.

### [Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability](https://arxiv.org/abs/2601.18778v1)
**Authors:** Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe
**Published:** 2026-01-26
**Summary:** A meta-RL framework enables LLMs to self-generate curricula of synthetic problems to overcome learning plateaus on tasks with zero initial success.

**What's new**
- Introduces SOAR, a self-improvement framework using meta-RL where a teacher model proposes problems to improve a student on hard tasks.
- Shows grounded rewards based on student progress outperform intrinsic rewards, avoiding instability and diversity collapse.
- Finds generated problem structure and well-posedness matter more for learning progress than solution correctness.
- Demonstrates meta-RL can unlock learning from sparse binary rewards on benchmarks with 0% initial success.

### [HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs](https://arxiv.org/abs/2601.18753v1)
**Authors:** Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou
**Published:** 2026-01-26
**Summary:** HalluGuard is a unified detection method using a Neural Tangent Kernel-based score to jointly identify data-driven and reasoning-driven hallucinations in LLMs.

**What's new**
- Introduces Hallucination Risk Bound, a theoretical framework decomposing risk into data-driven and reasoning-driven components.
- Proposes HalluGuard, an NTK-based score leveraging geometry and representations for joint hallucination detection.
- Achieves state-of-the-art performance across 10 benchmarks, 11 baselines, and 9 LLM backbones.

### [Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning](https://arxiv.org/abs/2601.18699v1)
**Authors:** Olaf Yunus Laitinen Imanov
**Published:** 2026-01-26
**Summary:** Mechanistic study identifies gradient interference, representational drift, and loss landscape flattening as key drivers of catastrophic forgetting during continual fine-tuning of large language models.

**What's new**
- Identifies three core mechanisms: gradient interference in attention, representational drift, and loss landscape flattening.
- Shows forgetting severity strongly correlates with task similarity and gradient alignment metrics.
- Finds 15-23% of attention heads are severely disrupted, with lower layers more susceptible.

### [Contrasting Global and Patient-Specific Regression Models via a Neural Network Representation](https://arxiv.org/abs/2601.18658v1)
**Authors:** Max Behrens, Daiana Stolz, Eleni Papakonstantinou, Janis M. Nolde, Gabriele Bellerino, Angelika Rohde, Moritz Hess, Harald Binder
**Published:** 2026-01-26
**Summary:** A diagnostic tool using autoencoder-based localized regression to identify patient subgroups where a global clinical prediction model is inadequate.

**What's new**
- Proposes a tool to contrast global and patient-specific regression models, identifying where global models fail.
- Uses an autoencoder to learn a latent representation optimized for both data reconstruction and localized regression.
- Applies the method to a COPD study, showing most patients fit the global model but specific subgroups benefit from personalization.
- Demonstrates how to map subgroup models back to original predictors to explain global model shortcomings.

### [Unheard in the Digital Age: Rethinking AI Bias and Speech Diversity](https://arxiv.org/abs/2601.18641v1)
**Authors:** Onyedikachi Hope Amaechi-Okorie, Branislav Radeljic
**Published:** 2026-01-26
**Summary:** The article argues that AI speech systems encode bias against atypical speech, causing digital exclusion, and calls for inclusive design and policy reform to treat speech diversity as an equity issue.

**What's new**
- Reframes AI bias in speech recognition as a structural equity issue, not just an accessibility problem.
- Highlights how AI systems trained on standardized speech fail and exclude users with atypical speech patterns.
- Advocates for co-created AI solutions and anti-bias training to elevate the rights of atypical speakers.
- Calls for enforceable policy reforms that explicitly recognize and protect speech diversity.

### [Assessing the Quality of Mental Health Support in LLM Responses through Multi-Attribute Human Evaluation](https://arxiv.org/abs/2601.18630v1)
**Authors:** Abeer Badawi, Md Tahmid Rahman Laskar, Elahe Rahimi, Sheri Grach, Lindsay Bertrand, Lames Danok, Frank Rudzicz, Jimmy Huang, Elham Dolatabadi
**Published:** 2026-01-26
**Summary:** Human evaluation by psychiatric experts reveals LLMs provide safe, clinically appropriate cognitive support but lack stable emotional alignment in mental health conversations.

**What's new**
- Introduces a human-grounded evaluation method with a 6-attribute rubric assessed by psychiatric experts on 500 conversations.
- Finds LLMs show strong cognitive reliability but unstable affective alignment, creating a cognitive-affective gap.
- Reveals closed-source models like GPT-4o offer more balanced responses than variable, emotionally flat open-source models.
- Advocates for failure-aware evaluation frameworks prioritizing relational sensitivity alongside informational accuracy.

### [CASSANDRA: Programmatic and Probabilistic Learning and Inference for Stochastic World Modeling](https://arxiv.org/abs/2601.18620v1)
**Authors:** Panagiotis Lymperopoulos, Abhiramon Rajasekharan, Ian Berlot-Attwell, Stéphane Aroca-Ouellette, Kaheer Suleman
**Published:** 2026-01-26
**Summary:** CASSANDRA is a neurosymbolic world modeling method that uses an LLM as a knowledge prior to build lightweight, programmatic and probabilistic transition models for planning in business domains.

**What's new**
- Uses LLM-synthesized code to model deterministic features of a world.
- Employs LLM-guided structure learning to build a probabilistic graphical model for stochastic variables.
- Demonstrates improved transition prediction and planning in business simulators like a coffee shop and theme park.

### [Learning long term climate-resilient transport adaptation pathways under direct and indirect flood impacts using reinforcement learning](https://arxiv.org/abs/2601.18586v1)
**Authors:** Miguel Costa, Arthur Vandervoort, Carolin Schmidt, Morten W. Petersen, Martin Drews, Karyn Morrissey, Francisco C. Pereira
**Published:** 2026-01-26
**Summary:** A reinforcement learning framework couples climate and impact models to learn adaptive, multi-decade urban transport investment pathways resilient to flooding under deep uncertainty.

**What's new**
- Proposes a generic RL framework integrating long-term climate projections with hazard, impact, and cost models for adaptation planning.
- Learns coordinated spatio-temporal investment pathways that trade off costs against avoided direct and indirect flood impacts.
- Demonstrates improved robustness over conventional baselines for pluvial flooding in Copenhagen from 2024-2100.
- Highlights transferability to other urban hazards and cities through a modular, integrated assessment approach.

### [An Unsupervised Tensor-Based Domain Alignment](https://arxiv.org/abs/2601.18564v1)
**Authors:** Chong Hyun Lee, Kibae Lee, Hyun Hee Yim
**Published:** 2026-01-26
**Summary:** An unsupervised tensor domain alignment method using iterative optimization on an oblique manifold to enhance speed and classification accuracy.

**What's new**
- Uses an oblique manifold for optimization, offering more flexibility than the traditional Stiefel manifold.
- Introduces regularization terms to preserve the variance of both source and target tensors for robustness.
- Generalizes existing tensor-based domain adaptation methods as special cases within its framework.
- Demonstrates improved domain adaptation conversion speed and higher classification accuracy in experiments.

### [Scalable Transit Delay Prediction at City Scale: A Systematic Approach with Multi-Resolution Feature Engineering and Deep Learning](https://arxiv.org/abs/2601.18521v1)
**Authors:** Emna Boudabbous, Mohamed Karaa, Lokman Sboui, Julio Montecinos, Omar Alam
**Published:** 2026-01-26
**Summary:** A scalable deep learning pipeline for city-wide bus delay prediction using multi-resolution feature engineering, hybrid clustering, and efficient LSTM models.

**What's new**
- Introduces multi-resolution feature engineering generating 1,683 spatiotemporal features from H3 cells, routes, and temporal patterns
- Proposes hybrid H3+topology clustering to avoid giant cluster problem, creating 12 balanced route clusters
- Uses Adaptive PCA to compress features to 83 components while preserving 95% variance
- Global LSTM with cluster-aware features outperforms transformers by 18-52% with 275x fewer parameters
- Provides systematic framework for real-time, city-scale deployment reusable across transit networks

### [Just-In-Time Reinforcement Learning: Continual Learning in LLM Agents Without Gradient Updates](https://arxiv.org/abs/2601.18510v1)
**Authors:** Yibo Li, Zijie Lin, Ailin Deng, Xuan Zhang, Yufei He, Shuo Ji, Tri Cao, Bryan Hooi
**Published:** 2026-01-26
**Summary:** A training-free framework that enables LLM agents to adapt continually at test time by retrieving past experiences to modulate action probabilities, avoiding gradient updates.

**What's new**
- Proposes a training-free RL framework for LLM agents that avoids gradient updates and catastrophic forgetting.
- Uses a dynamic memory of experiences to retrieve trajectories and estimate action advantages on-the-fly.
- Modulates LLM output logits directly via an additive update rule derived as a closed-form solution to a KL-constrained objective.
- Achieves state-of-the-art results on WebArena and Jericho, outperforming fine-tuning methods while reducing costs 30x.

### [DEEPMED: Building a Medical DeepResearch Agent via Multi-hop Med-Search Data and Turn-Controlled Agentic Training & Inference](https://arxiv.org/abs/2601.18496v1)
**Authors:** Zihan wang, Hao Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yiqun Zhang, Jinghao Lin, Haihua Yang, Xiaozhong Ji
**Published:** 2026-01-26
**Summary:** DeepMed is a medical DeepResearch agent that improves reasoning by using multi-hop search data, turn-controlled training, and step-limited inference to reduce hallucinations.

**What's new**
- Introduces a multi-hop medical search QA synthesis method to ground reasoning in verifiable evidence within clinical contexts.
- Uses a difficulty-aware turn-penalty during training to suppress excessive and noisy tool-calls.
- Implements a step-monitoring inference mechanism to validate hypotheses and prevent context degradation.
- Achieves an average 9.79% improvement over its base model across seven medical benchmarks.

### [AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security](https://arxiv.org/abs/2601.18491v1)
**Authors:** Dongrui Liu, Qihan Ren, Chen Qian, Shuai Shao, Yuejin Xie, Yu Li, Zhonghao Yang, Haoyu Luo, Peng Wang, Qingyu Liu, Binxin Hu, Ling Tang, Jilin Mei, Dadi Guo, Leitao Yuan, Junyao Yang, Guanxu Chen, Qihao Lin, Yi Yu, Bo Zhang, Jiaxuan Guo, Jie Zhang, Wenqi Shao, Huiqi Deng, Zhiheng Xi, Wenjie Wang, Wenxuan Wang, Wen Shen, Zhikai Chen, Haoyu Xie, Jialing Tao, Juntao Dai, Jiaming Ji, Zhongjie Ba, Linfeng Zhang, Yong Liu, Quanshi Zhang, Lei Zhu, Zhihua Wei, Hui Xue, Chaochao Lu, Jing Shao, Xia Hu
**Published:** 2026-01-26
**Summary:** A diagnostic guardrail framework with a 3D risk taxonomy and fine-grained monitoring to detect and explain unsafe AI agent behaviors.

**What's new**
- Proposes a unified 3D taxonomy categorizing agent risks by source, failure mode, and consequence.
- Introduces ATBench, a new fine-grained agentic safety benchmark for evaluation.
- Provides diagnostic root-cause analysis for unsafe actions, not just binary detection.
- Offers transparent provenance and reasoning for seemingly safe but unreasonable behaviors.
- Releases open-source models (4B, 7B, 8B parameters) across Qwen and Llama families.

### [Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs](https://arxiv.org/abs/2601.18483v1)
**Authors:** Arya Labroo, Ivaxi Sheth, Vyas Raina, Amaani Ahmed, Mario Fritz
**Published:** 2026-01-26
**Summary:** LLMs struggle to generate text that combines two independent concepts like humor and persuasiveness simultaneously, revealing a compositionality gap in naive prompting.

**What's new**
- Introduces an evaluation framework for fine-grained control over single and dual textual concepts in LLMs.
- Finds performance often drops in dual-concept settings, even for intuitively separable concepts like humor and persuasiveness.
- Reveals a fundamental limitation: models struggle with compositionality via naive prompting-based control.
- Provides systematic evidence of this gap to guide future methods for multi-concept control.

### [Enhancing Control Policy Smoothness by Aligning Actions with Predictions from Preceding States](https://arxiv.org/abs/2601.18479v1)
**Authors:** Kyoleen Kwak, Hyoseok Hwang
**Published:** 2026-01-26
**Summary:** ASAP reduces control policy oscillations by aligning actions with predictions from transition-induced similar states and penalizing high-frequency action changes.

**What's new**
- Introduces transition-induced similar states based on next-state distributions from previous states
- Proposes ASAP loss aligning actions with those from transition-induced similar states
- Penalizes second-order action differences to suppress high-frequency oscillations
- Uses only environmental feedback and collected data, better capturing system dynamics
- Demonstrates smoother control and improved performance in Gymnasium/Isaac-Lab

### [OffSeeker: Online Reinforcement Learning Is Not All You Need for Deep Research Agents](https://arxiv.org/abs/2601.18467v1)
**Authors:** Yuhang Zhou, Kai Zheng, Qiguang Chen, Mengkang Hu, Qingfeng Sun, Can Xu, Jingjing Chen
**Published:** 2026-01-26
**Summary:** OffSeeker is a powerful 8B-parameter research agent trained entirely offline using a new open-source suite, challenging the need for expensive online reinforcement learning.

**What's new**
- Introduces DeepForge, a task synthesis framework generating large-scale research queries without heavy preprocessing
- Provides a fully open-source suite with 66k QA pairs, 33k SFT trajectories, and 21k DPO pairs for offline training
- Demonstrates that expensive online RL is not essential for building capable deep research agents
- Shows OffSeeker (8B) outperforms similar-sized agents and competes with larger 30B models trained with online RL

### [3DGesPolicy: Phoneme-Aware Holistic Co-Speech Gesture Generation Based on Action Control](https://arxiv.org/abs/2601.18451v1)
**Authors:** Xuanmeng Sha, Liyun Zhang, Tomohiro Mashita, Naoya Chiba, Yuki Uranishi
**Published:** 2026-01-26
**Summary:** A framework that generates holistic, speech-aligned full-body and facial gestures by modeling motion as continuous actions using a diffusion policy and phoneme-aware fusion.

**What's new**
- Reformulates gesture generation as a continuous trajectory control problem using a diffusion policy from robotics.
- Models frame-to-frame variations as unified holistic actions for spatially and semantically coherent motion.
- Introduces a Gesture-Audio-Phoneme fusion module for fine-grained alignment of speech, body, and facial expressions.
- Demonstrates state-of-the-art performance on the BEAT2 dataset for natural and expressive holistic gestures.

### [On Procrustes Contamination in Machine Learning Applications of Geometric Morphometrics](https://arxiv.org/abs/2601.18448v1)
**Authors:** Lloyd Austin Courtenay
**Published:** 2026-01-26
**Summary:** Standard Procrustes alignment before data splitting contaminates ML models; a new realignment method fixes this dependency and clarifies statistical constraints.

**What's new**
- Formally characterizes contamination from Procrustes alignment before train/test splits in morphometrics ML
- Proposes a novel realignment procedure aligning test data to training set to eliminate dependency
- Reveals a robust diagonal relationship between sample size and landmark space scaling RMSE
- Demonstrates performance degradation when ignoring spatial autocorrelation among landmarks
- Provides practical guidelines for preprocessing and clarifies Procrustes shape space constraints

### [Gradient Regularized Natural Gradients](https://arxiv.org/abs/2601.18420v1)
**Authors:** Satya Prakash Dash, Hossein Abdi, Wei Pan, Samuel Kaski, Mingfei Sun
**Published:** 2026-01-26
**Summary:** GRNG integrates gradient regularization with natural gradient descent, improving optimization speed and generalization via scalable frequentist and Bayesian variants.

**What's new**
- Combines gradient regularization with natural gradient updates for better generalization and faster convergence
- Introduces two scalable variants: frequentist (structured FIM approximations) and Bayesian (Regularized-Kalman, no FIM inversion)
- Provides convergence guarantees showing regularization improves stability and enables global minima convergence
- Empirically outperforms SGD, AdamW, K-FAC, and Sophia on vision and language benchmarks

### [daVinci-Dev: Agent-native Mid-training for Software Engineering](https://arxiv.org/abs/2601.18418v1)
**Authors:** Ji Zeng, Dayuan Fu, Tiantian Mi, Yumin Zhuang, Yaxing Huang, Xuefeng Li, Lyumanshan Ye, Muhang Xie, Qishuo Hua, Zhen Huang, Mohan Jiang, Hanning Wang, Jifan Lin, Yang Xiao, Jie Sun, Yunze Wu, Pengfei Liu
**Published:** 2026-01-26
**Summary:** Introduces agent-native mid-training for software engineering agents, using contextually- and environmentally-native trajectories to bridge the gap between static training data and dynamic development environments.

**What's new**
- Proposes agentic mid-training as a scalable alternative to post-training for instilling foundational agentic behaviors in LLMs
- Introduces agent-native data synthesis with two trajectory types: contextually-native for broad coverage and environmentally-native for execution authenticity
- Achieves state-of-the-art results on SWE-Bench Verified with 56.1% and 58.5% resolution rates using 32B and 72B models respectively
- Demonstrates superiority over previous methods while using less than half the mid-training tokens (73.1B vs previous approaches)

### [Frequency-Based Hyperparameter Selection in Games](https://arxiv.org/abs/2601.18409v1)
**Authors:** Aniket Sanyal, Baraah A. M. Sidahmed, Rebekka Burkholz, Tatjana Chavdarova
**Published:** 2026-01-26
**Summary:** A principled method for hyperparameter selection in smooth games using frequency analysis, extending LookAhead with adaptive tuning.

**What's new**
- Introduces Modal LookAhead (MoLA), an adaptive extension of LookAhead for games.
- Uses frequency estimation of oscillatory dynamics to select hyperparameters.
- Provides convergence guarantees for MoLA in rotational and mixed regimes.
- Demonstrates accelerated training with minimal computational overhead.

### [Superlinear Multi-Step Attention](https://arxiv.org/abs/2601.18401v1)
**Authors:** Yufeng Huang
**Published:** 2026-01-26
**Summary:** A multi-step attention architecture achieving subquadratic complexity for long sequences while preserving random context access via learnable span selection.

**What's new**
- Reformulates causal self-attention as an N-step search, achieving O(L^(1+1/N)) complexity
- Demonstrates O(L^3/2) instantiation with span-search followed by span-attention
- Shows O(L^1.54) configuration achieving 114 tokens/sec at 1M context on a single GPU
- Validates learnable span selection via strong NIAH task performance up to 256K context

### [Estimating Dense-Packed Zone Height in Liquid-Liquid Separation: A Physics-Informed Neural Network Approach](https://arxiv.org/abs/2601.18399v1)
**Authors:** Mehmet Velioglu, Song Zhai, Alexander Mitsos, Adel Mhamdi, Andreas Jupke, Manuel Dahmen
**Published:** 2026-01-26
**Summary:** A physics-informed neural network (PINN) estimates dense-packed zone height in liquid-liquid separators using only flow measurements, combining synthetic data from a mechanistic model with scarce experimental data for accurate state estimation.

**What's new**
- Uses a two-stage PINN: pretrained on synthetic data from a low-fidelity mechanistic model, then fine-tuned with scarce experimental data
- Integrates the differentiable PINN into an Extended Kalman Filter-inspired framework for state estimation from flow-rate measurements
- Reduces reliance on expensive optical measurements by estimating phase heights from inexpensive volume flow data only
- Demonstrates superior accuracy over non-pretrained PINNs and purely data-driven neural networks in phase-height estimation

<details><summary>More papers (118)</summary>

**Materials Informatics (ML for materials)**
- [Physics-Informed Uncertainty Enables Reliable AI-driven Design](https://arxiv.org/abs/2601.18638v1)
- [The generalised balanced power diagram: flat sections, affine transformations and an improved rendering algorithm](https://arxiv.org/abs/2601.18593v1)
- [Global Optimization of Atomic Clusters via Physically-Constrained Tensor Train Decomposition](https://arxiv.org/abs/2601.18592v1)
- [Refinement and Performance Benchmark for Range-Separated Water Force Field](https://arxiv.org/abs/2601.18416v1)
- [Data-Efficient Electromagnetic Surrogate Solver Through Dissipative Relaxation Transfer Learning](https://arxiv.org/abs/2601.18235v1)

**AI/ML (cs.AI/cs.LG/stat.ML)**
- [ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models](https://arxiv.org/abs/2601.18796v1)
- [MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data](https://arxiv.org/abs/2601.18792v1)
- [Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets](https://arxiv.org/abs/2601.18791v1)
- [Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings](https://arxiv.org/abs/2601.18788v1)
- [Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System](https://arxiv.org/abs/2601.18785v1)
- [Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic](https://arxiv.org/abs/2601.18783v1)
- [POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration](https://arxiv.org/abs/2601.18779v1)
- [PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation](https://arxiv.org/abs/2601.18777v1)
- [Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory](https://arxiv.org/abs/2601.18771v1)
- [Learning to Discover: A Generalized Framework for Raga Identification without Forgetting](https://arxiv.org/abs/2601.18766v1)
- [Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values](https://arxiv.org/abs/2601.18760v1)
- [$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks](https://arxiv.org/abs/2601.18754v1)
- [Trust, Don't Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback](https://arxiv.org/abs/2601.18751v1)
- [Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval](https://arxiv.org/abs/2601.18747v1)
- [TSRBench: A Comprehensive Multi-task Multi-modal Time Series Reasoning Benchmark for Generalist Models](https://arxiv.org/abs/2601.18744v1)
- [SeNeDiF-OOD: Semantic Nested Dichotomy Fusion for Out-of-Distribution Detection Methodology in Open-World Classification. A Case Study on Monument Style Classification](https://arxiv.org/abs/2601.18739v1)
- [Benchmarking Machine Learning Models for IoT Malware Detection under Data Scarcity and Drift](https://arxiv.org/abs/2601.18736v1)
- [Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems](https://arxiv.org/abs/2601.18735v1)
- [Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models](https://arxiv.org/abs/2601.18734v1)
- [Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge](https://arxiv.org/abs/2601.18733v1)
- [Optimal Use of Preferences in Artificial Intelligence Algorithms](https://arxiv.org/abs/2601.18732v1)
- [One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment](https://arxiv.org/abs/2601.18731v1)
- [Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale](https://arxiv.org/abs/2601.18730v1)
- [Riemannian AmbientFlow: Towards Simultaneous Manifold Learning and Generative Modeling from Corrupted Data](https://arxiv.org/abs/2601.18728v1)
- [HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences](https://arxiv.org/abs/2601.18724v1)
- [Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning](https://arxiv.org/abs/2601.18722v1)
- [Conditioned Generative Modeling of Molecular Glues: A Realistic AI Approach for Synthesizable Drug-like Molecules](https://arxiv.org/abs/2601.18716v1)
- [Low Cost, High Efficiency: LiDAR Place Recognition in Vineyards with Matryoshka Representation Learning](https://arxiv.org/abs/2601.18714v1)
- [Point transformer for protein structural heterogeneity analysis using CryoEM](https://arxiv.org/abs/2601.18713v1)
- [Analyzing Images of Blood Cells with Quantum Machine Learning Methods: Equilibrium Propagation and Variational Quantum Circuits to Detect Acute Myeloid Leukemia](https://arxiv.org/abs/2601.18710v1)
- [SMART: Scalable Mesh-free Aerodynamic Simulations from Raw Geometries using a Transformer-based Surrogate Model](https://arxiv.org/abs/2601.18707v1)
- [Health-SCORE: Towards Scalable Rubrics for Improving Health-LLMs](https://arxiv.org/abs/2601.18706v1)
- [Data-Driven Qubit Characterization and Optimal Control using Deep Learning](https://arxiv.org/abs/2601.18704v1)
- [From Fuzzy to Exact: The Halo Architecture for Infinite-Depth Reasoning via Rational Arithmetic](https://arxiv.org/abs/2601.18702v1)
- [TEA-Bench: A Systematic Benchmarking of Tool-enhanced Emotional Support Dialogue Agent](https://arxiv.org/abs/2601.18700v1)
- [Explainability Methods for Hardware Trojan Detection: A Systematic Comparison](https://arxiv.org/abs/2601.18696v1)
- [Neural Multi-Speaker Voice Cloning for Nepali in Low-Resource Settings](https://arxiv.org/abs/2601.18694v1)
- [LLAMA LIMA: A Living Meta-Analysis on the Effects of Generative AI on Learning Mathematics](https://arxiv.org/abs/2601.18685v1)
- [Learned harmonic mean estimation of the marginal likelihood for multimodal posteriors with flow matching](https://arxiv.org/abs/2601.18683v1)
- [ART for Diffusion Sampling: A Reinforcement Learning Approach to Timestep Schedule](https://arxiv.org/abs/2601.18681v1)
- [Counterfactual Explanations on Robust Perceptual Geodesics](https://arxiv.org/abs/2601.18678v1)
- [Out-of-Distribution Radar Detection with Complex VAEs: Theory, Whitening, and ANMF Fusion](https://arxiv.org/abs/2601.18677v1)
- [Quasi Monte Carlo methods enable extremely low-dimensional deep generative models](https://arxiv.org/abs/2601.18676v1)
- [Learning temporal embeddings from electronic health records of chronic kidney disease patients](https://arxiv.org/abs/2601.18675v1)
- [A Dynamic Framework for Grid Adaptation in Kolmogorov-Arnold Networks](https://arxiv.org/abs/2601.18672v1)
- [Uniform Computability of PAC Learning](https://arxiv.org/abs/2601.18663v1)
- [FaLW: A Forgetting-aware Loss Reweighting for Long-tailed Unlearning](https://arxiv.org/abs/2601.18650v1)
- [FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory](https://arxiv.org/abs/2601.18642v1)
- [TwinPurify: Purifying gene expression data to reveal tumor-intrinsic transcriptional programs via self-supervised learning](https://arxiv.org/abs/2601.18640v1)
- [Universality of Many-body Projected Ensemble for Learning Quantum Data Distribution](https://arxiv.org/abs/2601.18637v1)
- [AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning](https://arxiv.org/abs/2601.18631v1)
- [Rank-1 Approximation of Inverse Fisher for Natural Policy Gradients in Deep Reinforcement Learning](https://arxiv.org/abs/2601.18626v1)
- [Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks](https://arxiv.org/abs/2601.18617v1)
- [Geometry-Free Conditional Diffusion Modeling for Solving the Inverse Electrocardiography Problem](https://arxiv.org/abs/2601.18615v1)
- [PolySHAP: Extending KernelSHAP with Interaction-Informed Polynomial Regression](https://arxiv.org/abs/2601.18608v1)
- [LaCoGSEA: Unsupervised deep learning for pathway analysis via latent correlation](https://arxiv.org/abs/2601.18604v1)
- [A Balanced Neuro-Symbolic Approach for Commonsense Abductive Logic](https://arxiv.org/abs/2601.18595v1)
- [Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs](https://arxiv.org/abs/2601.18588v1)
- [K-Myriad: Jump-starting reinforcement learning with unsupervised parallel agents](https://arxiv.org/abs/2601.18580v1)
- [FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG](https://arxiv.org/abs/2601.18579v1)
- [Self-Refining Video Sampling](https://arxiv.org/abs/2601.18577v1)
- [Attention-Based Neural-Augmented Kalman Filter for Legged Robot State Estimation](https://arxiv.org/abs/2601.18569v1)
- [Generative Diffusion Augmentation with Quantum-Enhanced Discrimination for Medical Image Diagnosis](https://arxiv.org/abs/2601.18556v1)
- [Deconstructing Instruction-Following: A New Benchmark for Granular Evaluation of Large Language Model Instruction Compliance Abilities](https://arxiv.org/abs/2601.18554v1)
- [Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection](https://arxiv.org/abs/2601.18552v1)
- [Information Hidden in Gradients of Regression with Target Noise](https://arxiv.org/abs/2601.18546v1)
- [SKETCH: Semantic Key-Point Conditioning for Long-Horizon Vessel Trajectory Prediction](https://arxiv.org/abs/2601.18537v1)
- [From Cold Start to Active Learning: Embedding-Based Scan Selection for Medical Image Segmentation](https://arxiv.org/abs/2601.18532v1)
- [Closing the Modality Gap Aligns Group-Wise Semantics](https://arxiv.org/abs/2601.18525v1)
- [From Human Labels to Literature: Semi-Supervised Learning of NMR Chemical Shifts at Scale](https://arxiv.org/abs/2601.18524v1)
- [LipNeXt: Scaling up Lipschitz-based Certified Robustness to Billion-parameter Models](https://arxiv.org/abs/2601.18513v1)
- [Conformal Prediction Algorithms for Time Series Forecasting: Methods and Benchmark](https://arxiv.org/abs/2601.18509v1)
- [Nearly Optimal Bayesian Inference for Structural Missingness](https://arxiv.org/abs/2601.18500v1)
- [Fair-Eye Net: A Fair, Trustworthy, Multimodal Integrated Glaucoma Full Chain AI System](https://arxiv.org/abs/2601.18464v1)
- [GCFX: Generative Counterfactual Explanations for Deep Graph Models at the Model Level](https://arxiv.org/abs/2601.18447v1)
- [Fusion of Spatio-Temporal and Multi-Scale Frequency Features for Dry Electrodes MI-EEG Decoding](https://arxiv.org/abs/2601.18424v1)
- [Emergent Cooperation in Quantum Multi-Agent Reinforcement Learning Using Communication](https://arxiv.org/abs/2601.18419v1)
- [Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models](https://arxiv.org/abs/2601.18383v1)
- [AI Agent for Reverse-Engineering Legacy Finite-Difference Code and Translating to Devito](https://arxiv.org/abs/2601.18381v1)
- [Making medical vision-language models think causally across modalities with retrieval-augmented cross-modal reasoning](https://arxiv.org/abs/2601.18356v1)
- [Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books](https://arxiv.org/abs/2601.18353v1)
- [Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning](https://arxiv.org/abs/2601.18352v1)
- [When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs](https://arxiv.org/abs/2601.18350v1)
- [Structural Gender Bias in Credit Scoring: Proxy Leakage](https://arxiv.org/abs/2601.18342v1)
- [A Dataset for Automatic Vocal Mode Classification](https://arxiv.org/abs/2601.18339v1)
- [Analytic Incremental Learning For Sound Source Localization With Imbalance Rectification](https://arxiv.org/abs/2601.18335v1)
- [A Tumor Aware DenseNet Swin Hybrid Learning with Boosted and Hierarchical Feature Spaces for Large-Scale Brain MRI Classification](https://arxiv.org/abs/2601.18330v1)
- [Discriminability-Driven Spatial-Channel Selection with Gradient Norm for Drone Signal OOD Detection](https://arxiv.org/abs/2601.18329v1)
- [Cognitive Fusion of ZC Sequences and Time-Frequency Images for Out-of-Distribution Detection of Drone Signals](https://arxiv.org/abs/2601.18326v1)
- [MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization](https://arxiv.org/abs/2601.18320v1)
- [A Master Class on Reproducibility: A Student Hackathon on Advanced MRI Reconstruction Methods](https://arxiv.org/abs/2601.18314v1)
- [Convex Chance-Constrained Stochastic Control under Uncertain Specifications with Application to Learning-Based Hybrid Powertrain Control](https://arxiv.org/abs/2601.18313v1)
- [A Generative AI-Driven Reliability Layer for Action-Oriented Disaster Resilience](https://arxiv.org/abs/2601.18308v1)
- [Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM](https://arxiv.org/abs/2601.18306v1)
- [Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning](https://arxiv.org/abs/2601.18296v1)
- [TriPlay-RL: Tri-Role Self-Play Reinforcement Learning for LLM Safety Alignment](https://arxiv.org/abs/2601.18292v1)
- [Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning](https://arxiv.org/abs/2601.18282v1)
- [What Do Learned Models Measure?](https://arxiv.org/abs/2601.18278v1)
- [Toward Scalable Normalizing Flows for the Hubbard Model](https://arxiv.org/abs/2601.18273v1)
- [Neural Network Approximation: A View from Polytope Decomposition](https://arxiv.org/abs/2601.18264v1)
- [FGGM: Fisher-Guided Gradient Masking for Continual Learning](https://arxiv.org/abs/2601.18261v1)
- [Beyond Retention: Orchestrating Structural Safety and Plasticity in Continual Learning for LLMs](https://arxiv.org/abs/2601.18255v1)
- [BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation](https://arxiv.org/abs/2601.18253v1)
- [Co-PLNet: A Collaborative Point-Line Network for Prompt-Guided Wireframe Parsing](https://arxiv.org/abs/2601.18252v1)
- [A multimodal vision foundation model for generalizable knee pathology](https://arxiv.org/abs/2601.18250v1)
- [Tractable Gaussian Phase Retrieval with Heavy Tails and Adversarial Corruption with Near-Linear Sample Complexity](https://arxiv.org/abs/2601.18245v1)
- [TAM-Eval: Evaluating LLMs for Automated Unit Test Maintenance](https://arxiv.org/abs/2601.18241v1)
- [TechING: Towards Real World Technical Image Understanding via VLMs](https://arxiv.org/abs/2601.18238v1)
- [Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions](https://arxiv.org/abs/2601.18234v1)
- [Rethinking Cross-Modal Fine-Tuning: Optimizing the Interaction between Feature Alignment and Target Fitting](https://arxiv.org/abs/2601.18231v1)
- [Facial Emotion Recognition on FER-2013 using an EfficientNetB2-Based Approach](https://arxiv.org/abs/2601.18228v1)
- [Yunjue Agent Tech Report: A Fully Reproducible, Zero-Start In-Situ Self-Evolving Agent System for Open-Ended Tasks](https://arxiv.org/abs/2601.18226v1)
- [ShopSimulator: Evaluating and Exploring RL-Driven LLM Agent for Shopping Assistants](https://arxiv.org/abs/2601.18225v1)

</details>

<details><summary>Search definitions</summary>

- Materials Informatics (ML for materials) — `(cat:cond-mat.mtrl-sci OR cat:physics.chem-ph OR cat:physics.comp-ph) AND (abs:"machine learning" OR abs:"deep learning" OR abs:neural OR abs:graph OR abs:transformer
 OR ti:"machine learning" OR ti:"deep learning" OR ti:neural OR ti:graph OR ti:transformer)`
- AI/ML (cs.AI/cs.LG/stat.ML) — `cat:cs.AI OR cat:cs.LG OR cat:stat.ML`

</details>
