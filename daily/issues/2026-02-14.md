---
title: "Daily Issue (JST): 2026-02-14"
---

# Daily Issue (JST): 2026-02-14

## What's New Today

**AI Scaling, Reasoning, and Safety Advancements Dominate Research; Materials Insights Emerge**

Today's research highlights a strong focus on improving AI systems through test-time scaling, verification, and more efficient reasoning architectures. A parallel stream of work tackles safety, alignment, and the societal impact of these models. In the physical sciences, new methods for materials simulation and analysis offer insights into electronic correlation, phase retrieval, and thermodynamic stability.

**Highlights**
- Test-time scaling and verification frameworks like CoVer and UniT aim to close the 'intention-action gap' in vision-language-action models and multimodal reasoning.
- New architectures and training methods, such as MonarchRT for efficient video attention and On-Policy Context Distillation, seek to boost performance and efficiency.
- Safety and alignment research addresses robustness with SafeNeuron, measures value trade-offs, and critiques benchmark reliability.
- Materials physics papers present a stochastic cluster expansion for electronic correlation and a Vision Transformer for multi-domain phase retrieval.
- Agentic systems are evaluated in dynamic environments (Gaia2) and optimized for tasks like web navigation and molecular design.
- Efficient inference is pursued via methods like dVoting for diffusion LLMs and PrefillShare for multi-LLM serving.

**Themes**
- AI: Test-Time Scaling & Verification
- AI: Efficient Architectures & Training
- AI: Safety, Alignment & Evaluation
- Materials: Simulation & Electronic Structure
- AI: Agentic Systems & Reasoning
- AI: Efficient Inference & Serving

**Keywords**
- test-time scaling
- verification
- efficient attention
- safety alignment
- reasoning models
- agentic AI
- materials simulation
- electronic correlation
- phase retrieval
- diffusion models
- reinforcement learning
- knowledge distillation

**Total new papers:** 240

## Featured Papers

### [Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment](https://arxiv.org/abs/2602.12281v1)
**Authors:** Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone
**Published:** 2026-02-12
**Summary:** A test-time verification method called CoVer improves robot action alignment with instructions by scaling rephrased instructions and action candidates, outperforming scaling policy pre-training.

**What's new**
- Introduces CoVer, a contrastive verifier for vision-language-action alignment that scales well with compute and data.
- Proposes boot-time compute and hierarchical verification to pre-select optimal instructions and actions at deployment.
- Shows verification scaling beats policy pre-training scaling, with 22% in-distribution and 13% out-of-distribution gains.
- Achieves further 45% improvement in real-world experiments and 14% gains in task progress on PolaRiS benchmark.

### [UniT: Unified Multimodal Chain-of-Thought Test-time Scaling](https://arxiv.org/abs/2602.12279v1)
**Authors:** Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu
**Published:** 2026-02-12
**Summary:** UniT enables unified multimodal models to perform iterative reasoning and refinement at test time using chain-of-thought scaling, improving performance on complex tasks.

**What's new**
- Extends test-time scaling to unified multimodal models for iterative reasoning and output refinement.
- Introduces agentic data synthesis and unified training to elicit cognitive behaviors like verification.
- Shows models trained on short reasoning chains generalize to longer inference chains at test time.
- Finds sequential chain-of-thought reasoning is more scalable than parallel sampling for TTS.
- Demonstrates training on generation/editing trajectories improves out-of-distribution visual reasoning.

### [AttentionRetriever: Attention Layers are Secretly Long Document Retrievers](https://arxiv.org/abs/2602.12278v1)
**Authors:** David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang
**Published:** 2026-02-12
**Summary:** AttentionRetriever uses attention mechanisms and entity-based retrieval to create context-aware embeddings for efficient, high-performance long document retrieval.

**What's new**
- Leverages attention mechanism and entity-based retrieval for context-aware embeddings in long documents.
- Addresses key long document retrieval challenges: context-awareness, causal dependence, and scope of retrieval.
- Outperforms existing retrieval models on long document datasets by a large margin.
- Maintains efficiency comparable to dense retrieval models despite improved performance.

### [Agentic Test-Time Scaling for WebAgents](https://arxiv.org/abs/2602.12276v1)
**Authors:** Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami
**Published:** 2026-02-12
**Summary:** CATTS dynamically allocates compute for multi-step web agents using vote-derived uncertainty, improving performance while reducing token usage compared to uniform scaling.

**What's new**
- Introduces CATTS, a method for dynamic compute allocation in multi-step agents based on vote uncertainty.
- Shows uniform per-step compute scaling saturates quickly in long-horizon web agent tasks.
- Demonstrates vote distribution statistics (entropy, margin) correlate with downstream success.
- Achieves up to 9.1% performance gain over React with up to 2.3x fewer tokens than uniform scaling.

### [On-Policy Context Distillation for Language Models](https://arxiv.org/abs/2602.12275v1)
**Authors:** Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei
**Published:** 2026-02-12
**Summary:** OPCD trains language models to internalize in-context knowledge by having them learn from their own generated outputs while aligning with a teacher model via reverse KL divergence.

**What's new**
- Bridges on-policy distillation with context distillation using student-generated trajectories
- Applies to experiential knowledge distillation from historical solution traces
- Enables system prompt distillation to internalize beneficial behaviors
- Outperforms baselines across reasoning, games, and domain tasks
- Allows effective cross-size distillation from larger teachers to smaller students

### [Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage](https://arxiv.org/abs/2602.12274v1)
**Authors:** Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen
**Published:** 2026-02-12
**Summary:** Fun-DDPS is a generative framework combining function-space diffusion models with neural operator surrogates for accurate forward and inverse modeling in carbon storage, handling extreme data sparsity.

**What's new**
- Decouples diffusion prior for geomodels from neural operator surrogate for dynamics, enabling robust recovery of missing information.
- Achieves 7.7% relative error in forward modeling with 25% observations, an 11x improvement over standard surrogates.
- First rigorous validation of diffusion-based inverse solvers against exact Rejection Sampling posteriors.
- Produces physically consistent realizations without high-frequency artifacts seen in joint-state baselines.
- Provides 4x improved sample efficiency compared to rejection sampling for inverse modeling.

### [Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs](https://arxiv.org/abs/2602.12273v1)
**Authors:** Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng
**Published:** 2026-02-12
**Summary:** iUzawa-Net is a deep neural network that unrolls an inexact Uzawa method to solve nonsmooth optimal control of linear PDEs in real-time, combining optimization algorithms with learnable components.

**What's new**
- First solver enabling real-time solutions for a class of nonsmooth optimal control problems of linear PDEs.
- Unrolls an inexact Uzawa method, replacing classical preconditioners and PDE solvers with learnable neural networks.
- Proves universal approximation properties and asymptotic epsilon-optimality for the network.
- Validates numerical efficiency on nonsmooth elliptic and parabolic optimal control problems.
- Provides a versatile framework for optimization-informed deep learning in PDE-constrained optimization.

### [MonarchRT: Efficient Attention for Real-Time Video Generation](https://arxiv.org/abs/2602.12271v1)
**Authors:** Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen
**Published:** 2026-02-12
**Summary:** MonarchRT introduces a structured attention parameterization using Monarch matrices to enable efficient, real-time video generation by overcoming the quadratic cost of 3D self-attention in diffusion transformers.

**What's new**
- Proposes Monarch-RT, a structured attention parameterization for video diffusion that factorizes attention using Monarch matrices for high expressivity and efficiency.
- Shows video attention combines periodic structure, sparse semantic correspondences, and dense mixing, exceeding the capacity of prior sparse approximations like top-k.
- Achieves up to 95% attention sparsity with no quality loss when applied to the state-of-the-art Self-Forcing model for real-time generation.
- Provides optimized Triton kernels that outperform FlashAttention variants by 1.4-11.8X on modern GPUs, enabling 16 FPS real-time generation on a single RTX 5090.

### [Creative Ownership in the Age of AI](https://arxiv.org/abs/2602.12270v1)
**Authors:** Annie Liang, Jay Lu
**Published:** 2026-02-12
**Summary:** Proposes a new copyright infringement criterion for AI: an output infringes if it couldn't have been generated without a specific work in its training data, analyzing when regulation constrains AI.

**What's new**
- Introduces a novel infringement test based on whether an AI output depends on a specific training work.
- Models generative AI as a closure operator mapping a training corpus to new outputs.
- Reveals an asymptotic dichotomy: regulation fades with light-tailed organic creation but persists with heavy-tailed.

### [CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use](https://arxiv.org/abs/2602.12268v1)
**Authors:** Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang
**Published:** 2026-02-12
**Summary:** CM2 is an RL framework that uses checklist rewards and LLM-simulated environments to train multi-turn, multi-step tool-using agents without needing verifiable outcome rewards.

**What's new**
- Replaces verifiable outcome rewards with checklist rewards based on fine-grained binary criteria and explicit evidence grounding.
- Employs a strategy of sparse reward assignment but dense evaluation criteria to balance stability and informativeness.
- Trains agents in a scalable LLM-simulated tool environment, avoiding costly engineering for large tool sets.
- Shows consistent improvements over supervised fine-tuning on benchmarks like tau-Bench, BFCL-V4, and ToolSandbox.

### [Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data](https://arxiv.org/abs/2602.12267v1)
**Authors:** Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar
**Published:** 2026-02-12
**Summary:** A self-supervised learning method that uses flow-guided neural operators to learn time-series representations by varying noise levels as a training degree of freedom, improving performance in biomedical tasks.

**What's new**
- Treats corruption level as a learnable degree of freedom instead of using fixed masking ratios like MAEs
- Introduces Flow-Guided Neural Operator combining operator learning and flow matching for SSL on time series
- Extracts hierarchical features from different network layers and flow times with varying noise strengths
- Uses clean inputs for inference while training with noise, eliminating randomness and boosting accuracy
- Demonstrates large gains in biomedical tasks like neural decoding and sleep stage classification

### [T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization](https://arxiv.org/abs/2602.12262v1)
**Authors:** Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas
**Published:** 2026-02-12
**Summary:** A trajectory self-distillation method with a discriminative objective to improve the quality of diffusion language models when generating text in very few steps.

**What's new**
- Introduces trajectory self-distillation to distill a model's own generative trajectories for few-step decoding.
- Uses Direct Discriminative Optimization (DDO), a reverse-KL objective, for mode-seeking distillation.
- Consistently outperforms strong few-step baselines and standard training under tight step budgets.
- Substantially narrows the performance gap between few-step and full-step decoding in diffusion LLMs.

### [Think like a Scientist: Physics-guided LLM Agent for Equation Discovery](https://arxiv.org/abs/2602.12259v1)
**Authors:** Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu
**Published:** 2026-02-12
**Summary:** A physics-guided LLM agent that discovers symbolic equations by first inferring physical properties like symmetries, then using them to configure symbolic regression tools.

**What's new**
- Introduces KeplerAgent, an agentic framework that mimics the multi-step reasoning process of scientists for equation discovery.
- First infers physical properties (e.g., symmetries) from data as priors, rather than guessing equations directly.
- Uses these physics-based priors to configure symbolic regression engines like PySINDy and PySR with constraints.
- Achieves higher symbolic accuracy and robustness to noise compared to LLM and traditional baselines on physical benchmarks.

### [Vision Transformer for Multi-Domain Phase Retrieval in Coherent Diffraction Imaging](https://arxiv.org/abs/2602.12255v1)
**Authors:** Jialun Liu, David Yang, Ian Robinson
**Published:** 2026-02-12
**Summary:** An unsupervised Vision Transformer solves multi-domain phase retrieval in coherent diffraction imaging for crystals with strong distortions, outperforming classical methods.

**What's new**
- Uses Fourier Vision Transformer for multi-domain phase retrieval from 2D Bragg diffraction intensities
- Couples reciprocal-space information globally via multiscale Fourier token mixing
- Achieves lowest reciprocal-space mismatch on synthetic and experimental data
- Improves robustness to random initializations versus iterative and CNN baselines
- Preserves domain-resolved reconstructions as domain count increases

### [A Stochastic Cluster Expansion for Electronic Correlation in Large Systems](https://arxiv.org/abs/2602.12254v1)
**Authors:** Annabelle Canestraight, Anthony J. Dominic, Andres Montoya-Castillo, Libor Veis, Vojtech Vlcek
**Published:** 2026-02-12
**Summary:** A stochastic method that efficiently calculates near-exact correlation energies for large systems without needing to pre-select an active space.

**What's new**
- Eliminates the need for prior active space selection, a major bottleneck for heterogeneous/extended systems
- Combines random sampling of environment orbitals with exact treatment of a subspace for near-DMRG accuracy
- Provides a quantitative diagnostic for molecule-solvent correlation to guide embedding decisions
- Enables systematically improvable many-body calculations in condensed-phase systems at reduced cost

### [Community Concealment from Unsupervised Graph Learning-Based Clustering](https://arxiv.org/abs/2602.12250v1)
**Authors:** Dalyapraz Manatova, Pablo Moriano, L. Jean Camp
**Published:** 2026-02-12
**Summary:** A method to hide a community from GNN-based clustering by rewiring edges and modifying node features, based on boundary connectivity and feature similarity.

**What's new**
- Identifies boundary connectivity and feature similarity as key factors for community concealment in graphs.
- Proposes a perturbation strategy that rewires edges and modifies node features to reduce community distinctiveness.
- Outperforms DICE under identical perturbation budgets with 20-45% median relative concealment improvements.
- Demonstrates group-level privacy risks in GNN-based community detection and offers a mitigation strategy.

### ["Sorry, I Didn't Catch That": How Speech Models Miss What Matters Most](https://arxiv.org/abs/2602.12249v1)
**Authors:** Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou
**Published:** 2026-02-12
**Summary:** Speech recognition models fail on short, high-stakes street name transcriptions, especially for non-English speakers, but fine-tuning with synthetic data reduces errors.

**What's new**
- Reveals a 44% average error rate in transcribing U.S. street names, far above benchmark performance.
- Shows mis-transcriptions cause larger routing distance errors for non-English primary speakers.
- Introduces a synthetic data method using text-to-speech to generate diverse pronunciations.
- Fine-tuning with under 1,000 synthetic samples improves accuracy by nearly 60% for non-English speakers.

### [ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction](https://arxiv.org/abs/2602.12247v1)
**Authors:** Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen
**Published:** 2026-02-12
**Summary:** ExtractBench is an open-source benchmark and evaluation framework for testing LLMs on complex PDF-to-JSON structured extraction tasks, revealing significant performance gaps.

**What's new**
- Introduces an end-to-end benchmark for PDF-to-JSON extraction with enterprise-scale schema breadth and 12,867 annotated fields.
- Provides a principled evaluation methodology that treats the schema as an executable spec with field-specific scoring metrics.
- Reveals frontier LLMs are unreliable on realistic schemas, with performance degrading sharply as schema complexity increases.
- Shows models achieve 0% valid output on a 369-field financial reporting schema across all tested models.

### [Moonshine v2: Ergodic Streaming Encoder ASR for Latency-Critical Speech Applications](https://arxiv.org/abs/2602.12241v1)
**Authors:** Manjunath Kudlur, Evan King, James Wang, Pete Warden
**Published:** 2026-02-12
**Summary:** Moonshine v2 is a streaming ASR model using sliding-window attention for low-latency, accurate speech recognition on edge devices, matching larger models' performance.

**What's new**
- Introduces sliding-window self-attention for bounded, low-latency inference in streaming ASR.
- Achieves state-of-the-art word error rates, matching accuracy of models 6x larger.
- Enables fast time-to-first-token for latency-critical applications like live transcription.
- Demonstrates local attention can compete with full attention at lower size and latency.

### [Harmonic-to-anharmonic thermodynamic integration made simple using REG TI](https://arxiv.org/abs/2602.12240v1)
**Authors:** Venkat Kapil
**Published:** 2026-02-12
**Summary:** A regularization method fixes a near-singularity in harmonic-to-anharmonic thermodynamic integration for solids with diffusive motions, enabling accurate free energy calculations.

**What's new**
- Introduces Regularized End-point Gradient (REG) TI to remove integrand singularity in standard thermodynamic integration.
- Enables accurate evaluation on a uniform grid for solids with diffusive degrees of freedom like rotating groups.
- Demonstrated on paracetamol polymorphs where methyl rotations cause near-singularity in standard TI.
- Simplifies and may automate anharmonic free energy calculations for solids.

### [Detecting Overflow in Compressed Token Representations for Retrieval-Augmented Generation](https://arxiv.org/abs/2602.12235v1)
**Authors:** Julia Belikova, Danila Rozhevskii, Dennis Svirin, Konstantin Polev, Alexander Panchenko
**Published:** 2026-02-12
**Summary:** Proposes methods to detect when compressed token representations in retrieval-augmented generation lose essential information needed to answer a query.

**What's new**
- Defines 'token overflow' as a regime where compressed tokens lack sufficient information for query answering.
- Shows query-agnostic saturation statistics can identify compressed tokens but are limited for overflow detection.
- Introduces lightweight probing classifiers using query and context representations for overflow detection.
- Achieves 0.72 average AUC-ROC on HotpotQA, SQuADv2, and TriviaQA datasets with query-aware detectors.

### [Categorical Flow Maps](https://arxiv.org/abs/2602.12233v1)
**Authors:** Daan Roos, Oscar Davis, Floor Eijkelboom, Michael Bronstein, Max Welling, İsmail İlkan Ceylan, Luca Ambrogioni, Jan-Willem van de Meent
**Published:** 2026-02-12
**Summary:** A flow-matching method for accelerated few-step generation of categorical data via self-distillation, achieving state-of-the-art results on images, molecules, and text.

**What's new**
- Introduces flow maps that transport probability mass toward a predicted endpoint on the simplex for categorical data
- Enables training with existing distillation techniques and a new endpoint consistency objective
- Continuous formulation unlocks test-time guidance and reweighting techniques for categorical sampling
- Achieves state-of-the-art few-step results on images, molecular graphs, and text, even in single-step generation

### [Diffusion Alignment Beyond KL: Variance Minimisation as Effective Policy Optimiser](https://arxiv.org/abs/2602.12229v1)
**Authors:** Zijing Ou, Jacob Si, Junyi Zhu, Ondrej Bohdal, Mete Ozay, Taha Ceritli, Yingzhen Li
**Published:** 2026-02-12
**Summary:** VMPO reformulates diffusion alignment as minimizing variance of importance weights rather than KL divergence, providing a unified framework that recovers existing methods and suggests new directions.

**What's new**
- Introduces Variance Minimisation Policy Optimisation (VMPO) for diffusion alignment via importance weight variance reduction
- Proves variance objective is minimized by reward-tilted target distribution and matches KL gradient under on-policy sampling
- Provides unified framework that recovers existing diffusion alignment methods under different variance strategies
- Suggests new design directions beyond traditional KL-based alignment approaches

### [Kagome edge states under lattice termination, spin-orbit coupling, and magnetic order](https://arxiv.org/abs/2602.12223v1)
**Authors:** Sajid Sekh, Annica M. Black-Schaffer, Andrzej Ptok
**Published:** 2026-02-12
**Summary:** Study shows how lattice termination, spin-orbit coupling, and magnetic order control edge states and topological phases in kagome lattices.

**What's new**
- Edge states in pristine kagome lattice are highly sensitive to boundary geometry, with some terminations suppressing them.
- Kane-Mele spin-orbit coupling creates a robust Z2 topological insulator with helical edge states, insensitive to termination.
- Zeeman field with Rashba coupling drives Chern insulator phases with chiral edge modes matching Chern numbers.
- Non-coplanar magnetic textures generate multiple Chern phases via scalar spin chirality, with Kane-Mele coupling tuning gaps.
- Results provide insights for designing kagome materials with tunable edge states and topological properties.

<details><summary>More papers (216)</summary>

**Materials & Physics (cond-mat / comp-ph / chem-ph)**
- [Magnetopological mechanics in Maxwell lattice frustrated Mott insulators](https://arxiv.org/abs/2602.12168v1)
- [A critical assessment of bonding descriptors for predicting materials properties](https://arxiv.org/abs/2602.12109v1)
- [Stacking theory for bilayer two-dimensional magnets](https://arxiv.org/abs/2602.12068v1)
- [Bond failure in peridynamics: Nonequivalence of critical stretch and critical energy density criteria](https://arxiv.org/abs/2602.12061v1)
- [Markov State Models for Tracking Reaction Dynamics on Catalytic Nanoparticles](https://arxiv.org/abs/2602.12037v1)
- [Thermodynamic Stability and Hydrogen Bonds in Mixed Halide Perovskites](https://arxiv.org/abs/2602.12031v1)
- [Topological chiral random walker](https://arxiv.org/abs/2602.12020v1)
- [Tuning Optical Properties of FTO via Carbonaceous Al2O3 Microdot Deposition by DC plasma sputtering](https://arxiv.org/abs/2602.11970v1)
- [Emergence of a Helical Metal in Rippled Ultrathin Topological Insulator Sb\textsubscript{2}Te\textsubscript{3} on Graphene](https://arxiv.org/abs/2602.11932v1)
- [Phaseless auxiliary-field quantum Monte Carlo method with spin-orbit coupling](https://arxiv.org/abs/2602.11866v1)
- [Stacking-dependent magnetic ordering in bilayer ScI$_{2}$](https://arxiv.org/abs/2602.11781v1)
- [Ultra-Fast 3D Porous Media Generation: a GPU- Accelerated List-Indexed Explicit Time-Stepping QSGS Algorithm](https://arxiv.org/abs/2602.11734v1)
- [Emergence of a spin Hall topological Hall effect in the non-collinear phase of the ferrimagnetic insulator terbium-iron garnet](https://arxiv.org/abs/2602.11721v1)
- [Rust-accelerated powder X-ray diffraction simulation for high-throughput and machine-learning-driven materials science](https://arxiv.org/abs/2602.11709v1)
- [A Hardware-Native Realisation of Semi-Empirical Electronic Structure Theory on Field-Programmable Gate Arrays](https://arxiv.org/abs/2602.11702v1)
- [Experimental challenges and prospects for quantum-enhanced energy conversion: Stationary Fano coherence in V-type qutrits interacting with polarized incoherent radiation](https://arxiv.org/abs/2602.11695v1)
- [Epitaxial Growth and Anomalous Hall Effect in High-Quality Altermagnetic $α$-MnTe Thin Films](https://arxiv.org/abs/2602.11645v1)
- [ArGEnT: Arbitrary Geometry-encoded Transformer for Operator Learning](https://arxiv.org/abs/2602.11626v1)
- [Rapid Dissipative Ground State Preparation at Chemical Transition States](https://arxiv.org/abs/2602.11603v1)
- [Strain-Driven Altermagnetic Spin Splitting Effect in RuO$_2$](https://arxiv.org/abs/2602.11602v1)

**AI/ML (cs.AI/cs.LG/stat.ML/cs.CL)**
- [On the implicit regularization of Langevin dynamics with projected noise](https://arxiv.org/abs/2602.12257v1)
- [Is Online Linear Optimization Sufficient for Strategic Robustness?](https://arxiv.org/abs/2602.12253v1)
- [A technical curriculum on language-oriented artificial intelligence in translation and specialised communication](https://arxiv.org/abs/2602.12251v1)
- [Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces](https://arxiv.org/abs/2602.12245v1)
- [Olmix: A Framework for Data Mixing Throughout LM Development](https://arxiv.org/abs/2602.12237v1)
- [Energy-Aware Spike Budgeting for Continual Learning in Spiking Neural Networks for Neuromorphic Vision](https://arxiv.org/abs/2602.12236v1)
- [Bandit Learning in Matching Markets with Interviews](https://arxiv.org/abs/2602.12224v1)
- [Towards On-Policy SFT: Distribution Discriminant Theory and its Applications in LLM Training](https://arxiv.org/abs/2602.12222v1)
- [The Observer Effect in World Models: Invasive Adaptation Corrupts Latent Physics](https://arxiv.org/abs/2602.12218v1)
- [VIRENA: Virtual Arena for Research, Education, and Democratic Innovation](https://arxiv.org/abs/2602.12207v1)
- [DeepGen 1.0: A Lightweight Unified Multimodal Model for Advancing Image Generation and Editing](https://arxiv.org/abs/2602.12205v1)
- [Learning to Forget Attention: Memory Consolidation for Adaptive Compute Reduction](https://arxiv.org/abs/2602.12204v1)
- [ExStrucTiny: A Benchmark for Schema-Variable Structured Information Extraction from Document Images](https://arxiv.org/abs/2602.12203v1)
- [Visual Reasoning Benchmark: Evaluating Multimodal LLMs on Classroom-Authentic Visual Problems from Primary Education](https://arxiv.org/abs/2602.12196v1)
- [Query-focused and Memory-aware Reranker for Long Context Processing](https://arxiv.org/abs/2602.12192v1)
- [WaveFormer: Wavelet Embedding Transformer for Biomedical Signals](https://arxiv.org/abs/2602.12189v1)
- [SAGEO Arena: A Realistic Environment for Evaluating Search-Augmented Generative Engine Optimization](https://arxiv.org/abs/2602.12187v1)
- [Convex Markov Games and Beyond: New Proof of Existence, Characterization and Learning Algorithms for Nash Equilibria](https://arxiv.org/abs/2602.12181v1)
- [How Sampling Shapes LLM Alignment: From One-Shot Optima to Iterative Dynamics](https://arxiv.org/abs/2602.12180v1)
- [SAM3-LiteText: An Anatomical Study of the SAM3 Text Encoder for Efficient Vision-Language Segmentation](https://arxiv.org/abs/2602.12173v1)
- [Pedagogically-Inspired Data Synthesis for Language Model Knowledge Distillation](https://arxiv.org/abs/2602.12172v1)
- [Statistical Parsing for Logical Information Retrieval](https://arxiv.org/abs/2602.12170v1)
- [Sci-CoE: Co-evolving Scientific Reasoning LLMs via Geometric Consensus with Sparse Supervision](https://arxiv.org/abs/2602.12164v1)
- [Amortized Molecular Optimization via Group Relative Policy Optimization](https://arxiv.org/abs/2602.12162v1)
- [3DGSNav: Enhancing Vision-Language Model Reasoning for Object Navigation via Active 3D Gaussian Splatting](https://arxiv.org/abs/2602.12159v1)
- [SafeNeuron: Neuron-Level Safety Alignment for Large Language Models](https://arxiv.org/abs/2602.12158v1)
- [dVoting: Fast Voting for dLLMs](https://arxiv.org/abs/2602.12153v1)
- [GPT-4o Lacks Core Features of Theory of Mind](https://arxiv.org/abs/2602.12150v1)
- [It's TIME: Towards the Next Generation of Time Series Forecasting Benchmarks](https://arxiv.org/abs/2602.12147v1)
- [Seq2Seq2Seq: Lossless Data Compression via Discrete Latent Transformers and Reinforcement Learning](https://arxiv.org/abs/2602.12146v1)
- [On the Adoption of AI Coding Agents in Open-source Android and iOS Development](https://arxiv.org/abs/2602.12144v1)
- [STAR : Bridging Statistical and Agentic Reasoning for Large Model Performance Prediction](https://arxiv.org/abs/2602.12143v1)
- [Oscillators Are All You Need: Irregular Time Series Modelling via Damped Harmonic Oscillators with Closed-Form Solutions](https://arxiv.org/abs/2602.12139v1)
- [CitiLink-Minutes: A Multilayer Annotated Dataset of Municipal Meeting Minutes](https://arxiv.org/abs/2602.12137v1)
- [WavBench: Benchmarking Reasoning, Colloquialism, and Paralinguistics for End-to-End Spoken Dialogue Models](https://arxiv.org/abs/2602.12135v1)
- [Value Alignment Tax: Measuring Value Trade-offs in LLM Alignment](https://arxiv.org/abs/2602.12134v1)
- [Neutral Prompts, Non-Neutral People: Quantifying Gender and Skin-Tone Bias in Gemini Flash 2.5 Image and GPT Image 1.5](https://arxiv.org/abs/2602.12133v1)
- [A Rule-based Computational Model for Gaidhlig Morphology](https://arxiv.org/abs/2602.12132v1)
- [Towards Personalized Bangla Book Recommendation: A Large-Scale Multi-Entity Book Graph Dataset](https://arxiv.org/abs/2602.12129v1)
- [HLA: Hadamard Linear Attention](https://arxiv.org/abs/2602.12128v1)
- [Learning beyond Teacher: Generalized On-Policy Distillation with Reward Extrapolation](https://arxiv.org/abs/2602.12125v1)
- [Capability-Oriented Training Induced Alignment Risk](https://arxiv.org/abs/2602.12124v1)
- [Meta-Sel: Efficient Demonstration Selection for In-Context Learning via Supervised Meta-Learning](https://arxiv.org/abs/2602.12123v1)
- [Commencing-Student Enrolment Forecasting Under Data Sparsity with Time Series Foundation Models](https://arxiv.org/abs/2602.12120v1)
- [KAN-FIF: Spline-Parameterized Lightweight Physics-based Tropical Cyclone Estimation on Meteorological Satellite](https://arxiv.org/abs/2602.12117v1)
- [P-GenRM: Personalized Generative Reward Model with Test-time User-based Scaling](https://arxiv.org/abs/2602.12116v1)
- [Stop Unnecessary Reflection: Training LRMs for Efficient Reasoning with Adaptive Reflection and Length Coordinated Penalty](https://arxiv.org/abs/2602.12113v1)
- [Few-Shot Design Optimization by Exploiting Auxiliary Information](https://arxiv.org/abs/2602.12112v1)
- [The Pensieve Paradigm: Stateful Language Models Mastering Their Own Context](https://arxiv.org/abs/2602.12108v1)
- [On the Complexity of Offline Reinforcement Learning with $Q^\star$-Approximation and Partial Coverage](https://arxiv.org/abs/2602.12107v1)
- [Iskra: A System for Inverse Geometry Processing](https://arxiv.org/abs/2602.12105v1)
- [Multi Graph Search for High-Dimensional Robot Motion Planning](https://arxiv.org/abs/2602.12096v1)
- [DeepSight: An All-in-One LM Safety Toolkit](https://arxiv.org/abs/2602.12092v1)
- [Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation](https://arxiv.org/abs/2602.12089v1)
- [Geometry of Uncertainty: Learning Metric Spaces for Multimodal State Estimation in RL](https://arxiv.org/abs/2602.12087v1)
- [Differentiable Modal Logic for Multi-Agent Diagnosis, Orchestration and Communication](https://arxiv.org/abs/2602.12083v1)
- [Empirical Gaussian Processes](https://arxiv.org/abs/2602.12082v1)
- [PathCRF: Ball-Free Soccer Event Detection via Possession Path Inference from Player Trajectories](https://arxiv.org/abs/2602.12080v1)
- [Tiny Recursive Reasoning with Mamba-2 Attention Hybrid](https://arxiv.org/abs/2602.12078v1)
- [ModelWisdom: An Integrated Toolkit for TLA+ Model Visualization, Digest and Repair](https://arxiv.org/abs/2602.12058v1)
- [LawThinker: A Deep Research Legal Agent in Dynamic Environments](https://arxiv.org/abs/2602.12056v1)
- [Multi UAVs Preflight Planning in a Shared and Dynamic Airspace](https://arxiv.org/abs/2602.12055v1)
- [Improving HPC Code Generation Capability of LLMs via Online Reinforcement Learning with Real-Machine Benchmark Rewards](https://arxiv.org/abs/2602.12049v1)
- [Safety Beyond the Training Data: Robust Out-of-Distribution MPC via Conformalized System Level Synthesis](https://arxiv.org/abs/2602.12047v1)
- [Fourier Transformers for Latent Crystallographic Diffusion and Generative Modeling](https://arxiv.org/abs/2602.12045v1)
- [Improved Inference for CSDID Using the Cluster Jackknife](https://arxiv.org/abs/2602.12043v1)
- [The Implicit Bias of Logit Regularization](https://arxiv.org/abs/2602.12039v1)
- [An Empirical Study of the Imbalance Issue in Software Vulnerability Detection](https://arxiv.org/abs/2602.12038v1)
- [Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://arxiv.org/abs/2602.12036v1)
- [PrefillShare: A Shared Prefill Module for KV Reuse in Multi-LLM Disaggregated Serving](https://arxiv.org/abs/2602.12029v1)
- [Protein Circuit Tracing via Cross-layer Transcoders](https://arxiv.org/abs/2602.12026v1)
- [Decomposition of Spillover Effects Under Misspecification:Pseudo-true Estimands and a Local--Global Extension](https://arxiv.org/abs/2602.12023v1)
- [Improved state mixing in higher-order and block diagonal linear recurrent networks](https://arxiv.org/abs/2602.12021v1)
- [Artificial intelligence is creating a new global linguistic hierarchy](https://arxiv.org/abs/2602.12018v1)
- [Disentangling Ambiguity from Instability in Large Language Models: A Clinical Text-to-SQL Case Study](https://arxiv.org/abs/2602.12015v1)
- [FedGRPO: Privately Optimizing Foundation Models with Group-Relative Rewards from Domain Client](https://arxiv.org/abs/2602.12014v1)
- [InjectRBP: Steering Large Language Model Reasoning Behavior via Pattern Injection](https://arxiv.org/abs/2602.12013v1)
- [On the Sensitivity of Firing Rate-Based Federated Spiking Neural Networks to Differential Privacy](https://arxiv.org/abs/2602.12009v1)
- [LaCy: What Small Language Models Can and Should Learn is Not Just a Question of Loss](https://arxiv.org/abs/2602.12005v1)
- [CSEval: A Framework for Evaluating Clinical Semantics in Text-to-Image Generation](https://arxiv.org/abs/2602.12004v1)
- [Momentum LMS Theory beyond Stationarity: Stability, Tracking, and Regret](https://arxiv.org/abs/2602.11995v1)
- [Evaluating AGENTS.md: Are Repository-Level Context Files Helpful for Coding Agents?](https://arxiv.org/abs/2602.11988v1)
- [Automatic Simplification of Common Vulnerabilities and Exposures Descriptions](https://arxiv.org/abs/2602.11982v1)
- [Accelerating Robotic Reinforcement Learning with Agent Guidance](https://arxiv.org/abs/2602.11978v1)
- [Calibrated Bayesian Deep Learning for Explainable Decision Support Systems Based on Medical Imaging](https://arxiv.org/abs/2602.11973v1)
- [DHPLT: large-scale multilingual diachronic corpora and word representations for semantic change modelling](https://arxiv.org/abs/2602.11968v1)
- [Manifold-Aware Temporal Domain Generalization for Large Language Models](https://arxiv.org/abs/2602.11965v1)
- [Gaia2: Benchmarking LLM Agents on Dynamic and Asynchronous Environments](https://arxiv.org/abs/2602.11964v1)
- [Scaling Model and Data for Multilingual Machine Translation with Open Large Language Models](https://arxiv.org/abs/2602.11961v1)
- [Benchmarking Vision-Language Models for French PDF-to-Markdown Conversion](https://arxiv.org/abs/2602.11960v1)
- [RAM-Net: Expressive Linear Attention with Selectively Addressable Memory](https://arxiv.org/abs/2602.11958v1)
- [Are Two LLMs Better Than One? A Student-Teacher Dual-Head LLMs Architecture for Pharmaceutical Content Optimization](https://arxiv.org/abs/2602.11957v1)
- [TAVAE: A VAE with Adaptable Priors Explains Contextual Modulation in the Visual Cortex](https://arxiv.org/abs/2602.11956v1)
- [Insights on Muon from Simple Quadratics](https://arxiv.org/abs/2602.11948v1)
- [Mixed-Integer Programming for Change-point Detection](https://arxiv.org/abs/2602.11947v1)
- [Towards Performance-Enhanced Model-Contrastive Federated Learning using Historical Information in Heterogeneous Scenarios](https://arxiv.org/abs/2602.11945v1)
- [Using predictive multiplicity to measure individual performance within the AI Act](https://arxiv.org/abs/2602.11944v1)
- [Synthesis of Late Gadolinium Enhancement Images via Implicit Neural Representations for Cardiac Scar Segmentation](https://arxiv.org/abs/2602.11942v1)
- [IncompeBench: A Permissively Licensed, Fine-Grained Benchmark for Music Information Retrieval](https://arxiv.org/abs/2602.11941v1)
- [Temporally Unified Adversarial Perturbations for Time Series Forecasting](https://arxiv.org/abs/2602.11940v1)
- [Do Large Language Models Adapt to Language Variation across Socioeconomic Status?](https://arxiv.org/abs/2602.11939v1)
- [Who is the richest club in the championship? Detecting and Rewriting Underspecified Questions Improve QA Performance](https://arxiv.org/abs/2602.11938v1)
- [Extending Puzzle for Mixture-of-Experts Reasoning Models with Application to GPT-OSS Acceleration](https://arxiv.org/abs/2602.11937v1)
- [Cross-Modal Robustness Transfer (CMRT): Training Robust Speech Translation Models Using Adversarial Text](https://arxiv.org/abs/2602.11933v1)
- [AdaptEvolve: Improving Efficiency of Evolutionary AI Agents through Adaptive Model Selection](https://arxiv.org/abs/2602.11931v1)
- [Who Does What? Archetypes of Roles Assigned to LLMs During Human-AI Decision-Making](https://arxiv.org/abs/2602.11924v1)
- [Learning Conditional Averages](https://arxiv.org/abs/2602.11920v1)
- [DynaHOI: Benchmarking Hand-Object Interaction for Dynamic Target](https://arxiv.org/abs/2602.11919v1)
- [MEME: Modeling the Evolutionary Modes of Financial Markets](https://arxiv.org/abs/2602.11918v1)
- [AlphaPROBE: Alpha Mining via Principled Retrieval and On-graph biased evolution](https://arxiv.org/abs/2602.11917v1)
- [TADA! Tuning Audio Diffusion Models through Activation Steering](https://arxiv.org/abs/2602.11910v1)
- [Echo: Towards Advanced Audio Comprehension via Audio-Interleaved Reasoning](https://arxiv.org/abs/2602.11909v1)
- [When Should LLMs Be Less Specific? Selective Abstraction for Reliable Long-Form Text Generation](https://arxiv.org/abs/2602.11908v1)
- [Leveraging LLMs to support co-evolution between definitions and instances of textual DSLs: A Systematic Evaluation](https://arxiv.org/abs/2602.11904v1)
- [Mitigating Mismatch within Reference-based Preference Optimization](https://arxiv.org/abs/2602.11902v1)
- [Benchmark Illusion: Disagreement among LLMs and Its Scientific Consequences](https://arxiv.org/abs/2602.11898v1)
- [Agentic AI for Cybersecurity: A Meta-Cognitive Architecture for Governable Autonomy](https://arxiv.org/abs/2602.11897v1)
- [Universal Diffusion-Based Probabilistic Downscaling](https://arxiv.org/abs/2602.11893v1)
- [LLM-based Triplet Extraction from Financial Reports](https://arxiv.org/abs/2602.11886v1)
- [Where Bits Matter in World Model Planning: A Paired Mixed-Bit Study for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.11882v1)
- [From Atoms to Trees: Building a Structured Feature Forest with Hierarchical Sparse Autoencoders](https://arxiv.org/abs/2602.11881v1)
- [SynthRAR: Ring Artifacts Reduction in CT with Unrolled Network and Synthetic Data Training](https://arxiv.org/abs/2602.11880v1)
- [Towards Fair and Comprehensive Evaluation of Routers in Collaborative LLM Systems](https://arxiv.org/abs/2602.11877v1)
- [DMAP: A Distribution Map for Text](https://arxiv.org/abs/2602.11871v1)
- [Intelligent AI Delegation](https://arxiv.org/abs/2602.11865v1)
- [In-Context Function Learning in Large Language Models](https://arxiv.org/abs/2602.11863v1)
- [A$^{2}$V-SLP: Alignment-Aware Variational Modeling for Disentangled Sign Language Production](https://arxiv.org/abs/2602.11861v1)
- [Talk2DM: Enabling Natural Language Querying and Commonsense Reasoning for Vehicle-Road-Cloud Integrated Dynamic Maps with Large Language Models](https://arxiv.org/abs/2602.11860v1)
- [Zooming without Zooming: Region-to-Image Distillation for Fine-Grained Multimodal Perception](https://arxiv.org/abs/2602.11858v1)
- [Scale-Invariant Fast Convergence in Games](https://arxiv.org/abs/2602.11857v1)
- [Robust Optimization Approach and Learning Based Hide-and-Seek Game for Resilient Network Design](https://arxiv.org/abs/2602.11854v1)
- [Prototype Transformer: Towards Language Model Architectures Interpretable by Design](https://arxiv.org/abs/2602.11852v1)
- [Resource-Aware Deployment Optimization for Collaborative Intrusion Detection in Layered Networks](https://arxiv.org/abs/2602.11851v1)
- [Free Lunch for Stabilizing Rectified Flow Inversion](https://arxiv.org/abs/2602.11850v1)
- [Improving Neural Retrieval with Attribution-Guided Query Rewriting](https://arxiv.org/abs/2602.11841v1)
- [ULTRA:Urdu Language Transformer-based Recommendation Architecture](https://arxiv.org/abs/2602.11836v1)
- [EqDeepRx: Learning a Scalable MIMO Receiver](https://arxiv.org/abs/2602.11834v1)
- [Towards Sustainable Investment Policies Informed by Opponent Shaping](https://arxiv.org/abs/2602.11829v1)
- [CAAL: Confidence-Aware Active Learning for Heteroscedastic Atmospheric Regression](https://arxiv.org/abs/2602.11825v1)
- [Revis: Sparse Latent Steering to Mitigate Object Hallucination in Large Vision-Language Models](https://arxiv.org/abs/2602.11824v1)
- [A Comparative Study of MAP and LMMSE Estimators for Blind Inverse Problems](https://arxiv.org/abs/2602.11814v1)
- [Predicting LLM Output Length via Entropy-Guided Representations](https://arxiv.org/abs/2602.11812v1)
- [How to Sample High Quality 3D Fractals for Action Recognition Pre-Training?](https://arxiv.org/abs/2602.11810v1)
- [Deep Kernel Fusion for Transformers](https://arxiv.org/abs/2602.11808v1)
- [PuYun-LDM: A Latent Diffusion Model for High-Resolution Ensemble Weather Forecasts](https://arxiv.org/abs/2602.11807v1)
- [From Path Signatures to Sequential Modeling: Incremental Signature Contributions for Offline RL](https://arxiv.org/abs/2602.11805v1)
- [TopoFair: Linking Topological Bias to Fairness in Link Prediction Benchmarks](https://arxiv.org/abs/2602.11802v1)
- [SpaTeoGL: Spatiotemporal Graph Learning for Interpretable Seizure Onset Zone Analysis from Intracranial EEG](https://arxiv.org/abs/2602.11801v1)
- [Temporal Difference Learning with Constrained Initial Representations](https://arxiv.org/abs/2602.11800v1)
- [Hi-SAM: A Hierarchical Structure-Aware Multi-modal Framework for Large-Scale Recommendation](https://arxiv.org/abs/2602.11799v1)
- [A Subword Embedding Approach for Variation Detection in Luxembourgish User Comments](https://arxiv.org/abs/2602.11795v1)
- [Latent-Variable Learning of SPDEs via Wiener Chaos](https://arxiv.org/abs/2602.11794v1)
- [More Haste, Less Speed: Weaker Single-Layer Watermark Improves Distortion-Free Watermark Ensembles](https://arxiv.org/abs/2602.11793v1)
- [Detecting RLVR Training Data via Structural Convergence of Reasoning](https://arxiv.org/abs/2602.11792v1)
- [Beyond End-to-End Video Models: An LLM-Based Multi-Agent System for Educational Video Generation](https://arxiv.org/abs/2602.11790v1)
- [Decentralized Non-convex Stochastic Optimization with Heterogeneous Variance](https://arxiv.org/abs/2602.11789v1)
- [Evaluating LLM Safety Under Repeated Inference via Accelerated Prompt Stress Testing](https://arxiv.org/abs/2602.11786v1)
- [Safe Fairness Guarantees Without Demographics in Classification: Spectral Uncertainty Set Perspective](https://arxiv.org/abs/2602.11785v1)
- [FlowMind: Execute-Summarize for Structured Workflow Generation from LLM Reasoning](https://arxiv.org/abs/2602.11782v1)
- [RELATE: A Reinforcement Learning-Enhanced LLM Framework for Advertising Text Generation](https://arxiv.org/abs/2602.11780v1)
- [Temperature as a Meta-Policy: Adaptive Temperature in LLM Reinforcement Learning](https://arxiv.org/abs/2602.11779v1)
- [MUSE: Multi-Tenant Model Serving With Seamless Model Updates](https://arxiv.org/abs/2602.11776v1)
- [How to Optimize Multispecies Set Predictions in Presence-Absence Modeling ?](https://arxiv.org/abs/2602.11771v1)
- [TSR: Trajectory-Search Rollouts for Multi-Turn RL of LLM Agents](https://arxiv.org/abs/2602.11767v1)
- [MiniCPM-SALA: Hybridizing Sparse and Linear Attention for Efficient Long-Context Modeling](https://arxiv.org/abs/2602.11761v1)
- [Aggregate Models, Not Explanations: Improving Feature Importance Estimation](https://arxiv.org/abs/2602.11760v1)
- [TUBO: A Tailored ML Framework for Reliable Network Traffic Forecasting](https://arxiv.org/abs/2602.11759v1)
- [Cooperation Breakdown in LLM Agents Under Communication Delays](https://arxiv.org/abs/2602.11754v1)
- [AmbiBench: Benchmarking Mobile GUI Agents Beyond One-Shot Instructions in the Wild](https://arxiv.org/abs/2602.11750v1)
- [AIR: Improving Agent Safety through Incident Response](https://arxiv.org/abs/2602.11749v1)
- [Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning](https://arxiv.org/abs/2602.11748v1)
- [High-Probability Minimax Adaptive Estimation in Besov Spaces via Online-to-Batch](https://arxiv.org/abs/2602.11747v1)
- [Text2GQL-Bench: A Text to Graph Query Language Benchmark [Experiment, Analysis & Benchmark]](https://arxiv.org/abs/2602.11745v1)
- [U-Former ODE: Fast Probabilistic Forecasting of Irregular Time Series](https://arxiv.org/abs/2602.11738v1)
- [Mask What Matters: Mitigating Object Hallucinations in Multimodal Large Language Models with Object-Aligned Visual Contrastive Decoding](https://arxiv.org/abs/2602.11737v1)
- [Adapting Vision-Language Models for E-commerce Understanding at Scale](https://arxiv.org/abs/2602.11733v1)
- [Thinking with Drafting: Optical Decompression via Logical Reconstruction](https://arxiv.org/abs/2602.11731v1)
- [Cross-Architecture Model Diffing with Crosscoders: Unsupervised Discovery of Differences Between LLMs](https://arxiv.org/abs/2602.11729v1)
- [Dopamine: Brain Modes, Not Brains](https://arxiv.org/abs/2602.11726v1)
- [PAC-Bayesian Generalization Guarantees for Fairness on Stochastic and Deterministic Classifiers](https://arxiv.org/abs/2602.11722v1)

_…and 16 more._

</details>

<details><summary>Search definitions</summary>

- Materials & Physics (cond-mat / comp-ph / chem-ph) — `cat:cond-mat.mtrl-sci OR cat:physics.comp-ph OR cat:physics.chem-ph`
- AI/ML (cs.AI/cs.LG/stat.ML/cs.CL) — `cat:cs.AI OR cat:cs.LG OR cat:stat.ML OR cat:cs.CL`

</details>
